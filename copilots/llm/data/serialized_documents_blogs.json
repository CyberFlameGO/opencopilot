[
    {
        "page_content": "Emerging Architectures for LLM Applications\n\nby\n\nMatt Bornstein and \t\n\t\t\tRajko Radovanovic\n\nAI, machine & deep learning\n\nenterprise & SaaS\n\nAI\n\nGenerative AI\n\nmachine learning\n\nFacebook\n\nLinkedIn\n\nTwitter\n\nThe stack\n\nDesign pattern: In-context learning\n\nData preprocessing / embedding\n\nPrompt construction/ retrieval\n\nPrompt execution / inference\n\nWhat about agents?\n\nLooking ahead\n\nExplore more: AI + a16z\n\nLarge language models are a powerful new primitive for building software. But since they are so new\u2014and behave so differently from normal computing resources\u2014it\u2019s not always obvious how to use them.\n\nIn this post, we\u2019re sharing a reference architecture for the emerging LLM app stack. It shows the most common systems, tools, and design patterns we\u2019ve seen used by AI startups and sophisticated tech companies. This stack is still very early and may change substantially as the underlying technology advances, but we hope it will be a useful reference for developers working with LLMs now.\n\nThis work is based on conversations with AI startup founders and engineers. We relied especially on input from: Ted Benson, Harrison Chase, Ben Firshman, Ali Ghodsi, Raza Habib, Andrej Karpathy, Greg Kogan, Jerry Liu, Moin Nadeem, Diego Oppenheimer, Shreya Rajpal, Ion Stoica, Dennis Xu, Matei Zaharia, and Jared Zoneraich. Thank you for your help!\n\nThe stack\n\nHere\u2019s our current view of the LLM app stack (click to enlarge):\n\nAnd here\u2019s a list of links to each project for quick reference:\n\nData pipelines\n\nEmbedding model\n\nVector database\n\nPlayground\n\nOrchestration\n\nAPIs/plugins\n\nLLM cache\n\nDatabricks\n\nOpenAI\n\nPinecone\n\nOpenAI\n\nLangchain\n\nSerp\n\nRedis\n\nAirflow\n\nCohere\n\nWeaviate\n\nnat.dev\n\nLlamaIndex\n\nWolfram\n\nSQLite\n\nUnstructured\n\nHugging Face\n\nChromaDB\n\nHumanloop\n\nChatGPT\n\nZapier\n\nGPTCache\n\npgvector\n\nLogging / LLMops\n\nValidation\n\nApp hosting\n\nLLM APIs (proprietary)\n\nLLM APIs (open)\n\nCloud providers\n\nOpinionated clouds\n\nWeights & Biases\n\nGuardrails\n\nVercel\n\nOpenAI\n\nHugging Face\n\nAWS\n\nDatabricks\n\nMLflow\n\nRebuff\n\nSteamship\n\nAnthropic\n\nReplicate\n\nGCP\n\nAnyscale\n\nPromptLayer\n\nMicrosoft Guidance\n\nStreamlit\n\nAzure\n\nMosaic\n\nHelicone\n\nLMQL\n\nModal\n\nCoreWeave\n\nModal\n\nRunPod\n\nThere are many different ways to build with LLMs, including training models from scratch, fine-tuning open-source models, or using hosted APIs. The stack we\u2019re showing here is based on in-context learning, which is the design pattern we\u2019ve seen the majority of developers start with (and is only possible now with foundation models).\n\nThe next section gives a brief explanation of this pattern; experienced LLM developers can skip this section.\n\nDesign pattern: In-context learning\n\nThe core idea of in-context learning is to use LLMs off the shelf (i.e., without any fine-tuning), then control their behavior through clever prompting and conditioning on private \u201ccontextual\u201d data.\n\nFor example, say you\u2019re building a chatbot to answer questions about a set of legal documents. Taking a naive approach, you could paste all the documents into a ChatGPT or GPT-4 prompt, then ask a question about them at the end. This may work for very small datasets, but it doesn\u2019t scale. The biggest GPT-4 model can only process ~50 pages of input text, and performance (measured by inference time and accuracy) degrades badly as you approach this limit, called a context window.\n\nIn-context learning solves this problem with a clever trick: instead of sending all the documents with each LLM prompt, it sends only a handful of the most relevant documents. And the most relevant documents are determined with the help of . . . you guessed it . . . LLMs.\n\nAt a very high level, the workflow can be divided into three stages:\n\nData preprocessing / embedding: This stage involves storing private data (legal documents, in our example) to be retrieved later. Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database.\n\nPrompt construction / retrieval: When a user submits a query (a legal question, in this case), the application constructs a series of prompts to submit to the language model. A compiled prompt typically combines a prompt template hard-coded by the developer; examples of valid outputs called few-shot examples; any necessary information retrieved from external APIs; and a set of relevant documents retrieved from the vector database.\n\nPrompt execution / inference: Once the prompts have been compiled, they are submitted to a pre-trained LLM for inference\u2014including both proprietary model APIs and open-source or self-trained models. Some developers also add operational systems like logging, caching, and validation at this stage.\n\nThis looks like a lot of work, but it\u2019s usually easier than the alternative: training or fine-tuning the LLM itself. You don\u2019t need a specialized team of ML engineers to do in-context learning. You also don\u2019t need to host your own infrastructure or buy an expensive dedicated instance from OpenAI. This pattern effectively reduces an AI problem to a data engineering problem that most startups and big companies already know how to solve. It also tends to outperform fine-tuning for relatively small datasets\u2014since a specific piece of information needs to occur at least ~10 times in the training set before an LLM will remember it through fine-tuning\u2014and can incorporate new data in near real time.\n\nOne of the biggest questions around in-context learning is: What happens if we just change the underlying model to increase the context window? This is indeed possible, and it is an active area of research (e.g., see the Hyena paper or this recent post). But this comes with a number of tradeoffs\u2014primarily that cost and time of inference scale quadratically with the length of the prompt. Today, even linear scaling (the best theoretical outcome) would be cost-prohibitive for many applications. A single GPT-4 query over 10,000 pages would cost hundreds of dollars at current API rates. So, we don\u2019t expect wholesale changes to the stack based on expanded context windows, but we\u2019ll comment on this more in the body of the post.\n\nIf you\u2019d like to go deeper on in-context learning, there are a number of great resources in the AI canon (especially the \u201cPractical guides to building with LLMs\u201d section). In the remainder of this post, we\u2019ll walk through the reference stack, using the workflow above as a guide.\n\nContextual data for LLM apps includes text documents, PDFs, and even structured formats like CSV or SQL tables. Data-loading and transformation solutions for this data vary widely across developers we spoke with. Most use traditional ETL tools like Databricks or Airflow. Some also use document loaders built into orchestration frameworks like LangChain (powered by Unstructured) and LlamaIndex (powered by Llama Hub). We believe this piece of the stack is relatively underdeveloped, though, and there\u2019s an opportunity for data-replication solutions purpose-built for LLM apps.\n\nFor embeddings, most developers use the OpenAI API, specifically with the text-embedding-ada-002 model. It\u2019s easy to use (especially if you\u2019re already already using other OpenAI APIs), gives reasonably good results, and is becoming increasingly cheap. Some larger enterprises are also exploring Cohere, which focuses their product efforts more narrowly on embeddings and has better performance in certain scenarios. For developers who prefer open-source, the Sentence Transformers library from Hugging Face is a standard. It\u2019s also possible to create different types of embeddings tailored to different use cases; this is a niche practice today but a promising area of research.\n\nThe most important piece of the preprocessing pipeline, from a systems standpoint, is the vector database. It\u2019s responsible for efficiently storing, comparing, and retrieving up to billions of embeddings (i.e., vectors). The most common choice we\u2019ve seen in the market is Pinecone. It\u2019s the default because it\u2019s fully cloud-hosted\u2014so it\u2019s easy to get started with\u2014and has many of the features larger enterprises need in production (e.g., good performance at scale, SSO, and uptime SLAs).\n\nThere\u2019s a huge range of vector databases available, though. Notably:\n\nOpen source systems like Weaviate, Vespa, and Qdrant: They generally give excellent single-node performance and can be tailored for specific applications, so they are popular with experienced AI teams who prefer to build bespoke platforms.\n\nLocal vector management libraries like Chroma and Faiss: They have great developer experience and are easy to spin up for small apps and dev experiments. They don\u2019t necessarily substitute for a full database at scale.\n\nOLTP extensions like pgvector: For devs who see every database-shaped hole and try to insert Postgres\u2014or enterprises who buy most of their data infrastructure from a single cloud provider\u2014this is a good solution for vector support. It\u2019s not clear, in the long run, if it makes sense to tightly couple vector and scalar workloads.\n\nLooking ahead, most of the open source vector database companies are developing cloud offerings. Our research suggests achieving strong performance in the cloud, across a broad design space of possible use cases, is a very hard problem. Therefore, the option set may not change massively in the near term, but it likely will change in the long term. The key question is whether vector databases will resemble their OLTP and OLAP counterparts, consolidating around one or two popular systems.\n\nAnother open question is how embeddings and vector databases will evolve as the usable context window grows for most models. It\u2019s tempting to say embeddings will become less relevant, because contextual data can just be dropped into the prompt directly. However, feedback from experts on this topic suggests the opposite\u2014that the embedding pipeline may become more important over time. Large context windows are a powerful tool, but they also entail significant computational cost. So making efficient use of them becomes a priority. We may start to see different types of embedding models become popular, trained directly for model relevancy, and vector databases designed to enable and take advantage of this.\n\nStrategies for prompting LLMs and incorporating contextual data are becoming increasingly complex\u2014and increasingly important as a source of product differentiation. Most developers start new projects by experimenting with simple prompts, consisting of direct instructions (zero-shot prompting) or possibly some example outputs (few-shot prompting). These prompts often give good results but fall short of accuracy levels required for production deployments.\n\nThe next level of prompting jiu jitsu is designed to ground model responses in some source of truth and provide external context the model wasn\u2019t trained on. The Prompt Engineering Guide catalogs no fewer than 12 (!) more advanced prompting strategies, including chain-of-thought, self-consistency, generated knowledge, tree of thoughts, directional stimulus, and many others. These strategies can also be used in conjunction to support different LLM use cases like document question answering, chatbots, etc.\n\nThis is where orchestration frameworks like LangChain and LlamaIndex shine. They abstract away many of the details of prompt chaining; interfacing with external APIs (including determining when an API call is needed); retrieving contextual data from vector databases; and maintaining memory across multiple LLM calls. They also provide templates for many of the common applications mentioned above. Their output is a prompt, or series of prompts, to submit to a language model. These frameworks are widely used among hobbyists and startups looking to get an app off the ground, with LangChain the leader.\n\nLangChain is still a relatively new project (currently on version 0.0.201), but we\u2019re already starting to see apps built with it moving into production. Some developers, especially early adopters of LLMs, prefer to switch to raw Python in production to eliminate an added dependency. But we expect this DIY approach to decline over time for most use cases, in a similar way to the traditional web app stack.\n\nSharp-eyed readers will notice a seemingly weird entry in the orchestration box: ChatGPT. In its normal incarnation, ChatGPT is an app, not a developer tool. But it can also be accessed as an API. And, if you squint, it performs some of the same functions as other orchestration frameworks, such as: abstracting away the need for bespoke prompts; maintaining state; and retrieving contextual data via plugins, APIs, or other sources. While not a direct competitor to the other tools listed here, ChatGPT can be considered a substitute solution, and it may eventually become a viable, simple alternative to prompt construction.\n\nToday, OpenAI is the leader among\n\n. Nearly every developer we spoke with starts new LLM apps using the OpenAI API, usually with the\n\ngpt-4\n\nor\n\ngpt-4-32k\n\nmodel. This gives a best-case scenario for app performance and is easy to use, in that it operates on a wide range of input domains and usually requires no fine-tuning or self-hosting.\n\nWhen projects go into production and start to scale, a broader set of options come into play. Some of the common ones we heard include:\n\nSwitching to gpt-3.5-turbo: It\u2019s ~50x cheaper and significantly faster than GPT-4. Many apps don\u2019t need GPT-4-level accuracy, but do require low latency inference and cost effective support for free users.\n\nExperimenting with other proprietary vendors (especially Anthropic\u2019s Claude models): Claude offers fast inference, GPT-3.5-level accuracy, more customization options for large customers, and up to a 100k context window (though we\u2019ve found accuracy degrades with the length of input).\n\nTriaging some requests to open source models: This can be especially effective in high-volume B2C use cases like search or chat, where there\u2019s wide variance in query complexity and a need to serve free users cheaply.\n\nThis usually makes the most sense in conjunction with fine-tuning open source base models. We don\u2019t go deep on that tooling stack in this article, but platforms like Databricks, Anyscale, Mosaic, Modal, and RunPod are used by a growing number of engineering teams.\nA variety of inference options are available for open source models, including simple API interfaces from Hugging Face and Replicate; raw compute resources from the major cloud providers; and more opinionated cloud offerings like those listed above.\n\nOpen-source models trail proprietary offerings right now, but the gap is starting to close. The LLaMa models from Meta set a new bar for open source accuracy and kicked off a flurry of variants. Since LLaMa was licensed for research use only, a number of new providers have stepped in to train alternative base models (e.g., Together, Mosaic, Falcon, Mistral). Meta is also debating a truly open source release of LLaMa 2.\n\nWhen (not if) open source LLMs reach accuracy levels comparable to GPT-3.5, we expect to see a Stable Diffusion-like moment for text\u2014including massive experimentation, sharing, and productionizing of fine-tuned models. Hosting companies like Replicate are already adding tooling to make these models easier for software developers to consume. There\u2019s a growing belief among developers that smaller, fine-tuned models can reach state-of-the-art accuracy in narrow use cases.\n\nMost developers we spoke with haven\u2019t gone deep on operational tooling for LLMs yet. Caching is relatively common\u2014usually based on Redis\u2014because it improves application response times and cost. Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models. There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff). Most of these operational tools encourage use of their own Python clients to make LLM calls, so it will be interesting to see how these solutions coexist over time.\n\nFinally, the static portions of LLM apps (i.e. everything other than the model) also need to be hosted somewhere. The most common solutions we\u2019ve seen so far are standard options like Vercel or the major cloud providers. However, two new categories are emerging. Startups like Steamship provide end-to-end hosting for LLM apps, including orchestration (LangChain), multi-tenant data contexts, async tasks, vector storage, and key management. And companies like Anyscale and Modal allow developers to host models and Python code in one place.\n\nWhat about agents?\n\nThe most important components missing from this reference architecture are AI agent frameworks. AutoGPT, described as \u201can experimental open-source attempt to make GPT-4 fully autonomous,\u201d was the fastest-growing Github repo in history this spring, and practically every AI project or startup out there today includes agents in some form.\n\nMost developers we speak with are incredibly excited about the potential of agents. The in-context learning pattern we describe in this post is effective at solving hallucination and data-freshness problems, in order to better support content-generation tasks. Agents, on the other hand, give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment. They do this through a combination of advanced reasoning/planning, tool usage, and memory / recursion / self-reflection.\n\nSo, agents have the potential to become a central piece of the LLM app architecture (or even take over the whole stack, if you believe in recursive self-improvement). And existing frameworks like LangChain have incorporated some agent concepts already. There\u2019s only one problem: agents don\u2019t really work yet. Most agent frameworks today are in the proof-of-concept phase\u2014capable of incredible demos but not yet reliable, reproducible task-completion. We\u2019re keeping an eye on how they develop in the near future.\n\nLooking ahead\n\nPre-trained AI models represent the most important architectural change in software since the internet. They make it possible for individual developers to build incredible AI apps, in a matter of days, that surpass supervised machine learning projects that took big teams months to build.\n\nThe tools and patterns we\u2019ve laid out here are likely the starting point, not the end state, for integrating LLMs. We\u2019ll update this as major changes take place (e.g., a shift toward model training) and release new reference architectures where it makes sense. Please reach out if you have any feedback or suggestions.\n\n* *\n\nThe views expressed here are those of the individual AH Capital Management, L.L.C. (\u201ca16z\u201d) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein.\n\nThis content is provided for informational purposes only, and should not be relied upon as legal, business, investment, or tax advice. You should consult your own advisers as to those matters. References to any securities or digital assets are for illustrative purposes only, and do not constitute an investment recommendation or offer to provide investment advisory services. Furthermore, this content is not directed at nor intended for use by any investors or prospective investors, and may not under any circumstances be relied upon when making a decision to invest in any fund managed by a16z. (An offering to invest in an a16z fund will be made only by the private placement memorandum, subscription agreement, and other relevant documentation of any such fund and should be read in their entirety.) Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z, and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by Andreessen Horowitz (excluding investments for which the issuer has not provided permission for a16z to disclose publicly as well as unannounced investments in publicly traded digital assets) is available at\u00a0https://a16z.com/investments/.\n\nCharts and graphs provided within are for informational purposes solely and should not be relied upon when making any investment decision. Past performance is not indicative of future results. The content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Please see\u00a0https://a16z.com/disclosures\u00a0for additional important information.\n\nJune 20, 2023\n\nRelated Stories\n\nAI + a16z        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\ta16z editorial\n\nAI Glossary by a16z        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\ta16z editorial\n\nAI Canon        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\tDerrick Harris, \t\n\t\t\tMatt Bornstein, and \t\n\t\t\tGuido Appenzeller\n\nAI at the Intersection: The a16z Investment Thesis on AI in Bio + Health        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\tVijay Pande\n\nThe Getting Started with AI Stack for JavaScript        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\tYoko Li, \t\n\t\t\tJennifer Li, and \t\n\t\t\tMartin Casado\n\nRelated Stories\n\nAI + a16z        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\ta16z editorial\n\nAI Glossary by a16z        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\ta16z editorial\n\nAI Canon        \n\t    \n\n\t\n\t\tby\t\n\t\n\t\t\n\t\t\tDerrick Harris, \t\n\t\t\tMatt Bornstein, and \t\n\t\t\tGuido Appenzeller",
        "metadata": {
            "source": "https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/"
        }
    },
    {
        "page_content": "The New Language Model Stack\n\nHow companies are bringing AI applications to life\n\nBy Michelle Fradin and Lauren Reeder\n\nChatGPT unleashed a tidal wave of innovation with large language models (LLMs). More companies than ever before are bringing the power of natural language interaction to their products. The adoption of language model APIs is creating a new stack in its wake. To better understand the applications people are building and the stacks they are using to do so, we spoke with 33 companies across the Sequoia network, from seed stage startups to large public enterprises. We spoke with them two months ago and last week to capture the pace of change. As many founders and builders are in the midst of figuring out their AI strategies themselves, we wanted to share our findings even as this space is rapidly evolving.\n\n1. Nearly every company in the Sequoia network is building language models into their products.\u00a0We\u2019ve seen magical auto-complete features for everything from code (Sourcegraph, Warp, Github) to data science (Hex). We\u2019ve seen better chatbots for everything from customer support to employee support to consumer entertainment. Others are reimagining entire workflows with an AI-first lens: visual art (Midjourney), marketing (Hubspot, Attentive, Drift, Jasper, Copy, Writer), sales (Gong), contact centers (Cresta), legal (Ironclad, Harvey), accounting (Pilot), productivity (Notion), data engineering (dbt), search (Glean, Neeva), grocery shopping (Instacart), consumer payments (Klarna), and travel planning (Airbnb). These are just a few examples and they\u2019re only the beginning.\n\n2. The new stack for these applications centers on language model APIs, retrieval, and orchestration, but open source usage is also growing.\n\n65% had applications in production, up from 50% two months ago, while the remainder are still experimenting.\n\n94% are using a foundation model API. OpenAI\u2019s GPT was the clear favorite in our sample at 91%, however Anthropic interest grew over the last quarter to 15%. (Some companies are using multiple models).\n\n88% believe a retrieval mechanism, such as a vector database, would remain a key part of their stack. Retrieving relevant context for a model to reason about helps increase the quality of results, reduce \u201challucinations\u201d (inaccuracies), and solve data freshness issues. Some use purpose-built vector databases (Pinecone, Weaviate, Chroma, Qdrant, Milvus, and many more), while others use pgvector or AWS offerings.\n\n38% were interested in an LLM orchestration and application development framework like LangChain. Some use it for prototyping, while others use it in production. Adoption increased in the last few months.\n\nSub-10% were looking for tools to monitor LLM outputs, cost, or performance and A/B test prompts. We think interest in these areas may increase as more large companies and regulated industries adopt language models.\n\nA handful of companies are looking into complementary generative technologies, such as combining generative text and voice. We also believe this is an exciting growth area.\n\n15% built custom language models from scratch or open source, often in addition to using LLM APIs. Custom model training increased meaningfully from a few months ago. This requires its own stack of compute, model hub, hosting, training frameworks, experiment tracking and more from beloved companies like Hugging Face, Replicate, Foundry, Tecton, Weights & Biases, PyTorch, Scale, and more.\n\nEvery practitioner we spoke with said AI is moving too quickly to have high confidence in the end-state stack, but there was consensus that LLM APIs will remain a key pillar, followed in popularity by retrieval mechanisms and development frameworks like LangChain. Open source and custom model training and tuning also seem to be on the rise. Other areas of the stack are important, but earlier in maturity.\n\n3. Companies want to customize language models to their unique context.\u00a0Generalized language models are powerful, but not differentiating or sufficient for many use cases. Companies want to enable natural language interactions on their data\u2014their developer docs, product inventory, HR or IT rules, etc. In some cases, companies want to customize their models to their users\u2019 data as well: your personal notes, design layouts, data metrics or code base.\n\nRight now, there are three main ways to customize language models (for a deeper technical explanation, see Andrej\u2019s recent State of GPT talk at Microsoft Build):\n\nTrain a custom model from scratch. Highest degree of difficulty.\u00a0This is the classical and hardest way to solve this problem. It typically requires highly skilled ML scientists, lots of relevant data, training infrastructure and compute. This is one of the primary reasons why historically much natural language processing innovation occurred within mega-cap tech companies. BloombergGPT is a great example of a custom model effort outside of a mega-cap tech company, which used resources on Hugging Face and other open source tooling. As open source tooling improves and more companies innovate with LLMs, we expect to see more custom and pre-trained model usage.\n\nFine-tune a base model. Medium degree of difficulty.\u00a0This is updating the weights of a pre-trained model through additional training with further proprietary or domain-specific data. Open source innovation is also making this approach increasingly accessible, but it still often requires a sophisticated team. Some practitioners privately admit fine-tuning is much harder than it sounds and can have unintended consequences like model drift and \u201cbreaking\u201d the model\u2019s other skills without warning. While this approach has a greater chance of becoming more common, it is currently still out of reach for most companies. But again, this is changing quickly.\n\nUse a pre-trained model and retrieve relevant context. Lowest degree of difficulty.\u00a0People often think they want a model fine-tuned just for them, when really they just want the model to reason about their information at the right time. There are many ways to provide the model the right information at the right time: make a structured query to a SQL database, search across a product catalog, call some external API or use embeddings retrieval. The benefit of embeddings retrieval is that it makes unstructured data easily searchable using natural language. Technically, this is done by taking data, turning it into embeddings, storing those in a vector database, and when a query occurs, searching those embeddings for the most relevant context, and providing that to the model. This approach helps you hack the model\u2019s limited context window, is less expensive, solves the data freshness problem (e.g. ChatGPT doesn\u2019t know about the world after September 2021), and it can be done by a solo developer without formal machine learning training. Vector databases are useful because at high scale they make storing, searching and updating embeddings easier. So far, we\u2019ve observed larger companies stay within their enterprise cloud agreements and use tools from their cloud provider, while startups tend to use purpose-built vector databases. However, this space is highly dynamic. Context windows are growing (hot off the presses, OpenAI just expanded to 16K, and Anthropic has launched a 100K token context window). Foundational models and cloud databases may embed retrieval directly into their services. We\u2019re watching closely as this market evolves.\n\n4. Today, the stack for LLM APIs can feel separate from the custom model training stack, but these are blending together over time.It can sometimes feel like we have a tale of two stacks: the stack to leverage LLM APIs (more closed source, and geared towards developers) versus the stack to train custom language models (more open source, and historically geared towards more sophisticated machine learning teams). Some have wondered whether LLMs being readily available via API meant companies would do less of their own custom training. So far, we\u2019re seeing the opposite. As interest in AI grows and open source development accelerates, many companies become increasingly interested in training and fine-tuning their own models. We think the LLM API and custom model stacks will increasingly converge over time. For example, a company might train its own language model from open source, but supplement with retrieval via a vector database to solve data freshness issues. Smart startups building tools for the custom model stack are also working on extending their products to become more relevant to the LLM API revolution.\n\n5. The stack is becoming increasingly developer-friendly.Language model APIs put powerful ready-made models in the hands of the average developer, not just machine learning teams. Now that the population working with language models has meaningfully expanded to all developers, we believe we\u2019ll see more developer-oriented tooling. For example, LangChain helps developers build LLM applications by abstracting away commonly occurring problems: combining models into higher-level systems, chaining together multiple calls to models, connecting models to tools and data sources, building agents that can operate those tools, and helping avoid vendor lock-in by making it easier to switch language models. Some use LangChain for prototyping, while others continue to use it in production.\n\n6. Language models need to become more trustworthy (output quality, data privacy, security) for full adoption.\u00a0Before fully unleashing LLMs in their applications, many companies want better tools for handling data privacy, segregation, security, copyright, and monitoring model outputs. Companies in regulated industries from fintech to healthcare are especially focused on this. They are asking for software to alert, or ideally prevent, models from generating errors/hallucinations, discriminatory content, dangerous content, or exposing new security vulnerabilities. Robust Intelligence in particular has been tackling many of these challenges, with customers including Paypal, Expedia and others. Some companies are also concerned about how data shared with models is used for training: for instance, few understand that ChatGPT Consumer data is default used for training, while ChatGPT Business and the API data are not. As policies get clarified and more guardrails go into place, language models will be better trusted, and we may see another step change in adoption.\n\n7. Language model applications will become increasingly multi-modal.\u00a0Companies are already finding interesting ways to combine multiple generative models to great effect: Chatbots that combine text and speech generation unlock a new level of conversational experience. Text and voice models can be combined to help you to quickly overdub a video recording mistake instead of re-recording the whole thing. Models themselves are becoming increasingly multi-modal. We can imagine a future of rich consumer and enterprise AI applications that combine text, speech/audio, and image/video generation to create more engaging user experiences and accomplish more complex tasks.\n\n8. It\u2019s still early.\u00a0AI is just beginning to seep into every crevice of technology. Only 65% of those surveyed were in production today, and many of these are relatively simple applications. As more companies launch LLM applications, new hurdles will arise\u2014creating more opportunities for founders. The infrastructure layer will continue to evolve rapidly for the next several years. If only half the demos we see make it to production, we\u2019re in for an exciting ride ahead. It\u2019s thrilling to see founders from our earliest-stage Arc investment to Zoom all laser focused on the same thing: delighting users with AI.\n\nIf you\u2019re founding a company that will become a key pillar of the language model stack or an AI-first application, Sequoia would love to meet you.\u00a0Ambitious founders can increase their odds of success by applying to Arc, our catalyst for pre-seed and seed stage companies. Applications for Arc America Fall \u201923 are now open\u2014apply\u00a0here.\n\nThank you to all the founders and builders who contributed to this work, and Sequoia Partners Charlie Curnin, Pat Grady, Sonya Huang, Andrew Reed, Bill Coughran and friends at OpenAI for their input and review.\n\nShare\n\nShare this on Facebook\n\nShare this on Twitter\n\nShare this on LinkedIn\n\nShare this via email\n\nRelated Topics\n\n#AI\n\nDeveloper Tools 2.0\n\n\tBy Charlie Curnin, Josephine Chen and Stephanie Zhan\n\nPerspective\n\nRead\n\nAgents on the Brain\n\n\tBy Lauren Reeder, Cornelius Menke and Stephanie Zhan\n\nPerspective\n\nRead\n\nGenerative AI: A Creative New World\n\n\tBy Sonya Huang, Pat Grady and GPT-3\n\nPerspective\n\nRead\n\nGenerative AI Is Exploding. These Are The Most Important Trends To Know\n\n\tKonstantine Buhler on the 2023 AI 50 list\n\nPerspective\n\nRead\n\nJOIN OUR MAILING LIST\n\nGet the best stories from the Sequoia community.\n\nEmail address",
        "metadata": {
            "source": "https://www.sequoiacap.com/article/llm-stack-perspective/"
        }
    },
    {
        "page_content": "Building LLM applications for production\n\nApr 11, 2023\n      \n      \n        \u2022 Chip Huyen\n\n[Hacker News discussion, LinkedIn discussion, Twitter thread]\n\nA question that I\u2019ve been asked a lot recently is how large language models (LLMs) will change machine learning workflows. After working with several companies who are working with LLM applications and personally going down a rabbit hole building my applications, I realized two things:\n\nIt\u2019s easy to make something cool with LLMs, but very hard to make something production-ready with them.\n\nLLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field.\n\nThis post consists of three parts.\n\nPart 1 discusses the key challenges of productionizing LLM applications and the solutions that I\u2019ve seen.\n\nPart 2 discusses how to compose multiple tasks with control flows (e.g. if statement, for loop) and incorporate tools (e.g. SQL executor, bash, web browsers, third-party APIs) for more complex and powerful applications.\n\nPart 3 covers some of the promising use cases that I\u2019ve seen companies building on top of LLMs and how to construct them from smaller tasks.\n\nThere has been so much written about LLMs, so feel free to skip any section you\u2019re already familiar with.\n\nPart I. Challenges of productionizing prompt engineering\n\nThe ambiguity of natural languages\n\nPrompt evaluation\n\nPrompt versioning\n\nPrompt optimization\n\nCost and latency\n\nCost\n\nLatency\n\nThe impossibility of cost + latency analysis for LLMs\n\nPrompting vs. finetuning vs. alternatives\n\nPrompt tuning\n\nFinetuning with distillation\n\nEmbeddings + vector databases\n\nBackward and forward compatibility\n\nPart 2. Task composability\n\nApplications that consist of multiple tasks\n\nAgents, tools, and control flows\n\nTools vs. plugins\n\nControl flows: sequential, parallel, if, for loop\n\nControl flow with LLM agents\n\nTesting an agent\n\nPart 3. Promising use cases\n\nAI assistant\n\nChatbot\n\nProgramming and gaming\n\nLearning\n\nTalk-to-your-data\n\nCan LLMs do data analysis for me?\n\nSearch and recommendation\n\nSales\n\nSEO\n\nConclusion\n\nPart I. Challenges of productionizing prompt engineering\n\nThe ambiguity of natural languages\n\nFor most of the history of computers, engineers have written instructions in programming languages. Programming languages are \u201cmostly\u201d exact. Ambiguity causes frustration and even passionate hatred in developers (think dynamic typing in Python or JavaScript).\n\nIn prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience.\n\nThe flexibility comes from two directions: how users define instructions, and how LLMs respond to these instructions.\n\nFirst, the flexibility in user-defined prompts leads to silent failures. If someone accidentally makes some changes in code, like adding a random character or removing a line, it\u2019ll likely throw an error. However, if someone accidentally changes a prompt, it will still run but give very different outputs.\n\nWhile the flexibility in user-defined prompts is just an annoyance, the ambiguity in LLMs\u2019 generated responses can be a dealbreaker. It leads to two problems:\n\nAmbiguous output format: downstream applications on top of LLMs expect outputs in a certain format so that they can parse. We can craft our prompts to be explicit about the output format, but there\u2019s no guarantee that the outputs will always follow this format.\n\nInconsistency in user experience: when using an application, users expect certain consistency. Imagine an insurance company giving you a different quote every time you check on their website. LLMs are stochastic \u2013 there\u2019s no guarantee that an LLM will give you the same output for the same input every time.\n\n    You can force an LLM to give the same response by setting temperature = 0, which is, in general, a good practice. While it mostly solves the consistency problem, it doesn\u2019t inspire trust in the system. Imagine a teacher who gives you consistent scores only if that teacher sits in one particular room. If that teacher sits in different rooms, that teacher\u2019s scores for you will be wild.\n\nHow to solve this ambiguity problem?\n\nThis seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models\u2019 reliability.\n\nA couple of people who\u2019ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It\u2019s a different mindset compared to developing deterministic programs, but not something impossible to get used to.\n\nThis ambiguity can be mitigated by applying as much engineering rigor as possible. In the rest of this post, we\u2019ll discuss how to make prompt engineering, if not deterministic, systematic.\n\nPrompt evaluation\n\nA common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners).\n\nAs an example, consider trying to give a text a controversy score \u2013 it was a fun project that I did to find the correlation between a tweet\u2019s popularity and its controversialness. Here is the shortened prompt with 4 fewshot examples:\n\nExample: controversy scorer\n\nWhen doing fewshot learning, two questions to keep in mind:\n\nWhether the LLM understands the examples given in the prompt. One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn\u2019t perform well on the same examples given in the prompt, it is likely because the prompt isn\u2019t clear \u2013 you might want to rewrite the prompt or break the task into smaller tasks (and combine them together, discussed in detail in Part II of this post).\n\nWhether the LLM overfits to these fewshot examples. You can evaluate your model on separate examples.\n\nOne thing I\u2019ve also found useful is to ask models to give examples for which it would give a certain label. For example, I can ask the model to give me examples of texts for which it\u2019d give a score of 4. Then I\u2019d input these examples into the LLM to see if it\u2019ll indeed output 4.\n\nPrompt versioning\n\nSmall changes to a prompt can lead to very different results. It\u2019s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn\u2019t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments.\n\nPrompt optimization\n\nThere have been many papers + blog posts written on how to optimize prompts. I agree with Lilian Weng in her helpful blog post that most papers on prompt engineering are tricks that can be explained in a few sentences. OpenAI has a great notebook that explains many tips with examples. Here are some of them:\n\nPrompt the model to explain or explain step-by-step how it arrives at an answer, a technique known as Chain-of-Thought or COT (Wei et al., 2022). Tradeoff: COT can increase both latency and cost due to the increased number of output tokens [see Cost and latency section]\n\nGenerate many outputs for the same input. Pick the final output by either the majority vote  (also known as self-consistency technique by Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argument n (not an ideal API design if you ask me).\n\nBreak one big prompt into smaller, simpler prompts.\n\nMany tools promise to auto-optimize your prompts \u2013 they are quite expensive and usually just apply these tricks. One nice thing about these tools is that they\u2019re no code, which makes them appealing to non-coders.\n\nCost and latency\n\nCost\n\nThe more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost.\n\nOpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone.\n\nThe cost with long prompts isn\u2019t in experimentation but in inference.\n\nExperimentation-wise, prompt engineering is a cheap and fast way get something up and running. For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer.\n\nPrompt: 10k tokens ($0.06/1k tokens)\n\nOutput: 200 tokens ($0.12/1k tokens)\n\nEvaluate on 20 examples\n\nExperiment with 25 different versions of prompts\n\nThe cost of LLMOps is in inference.\n\nIf you use GPT-4 with 10k tokens in input and 200 tokens in output, it\u2019ll be $0.624 / prediction.\n\nIf you use GPT-3.5-turbo with 4k tokens for both input and output, it\u2019ll be $0.004 / prediction or $4 / 1k predictions.\n\nAs a thought exercise, in 2021, DoorDash ML models made 10 billion predictions a day. If each prediction costs $0.004, that\u2019d be $40 million a day!\n\nBy comparison, AWS personalization costs about $0.0417 / 1k predictions and AWS fraud detection costs about $7.5 / 1k predictions [for over 100,000 predictions a month]. AWS services are usually considered prohibitively expensive (and less flexible) for any company of a moderate scale.\n\nLatency\n\nInput tokens can be processed in parallel, which means that input length shouldn\u2019t affect the latency that much.\n\nHowever, output length significantly affects latency, which is likely due to output tokens being generated sequentially.\n\nEven for extremely short input (51 tokens) and output (1 token), the latency for gpt-3.5-turbo is around 500ms. If the output token increases to over 20 tokens, the latency is over 1 second.\n\nHere\u2019s an experiment I ran, each setting is run 20 times. All runs happen within 2 minutes. If I do the experiment again, the latency will be very different, but the relationship between the 3 settings should be similar.\n\nThis is another challenge of productionizing LLM applications using APIs like OpenAI: APIs are very unreliable, and no commitment yet on when SLAs will be provided.\n\n# tokens\n\np50 latency (sec)\n\np75 latency\n\np90 latency\n\ninput: 51 tokens, output: 1 token\n\n0.58\n\n0.63\n\n0.75\n\ninput: 232 tokens, output: 1 token\n\n0.53\n\n0.58\n\n0.64\n\ninput: 228 tokens, output: 26 tokens\n\n1.43\n\n1.49\n\n1.62\n\nIt is, unclear, how much of the latency is due to model, networking (which I imagine is huge due to high variance across runs), or some just inefficient engineering overhead. It\u2019s very possible that the latency will reduce significantly in a near future.\n\nWhile half a second seems high for many use cases, this number is incredibly impressive given how big the model is and the scale at which the API is being used. The number of parameters for gpt-3.5-turbo isn\u2019t public but is guesstimated to be around 150B. As of writing, no open-source model is that big. Google\u2019s T5 is 11B parameters and Facebook\u2019s largest LLaMA model is 65B parameters. People discussed on this GitHub thread what configuration they needed to make LLaMA models work, and it seemed like getting the 30B parameter model to work is hard enough. The most successful one seemed to be randaller who was able to get the 30B parameter model work on 128 GB of RAM, which takes a few seconds just to generate one token.\n\nThe impossibility of cost + latency analysis for LLMs\n\nThe LLM application world is moving so fast that any cost + latency analysis is bound to go outdated quickly. Matt Ross, a senior manager of applied research at Scribd, told me that the estimated API cost for his use cases has gone down two orders of magnitude over the last year. Latency has significantly decreased as well. Similarly, many teams have told me they feel like they have to redo the feasibility estimation and buy (using paid APIs) vs. build (using open source models) decision every week.\n\nPrompting vs. finetuning vs. alternatives\n\nPrompting: for each sample, explicitly tell your model how it should respond.\n\nFinetuning: train a model on how to respond, so you don\u2019t have to specify that in your prompt.\n\nThere are 3 main factors when considering prompting vs. finetuning: data availability, performance, and cost.\n\nIf you have only a few examples, prompting is quick and easy to get started. There\u2019s a limit to how many examples you can include in your prompt due to the maximum input token length.\n\nThe number of examples you need to finetune a model to your task, of course, depends on the task and the model. In my experience, however, you can expect a noticeable change in your model performance if you finetune on 100s examples. However, the result might not be much better than prompting.\n\nIn How Many Data Points is a Prompt Worth? (2021), \u200b\u200bScao and Rush found that a prompt is worth approximately 100 examples (caveat: variance across tasks and models is high \u2013 see image below). The general trend is that as you increase the number of examples, finetuning will give better model performance than prompting. There\u2019s no limit to how many examples you can use to finetune a model.\n\nThe benefit of finetuning is two folds:\n\nYou can get better model performance: can use more examples, examples becoming part of the model\u2019s internal knowledge.\n\nYou can reduce the cost of prediction. The more instruction you can bake into your model, the less instruction you have to put into your prompt. Say, if you can reduce 1k tokens in your prompt for each prediction, for 1M predictions on gpt-3.5-turbo, you\u2019d save $2000.\n\nPrompt tuning\n\nA cool idea that is between prompting and finetuning is prompt tuning, introduced by Leister et al. in 2021. Starting with a prompt, instead of changing this prompt, you programmatically change the embedding of this prompt. For prompt tuning to work, you need to be able to input prompts\u2019 embeddings into your LLM model and generate tokens from these embeddings, which currently, can only be done with open-source LLMs and not in OpenAI API. On T5, prompt tuning appears to perform much better than prompt engineering and can catch up with model tuning (see image below).\n\nFinetuning with distillation\n\nIn March 2023, a group of Stanford students released a promising idea: finetune a smaller open-source language model (LLaMA-7B, the 7 billion parameter version of LLaMA) on examples generated by a larger language model (text-davinci-003 \u2013 175 billion parameters). This technique of training a small model to imitate the behavior of a larger model is called distillation. The resulting finetuned model behaves similarly to text-davinci-003, while being a lot smaller and cheaper to run.\n\nFor finetuning, they used 52k instructions, which they inputted into text-davinci-003 to obtain outputs, which are then used to finetune LLaMa-7B. This costs under $500 to generate. The training process for finetuning costs under $100. See Stanford Alpaca: An Instruction-following LLaMA Model (Taori et al., 2023).\n\nThe appeal of this approach is obvious. After 3 weeks, their GitHub repo got almost 20K stars!! By comparison, HuggingFace\u2019s transformers repo took over a year to achieve a similar number of stars, and TensorFlow repo took 4 months.\n\nEmbeddings + vector databases\n\nOne direction that I find very promising is to use LLMs to generate embeddings and then build your ML applications on top of these embeddings, e.g. for search and recsys. As of April 2023, the cost for embeddings using the smaller model text-embedding-ada-002 is $0.0004/1k tokens. If each item averages 250 tokens (187 words), this pricing means $1 for every 10k items or $100 for 1 million items.\n\nWhile this still costs more than some existing open-source models, this is still very affordable, given that:\n\nYou usually only have to generate the embedding for each item once.\n\nWith OpenAI API, it\u2019s easy to generate embeddings for queries and new items in real-time.\n\nTo learn more about using GPT embeddings, check out SGPT (Niklas Muennighoff, 2022) or this analysis on the performance and cost GPT-3 embeddings (Nils Reimers, 2022). Some of the numbers in Nils\u2019 post are already outdated (the field is moving so fast!!), but the method is great!\n\nThe main cost of embedding models for real-time use cases is loading these embeddings into a vector database for low-latency retrieval. However, you\u2019ll have this cost regardless of which embeddings you use. It\u2019s exciting to see so many vector databases blossoming \u2013 the new ones such as Pinecone, Qdrant, Weaviate, Chroma as well as the incumbents Faiss, Redis, Milvus, ScaNN.\n\nIf 2021 was the year of graph databases, 2023 is the year of vector databases.\n\nBackward and forward compatibility\n\nHacker News discussion: Who is working on forward and backward compatibility for LLMs?\n\nFoundational models can work out of the box for many tasks without us having to retrain them as much. However, they do need to be retrained or finetuned from time to time as they go outdated. According to Lilian Weng\u2019s Prompt Engineering post:\n\nOne observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.\n\nIn traditional software, when software gets an update, ideally it should still work with the code written for its older version. However, with prompt engineering, if you want to use a newer model, there\u2019s no way to guarantee that all your prompts will still work as intended with the newer model, so you\u2019ll likely have to rewrite your prompts again. If you expect the models you use to change at all, it\u2019s important to unit-test all your prompts using evaluation examples.\n\nOne argument I often hear is that prompt rewriting shouldn\u2019t be a problem because:\n\nNewer models should only work better than existing models. I\u2019m not convinced about this. Newer models might, overall, be better, but there will be use cases for which newer models are worse.\n\nExperiments with prompts are fast and cheap, as we discussed in the section Cost. While I agree with this argument, a big challenge I see in MLOps today is that there\u2019s a lack of centralized knowledge for model logic, feature logic, prompts, etc. An application might contain multiple prompts with complex logic (discussed in Part 2. Task composability). If the person who wrote the original prompt leaves, it might be hard to understand the intention behind the original prompt to update it. This can become similar to the situation when someone leaves behind a 700-line SQL query that nobody dares to touch.\n\nAnother challenge is that prompt patterns are not robust to changes. For example, many of the published prompts I\u2019ve seen start with \u201cI want you to act as XYZ\u201d. If OpenAI one day decides to print something like: \u201cI\u2019m an AI assistant and I can\u2019t act like XYZ\u201d, all these prompts will need to be updated.\n\nPart 2. Task composability\n\nApplications that consist of multiple tasks\n\nThe example controversy scorer above consists of one single task: given an input, output a controversy score. Most applications, however, are more complex. Consider the \u201ctalk-to-your-data\u201d use case where we want to connect to a database and query this database in natural language. Imagine a credit card transaction table. You want to ask things like: \"How many unique merchants are there in Phoenix and what are their names?\" and your database will return: \"There are 9 unique merchants in Phoenix and they are \u2026\".\n\nOne way to do this is to write a program that performs the following sequence of tasks:\n\nTask 1: convert natural language input from user to SQL query [LLM]\n\nTask 2: execute SQL query in the SQL database [SQL executor]\n\nTask 3: convert the SQL result into a natural language response to show user [LLM]\n\nAgents, tools, and control flows\n\nI did a small survey among people in my network and there doesn\u2019t seem to be any consensus on terminologies, yet.\n\nThe word agent is being thrown around a lot to refer to an application that can execute multiple tasks according to a given control flow (see Control flows section). A task can leverage one or more tools. In the example above, SQL executor is an example of a tool.\n\nNote: some people in my network resist using the term agent in this context as it is already overused in other contexts (e.g. agent to refer to a policy in reinforcement learning).\n\nTools vs. plugins\n\nOther than SQL executor, here are more examples of tools:\n\nsearch (e.g. by using Google Search API or Bing API)\n\nweb browser (e.g. given a URL, fetch its content)\n\nbash executor\n\ncalculator\n\nTools and plugins are basically the same things. You can think of plugins as tools contributed to the OpenAI plugin store. As of writing, OpenAI plugins aren\u2019t open to the public yet, but anyone can create and use tools.\n\nControl flows: sequential, parallel, if, for loop\n\nIn the example above, sequential is an example of a control flow in which one task is executed after another. There are other types of control flows such as parallel, if statement, for loop.\n\nSequential: executing task B after task A completes, likely because task B depends on Task A. For example, the SQL query can only be executed after it\u2019s been translated from the user input.\n\nParallel: executing tasks A and B at the same time.\n\nIf statement: executing task A or task B depending on the input.\n\nFor loop: repeat executing task A until a certain condition is met. For example, imagine you use browser action to get the content of a webpage and keep on using browser action to get the content of links found in that webpage until the agent feels like it\u2019s got sufficient information to answer the original question.\n\nNote: while parallel can definitely be useful, I haven\u2019t seen a lot of applications using it.\n\nControl flow with LLM agents\n\nIn traditional software engineering, conditions for control flows are exact. With LLM applications (also known as agents), conditions might also be determined by prompting.\n\nFor example, if you want your agent to choose between three actions search, SQL executor, and Chat, you might explain how it should choose one of these actions as follows (very approximate), In other words, you can use LLMs to decide the condition of the control flow!\n\nTesting an agent\n\nFor agents to be reliable, we\u2019d need to be able to build and test each task separately before combining them. There are two major types of failure modes:\n\nOne or more tasks fail. Potential causes:\n    \n      Control flow is wrong: a non-optional action is chosen\n      One or more tasks produce incorrect results\n\nAll tasks produce correct results but the overall solution is incorrect. Press et al. (2022) call this \u201ccomposability gap\u201d: the fraction of compositional questions that the model answers incorrectly out of all the compositional questions for which the model answers the sub-questions correctly.\n\nLike with software engineering, you can and should unit test each component as well as the control flow. For each component, you can define pairs of (input, expected output) as evaluation examples, which can be used to evaluate your application every time you update your prompts or control flows. You can also do integration tests for the entire application.\n\nPart 3. Promising use cases\n\nThe Internet has been flooded with cool demos of applications built with LLMs. Here are some of the most common and promising applications that I\u2019ve seen. I\u2019m sure that I\u2019m missing a ton.\n\nFor more ideas, check out the projects from two hackathons I\u2019ve seen:\n\nGPT-4 Hackathon Code Results [Mar 25, 2023]\n\nLangchain / Gen Mo Hackathon [Feb 25, 2023]\n\nAI assistant\n\nThis is hands down the most popular consumer use case. There are AI assistants built for different tasks for different groups of users \u2013 AI assistants for scheduling, making notes, pair programming, responding to emails, helping with parents, making reservations, booking flights, shopping, etc. \u2013 but, of course, the ultimate goal is an assistant that can assist you in everything.\n\nThis is also the holy grail that all big companies are working towards for years: Google with Google Assistant and Bard, Facebook with M and Blender, OpenAI (and by extension, Microsoft) with ChatGPT. Quora, which has a very high risk of being replaced by AIs, released their own app Poe that lets you chat with multiple LLMs. I\u2019m surprised Apple and Amazon haven\u2019t joined the race yet.\n\nChatbot\n\nChatbots are similar to AI assistants in terms of APIs. If AI assistants\u2019 goal is to fulfill tasks given by users, whereas chatbots\u2019 goal is to be more of a companion. For example, you can have chatbots that talk like celebrities, game/movie/book characters, businesspeople, authors, etc.\n\nMichelle Huang used her childhood journal entries as part of the prompt to GPT-3 to talk to the inner child.\n\nThe most interesting company in the consuming-chatbot space is probably Character.ai. It\u2019s a platform for people to create and share chatbots. The most popular types of chatbots on the platform, as writing, are anime and game characters, but you can also talk to a psychologist, a pair programming partner, or a language practice partner. You can talk, act, draw pictures, play text-based games (like AI Dungeon), and even enable voices for characters. I tried a few popular chatbots \u2013 none of them seem to be able to hold a conversation yet, but we\u2019re just at the beginning. Things can get even more interesting if there\u2019s a revenue-sharing model so that chatbot creators can get paid.\n\nProgramming and gaming\n\nThis is another popular category of LLM applications, as LLMs turn out to be incredibly good at writing and debugging code. GitHub Copilot is a pioneer (whose VSCode extension has had 5 million downloads as of writing). There have been pretty cool demos of using LLMs to write code:\n\nCreate web apps from natural languages\n\nFind security threats: Socket AI examines npm and PyPI packages in your codebase for security threats. When a potential issue is detected, they use ChatGPT to summarize findings.\n\nGaming\n    \n      Create games: e.g. Wyatt Cheng has an awesome video showing how he used ChatGPT to clone Flappy Bird.\n      Generate game characters.\n      Let you have more realistic conversations with game characters: check out this awesome demo by Convai!\n\nLearning\n\nWhenever ChatGPT was down, OpenAI discord is flooded with students complaining about not being to complete their homework. Some responded by banning the use of ChatGPT in school altogether. Some have a much better idea: how to incorporate ChatGPT to help students learn even faster. All EdTech companies I know are going full-speed on ChatGPT exploration.\n\nSome use cases:\n\nSummarize books\n\nAutomatically generate quizzes to make sure students understand a book or a lecture. Not only ChatGPT can generate questions, but it can also evaluate whether a student\u2019s input answers are correct.\n    \n      I tried and ChatGPT seemed pretty good at generating quizzes for Designing Machine Learning Systems. Will publish the quizzes generated soon!\n\nGrade / give feedback on essays\n\nWalk through math solutions\n\nBe a debate partner: ChatGPT is really good at taking different sides of the same debate topic.\n\nWith the rise of homeschooling, I expect to see a lot of applications of ChatGPT to help parents homeschool.\n\nTalk-to-your-data\n\nThis is, in my observation, the most popular enterprise application (so far). Many, many startups are building tools to let enterprise users query their internal data and policies in natural languages or in the Q&A fashion. Some focus on verticals such as legal contracts, resumes, financial data, or customer support. Given a company\u2019s all documentations, policies, and FAQs, you can build a chatbot that can respond your customer support requests.\n\nThe main way to do this application usually involves these 4 steps:\n\nOrganize your internal data into a database (SQL database, graph database, embedding/vector database, or just text database)\n\nGiven an input in natural language, convert it into the query language of the internal database. For example, if it\u2019s a SQL or graph database, this process can return a SQL query. If it\u2019s embedding database, it\u2019s might be an ANN (approximate nearest neighbor) retrieval query. If it\u2019s just purely text, this process can extract a search query.\n\nExecute the query in the database to obtain the query result.\n\nTranslate this query result into natural language.\n\nWhile this makes for really cool demos, I\u2019m not sure how defensible this category is. I\u2019ve seen startups building applications to let users query on top of databases like Google Drive or Notion, and it feels like that\u2019s a feature Google Drive or Notion can implement in a week.\n\nOpenAI has a pretty good tutorial on how to talk to your vector database.\n\nCan LLMs do data analysis for me?\n\nI tried inputting some data into gpt-3.5-turbo, and it seems to be able to detect some patterns. However, this only works for small data that can fit into the input prompt. Most production data is larger than that.\n\nSearch and recommendation\n\nSearch and recommendation has always been the bread and butter of enterprise use cases. It\u2019s going through a renaissance with LLMs. Search has been mostly keyword-based: you need a tent, you search for a tent. But what if you don\u2019t know what you need yet? For example, if you\u2019re going camping in the woods in Oregon in November, you might end up doing something like this:\n\nSearch to read about other people\u2019s experiences.\n\nRead those blog posts and manually extract a list of items you need.\n\nSearch for each of these items, either on Google or other websites.\n\nIf you search for \u201cthings you need for camping in oregon in november\u201d directly on Amazon or any e-commerce website, you\u2019ll get something like this:\n\nBut what if searching for \u201cthings you need for camping in oregon in november\u201d on Amazon actually returns you a list of things you need for your camping trip?\n\nIt\u2019s possible today with LLMs. For example, the application can be broken into the following steps:\n\nTask 1: convert the user query into a list of product names [LLM]\n\nTask 2: for each product name in the list, retrieve relevant products from your product catalog.\n\nIf this works, I wonder if we\u2019ll have LLM SEO: techniques to get your products recommended by LLMs.\n\nSales\n\nThe most obvious way to use LLMs for sales is to write sales emails. But nobody really wants more or better sales emails. However, several companies in my network are using LLMs to synthesize information about a company to see what they need.\n\nSEO\n\nSEO is about to get very weird. Many companies today rely on creating a lot of content hoping to rank high on Google. However, given that LLMs are REALLY good at generating content, and I already know a few startups whose service is to create unlimited SEO-optimized content for any given keyword, search engines will be flooded. SEO might become even more of a cat-and-mouse game: search engines come up with new algorithms to detect AI-generated content, and companies get better at bypassing these algorithms. People might also rely less on search, and more on brands (e.g. trust only the content created by certain people or companies).\n\nAnd we haven\u2019t even touched on SEO for LLMs yet: how to inject your content into LLMs\u2019 responses!!\n\nConclusion\n\nWe\u2019re still in the early days of LLMs applications \u2013 everything is evolving so fast. I recently read a book proposal on LLMs, and my first thought was: most of this will be outdated in a month. APIs are changing day to day. New applications are being discovered. Infrastructure is being aggressively optimized. Cost and latency analysis needs to be done on a weekly basis. New terminologies are being introduced.\n\nNot all of these changes will matter. For example, many prompt engineering papers remind me of the early days of deep learning when there were thousands of papers describing different ways to initialize weights. I imagine that tricks to tweak your prompts like: \"Answer truthfully\", \"I want you to act like \u2026\", writing \"question: \" instead of \"q:\" wouldn\u2019t matter in the long run.\n\nGiven that LLMs seem to be pretty good at writing prompts for themselves \u2013 see Large Language Models Are Human-Level Prompt Engineers (Zhou et al., 2022) \u2013 who knows that we\u2019ll need humans to tune prompts?\n\nHowever, given so much happening, it\u2019s hard to know which will matter, and which won\u2019t.\n\nI recently asked on LinkedIn how people keep up to date with the field. The strategy ranges from ignoring the hype to trying out all the tools.\n\nIgnore (most of) the hype\n\n    Vicki Boykis (Senior ML engineer @ Duo Security): I do the same thing as with any new frameworks in engineering or the data landscape: I skim the daily news, ignore most of it, and wait six months to see what sticks. Anything important will still be around, and there will be more survey papers and vetted implementations that help contextualize what\u2019s happening.\n\nRead only the summaries\n\n    Shashank Chaurasia (Engineering @ Microsoft): I use the Creative mode of BingChat to give me a quick summary of new articles, blogs and research papers related to Gen AI! I often chat with the research papers and github repos to understand the details.\n\nTry to keep up to date with the latest tools\n\n    Chris Alexiuk (Founding ML engineer @ Ox): I just try and build with each of the tools as they come out - that way, when the next step comes out, I\u2019m only looking at the delta.\n\nWhat\u2019s your strategy?\n\ncomments powered by Disqus.",
        "metadata": {
            "source": "https://huyenchip.com/2023/04/11/llm-engineering.html"
        }
    }
]