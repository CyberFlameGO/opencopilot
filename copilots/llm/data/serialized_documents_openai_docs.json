[
    {
        "page_content": "IntroductionOverviewLooking for ChatGPT? Head to chat.openai.com.The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. The OpenAI API can also be used to generate and edit images or convert speech into text. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.ResourcesExperiment in the playgroundRead the API referenceVisit the help centerView the current API statusCheck out the OpenAI Developer ForumLearn about our usage policiesAt OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our API data privacy page.Key conceptsGPTsOpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task. GPTs can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory GPT guide and in our GPT best practices guide.EmbeddingsAn embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our embeddings guide.TokensGPT and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string \" tokenization\" is decomposed as \" token\" and \"ization\", while a short and common word like \" the\" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our tokenizer tool to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.One limitation to keep in mind is that for a GPT model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each GPT and embeddings model can be found in the model index.GuidesJump into one of our guides to learn more.Quickstart tutorialLearn by building a quick sample applicationGPTLearn how to generate textGPT best practicesLearn best practices for building with GPT modelsEmbeddingsLearn how to search, classify, and compare textSpeech to textLearn how to turn speech into textImage generationLearn how to generate or edit imagesFine-tuningLearn how to train a model for your use case",
        "metadata": {
            "source": "https://platform.openai.com/docs/introduction"
        }
    },
    {
        "page_content": "IntroductionOverviewLooking for ChatGPT? Head to chat.openai.com.The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. The OpenAI API can also be used to generate and edit images or convert speech into text. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.ResourcesExperiment in the playgroundRead the API referenceVisit the help centerView the current API statusCheck out the OpenAI Developer ForumLearn about our usage policiesAt OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our API data privacy page.Key conceptsGPTsOpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task. GPTs can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory GPT guide and in our GPT best practices guide.EmbeddingsAn embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our embeddings guide.TokensGPT and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string \" tokenization\" is decomposed as \" token\" and \"ization\", while a short and common word like \" the\" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our tokenizer tool to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.One limitation to keep in mind is that for a GPT model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each GPT and embeddings model can be found in the model index.GuidesJump into one of our guides to learn more.Quickstart tutorialLearn by building a quick sample applicationGPTLearn how to generate textGPT best practicesLearn best practices for building with GPT modelsEmbeddingsLearn how to search, classify, and compare textSpeech to textLearn how to turn speech into textImage generationLearn how to generate or edit imagesFine-tuningLearn how to train a model for your use case",
        "metadata": {
            "source": "https://platform.openai.com/docs/introduction/overview"
        }
    },
    {
        "page_content": "IntroductionOverviewLooking for ChatGPT? Head to chat.openai.com.The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. The OpenAI API can also be used to generate and edit images or convert speech into text. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.ResourcesExperiment in the playgroundRead the API referenceVisit the help centerView the current API statusCheck out the OpenAI Developer ForumLearn about our usage policiesAt OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our API data privacy page.Key conceptsGPTsOpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task. GPTs can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory GPT guide and in our GPT best practices guide.EmbeddingsAn embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our embeddings guide.TokensGPT and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string \" tokenization\" is decomposed as \" token\" and \"ization\", while a short and common word like \" the\" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our tokenizer tool to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.One limitation to keep in mind is that for a GPT model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each GPT and embeddings model can be found in the model index.GuidesJump into one of our guides to learn more.Quickstart tutorialLearn by building a quick sample applicationGPTLearn how to generate textGPT best practicesLearn best practices for building with GPT modelsEmbeddingsLearn how to search, classify, and compare textSpeech to textLearn how to turn speech into textImage generationLearn how to generate or edit imagesFine-tuningLearn how to train a model for your use case",
        "metadata": {
            "source": "https://platform.openai.com/docs/introduction/key-concepts"
        }
    },
    {
        "page_content": "IntroductionOverviewLooking for ChatGPT? Head to chat.openai.com.The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. The OpenAI API can also be used to generate and edit images or convert speech into text. We offer a range of models with different capabilities and price points, as well as the ability to fine-tune custom models.ResourcesExperiment in the playgroundRead the API referenceVisit the help centerView the current API statusCheck out the OpenAI Developer ForumLearn about our usage policiesAt OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our API data privacy page.Key conceptsGPTsOpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task. GPTs can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory GPT guide and in our GPT best practices guide.EmbeddingsAn embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our embeddings guide.TokensGPT and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string \" tokenization\" is decomposed as \" token\" and \"ization\", while a short and common word like \" the\" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our tokenizer tool to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.One limitation to keep in mind is that for a GPT model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each GPT and embeddings model can be found in the model index.GuidesJump into one of our guides to learn more.Quickstart tutorialLearn by building a quick sample applicationGPTLearn how to generate textGPT best practicesLearn best practices for building with GPT modelsEmbeddingsLearn how to search, classify, and compare textSpeech to textLearn how to turn speech into textImage generationLearn how to generate or edit imagesFine-tuningLearn how to train a model for your use case",
        "metadata": {
            "source": "https://platform.openai.com/docs/introduction/next-steps"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/introduction"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/start-with-an-instruction"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/add-some-examples"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/adjust-your-settings"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/build-your-application"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/pricing"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/closing"
        }
    },
    {
        "page_content": "QuickstartLooking for ChatGPT? Head to chat.openai.com.OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.In this quickstart tutorial, you\u2019ll build a simple sample application. Along the way, you\u2019ll learn key concepts and techniques that are fundamental to using the API for any task, including:Content generationSummarizationClassification, categorization, and sentiment analysisData extractionTranslationMany more!IntroductionThe completions endpoint is the core of our API and provides a simple interface that\u2019s extremely flexible and powerful. You input some text as a prompt, and the API will return a text completion that attempts to match whatever instructions or context you gave it.PromptWrite a tagline for an ice cream shop.CompletionWe serve up smiles with every scoop!You can think of this as a very advanced autocomplete \u2014 the model processes your text prompt and tries to predict what\u2019s most likely to come next.1Start with an instructionImagine you want to create a pet name generator. Coming up with names from scratch is hard!First, you\u2019ll need a prompt that makes it clear what you want. Let\u2019s start with an instruction. Submit this prompt to generate your first completion.Suggest one name for a horse.\u200d\u200dNot bad! Now, try making your instruction more specific.Suggest one name for a black horse.\u200d\u200dAs you can see, adding a simple adjective to our prompt changes the resulting completion. Designing your prompt is essentially how you \u201cprogram\u201d the model.2Add some examplesCrafting good instructions is important for getting good results, but sometimes they aren\u2019t enough. Let\u2019s try making your instruction even more complex.Suggest three names for a horse that is a superhero.\u200d\u200dThis completion isn't quite what we want. These names are pretty generic, and it seems like the model didn't pick up on the horse part of our instruction. Let\u2019s see if we can get it to come up with some more relevant suggestions.In many cases, it\u2019s helpful to both show and tell the model what you want. Adding examples to your prompt can help communicate patterns or nuances. Try submitting this prompt which includes a couple examples.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dNice! Adding examples of the output we\u2019d expect for a given input helped the model provide the types of names we were looking for.3Adjust your settingsPrompt design isn\u2019t the only tool you have at your disposal. You can also control completions by adjusting your settings. One of the most important settings is called temperature.You may have noticed that if you submitted the same prompt multiple times in the examples above, the model would always return identical or very similar completions. This is because your temperature was set to 0.Try re-submitting the same prompt a few times with temperature set to 1.Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: Horse\nNames:\u200d\u200dTemperatureSee what happened? When temperature is above 0, submitting the same prompt results in different completions each time.Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.Deep diveUnderstanding tokens and probabilitiesFor your pet name generator, you probably want to be able to generate a lot of name ideas. A moderate temperature of 0.6 should work well.4Build your applicationNode.jsPython (Flask)Now that you\u2019ve found a good prompt and settings, you\u2019re ready to build your pet name generator! We\u2019ve written some code to get you started \u2014 follow the instructions below to download the code and run the app.SetupIf you don\u2019t have Node.js installed, install it from here. Then download the code by cloning this repository.git clone https://github.com/openai/openai-quickstart-node.gitIf you prefer not to use git, you can alternatively download the code using this zip file.Add your API keyTo get the app working, you\u2019ll need an API key. You can get one by signing up for an account and returning to this page.Run the appRun the following commands in the project directory to install the dependencies and run the app.npm install\nnpm run devOpen http://localhost:3000 in your browser and you should see the pet name generator!Understand the codeOpen up generate.js in the openai-quickstart-node/pages/api folder. At the bottom, you\u2019ll see the function that generates the prompt that we were using above. Since users will be entering the type of animal their pet is, it dynamically swaps out the part of the prompt that specifies the animal.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfunction generatePrompt(animal) {\n  const capitalizedAnimal = animal[0].toUpperCase() + animal.slice(1).toLowerCase();\n  return `Suggest three names for an animal that is a superhero.\n\nAnimal: Cat\nNames: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\nAnimal: Dog\nNames: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\nAnimal: ${capitalizedAnimal}\nNames:`;\n}On line 9 in generate.js, you\u2019ll see the code that sends the actual API request. As mentioned above, it uses the completions endpoint with a temperature of 0.6.1\n2\n3\n4\n5\nconst completion = await openai.createCompletion({\n  model: \"text-davinci-003\",\n  prompt: generatePrompt(req.body.animal),\n  temperature: 0.6,\n});And that\u2019s it! You should now have a full understanding of how your (superhero) pet name generator uses the OpenAI API!PricingWe offer a spectrum of models with different capabilities and price points. In this tutorial, we used text-davinci-003. We recommend using this model or gpt-3.5-turbo while experimenting since they will yield the best results. Once you\u2019ve got things working, you can see if the other models can produce the same results with lower latency and costs. Or if you might need to move to a more powerful model like gpt-4.The total number of tokens processed in a single request (both prompt and completion) can\u2019t exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months. Learn more.ClosingThese concepts and techniques will go a long way in helping you build your own application. That said, this simple example demonstrates just a sliver of what\u2019s possible! The completions endpoint is flexible enough to solve virtually any language processing task, including content generation, summarization, semantic search, topic tagging, sentiment analysis, and so much more.One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.For more advanced tasks, you might find yourself wishing you could provide more examples or context than you can fit in a single prompt. The fine-tuning API is a great option for more advanced tasks like this. Fine-tuning allows you to provide hundreds or even thousands of examples to customize a model for your specific use case.Next stepsTo get inspired and learn more about designing prompts for different tasks:Read our GPT guide.Explore our library of example prompts.Start experimenting in the Playground.Keep our usage policies in mind as you start building.Looking for ChatGPT? Head to chat.openai.com.",
        "metadata": {
            "source": "https://platform.openai.com/docs/quickstart/next-steps"
        }
    },
    {
        "page_content": "LibrariesPython libraryWe provide a Python library, which you can install as follows:$ pip install openaiOnce installed, you can use the bindings and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\nimport os\nimport openai\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])The bindings also will install a command-line utility you can use as follows:$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"Node.js libraryWe also have a Node.js library, which you can install by running the following command in your Node.js project directory:$ npm install openaiOnce installed, you can use the library and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\n8\n9\nconst { Configuration, OpenAIApi } = require(\"openai\");\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\nconst openai = new OpenAIApi(configuration);\nconst chat_completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [{ role: \"user\", content: \"Hello world\" }],\n});Azure OpenAI librariesMicrosoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.Azure OpenAI client library for .NETAzure OpenAI client library for JavaScriptAzure OpenAI client library for JavaAzure OpenAI client library for GoCommunity librariesThe libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!C# / .NETBetalgo.OpenAI by BetalgoOpenAI-API-dotnet by OkGoDoItOpenAI-DotNet by RageAgainstThePixelC++liboai by D7EADClojureopenai-clojure by wkokCrystalopenai-crystal by sferikDart/Flutteropenai by anasfikDelphiDelphiOpenAI by HemulGMElixiropenai.ex by mgalloGogo-gpt3 by sashabaranovJavaopenai-java by Theo KanningJuliaOpenAI.jl by rory-linehanKotlinopenai-kotlin by Mouaad AallamNode.jsopenai-api by Njerschowopenai-api-node by erlapsogpt-x by ceifagpt3 by poteatgpts by thencc@dalenguyen/openai by dalenguyentectalic/openai by tectalicPHPorhanerday/open-ai by orhanerdaytectalic/openai by tectalicopenai-php clinet by openai-phpPythonchronology by OthersideAIRrgpt3 by ben-aaron188Rubyopenai by nileshtrivediruby-openai by alexrudallRustasync-openai by 64bitfieri by lbkolevScalaopenai-scala-client by cequence-ioSwiftOpenAIKit by dylanshineOpenAI by MacPawUnityOpenAi-Api-Unity by hexthedevcom.openai.unity by RageAgainstThePixelUnreal EngineOpenAI-Api-Unreal by KellanM",
        "metadata": {
            "source": "https://platform.openai.com/docs/libraries"
        }
    },
    {
        "page_content": "LibrariesPython libraryWe provide a Python library, which you can install as follows:$ pip install openaiOnce installed, you can use the bindings and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\nimport os\nimport openai\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])The bindings also will install a command-line utility you can use as follows:$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"Node.js libraryWe also have a Node.js library, which you can install by running the following command in your Node.js project directory:$ npm install openaiOnce installed, you can use the library and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\n8\n9\nconst { Configuration, OpenAIApi } = require(\"openai\");\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\nconst openai = new OpenAIApi(configuration);\nconst chat_completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [{ role: \"user\", content: \"Hello world\" }],\n});Azure OpenAI librariesMicrosoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.Azure OpenAI client library for .NETAzure OpenAI client library for JavaScriptAzure OpenAI client library for JavaAzure OpenAI client library for GoCommunity librariesThe libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!C# / .NETBetalgo.OpenAI by BetalgoOpenAI-API-dotnet by OkGoDoItOpenAI-DotNet by RageAgainstThePixelC++liboai by D7EADClojureopenai-clojure by wkokCrystalopenai-crystal by sferikDart/Flutteropenai by anasfikDelphiDelphiOpenAI by HemulGMElixiropenai.ex by mgalloGogo-gpt3 by sashabaranovJavaopenai-java by Theo KanningJuliaOpenAI.jl by rory-linehanKotlinopenai-kotlin by Mouaad AallamNode.jsopenai-api by Njerschowopenai-api-node by erlapsogpt-x by ceifagpt3 by poteatgpts by thencc@dalenguyen/openai by dalenguyentectalic/openai by tectalicPHPorhanerday/open-ai by orhanerdaytectalic/openai by tectalicopenai-php clinet by openai-phpPythonchronology by OthersideAIRrgpt3 by ben-aaron188Rubyopenai by nileshtrivediruby-openai by alexrudallRustasync-openai by 64bitfieri by lbkolevScalaopenai-scala-client by cequence-ioSwiftOpenAIKit by dylanshineOpenAI by MacPawUnityOpenAi-Api-Unity by hexthedevcom.openai.unity by RageAgainstThePixelUnreal EngineOpenAI-Api-Unreal by KellanM",
        "metadata": {
            "source": "https://platform.openai.com/docs/libraries/python-library"
        }
    },
    {
        "page_content": "LibrariesPython libraryWe provide a Python library, which you can install as follows:$ pip install openaiOnce installed, you can use the bindings and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\nimport os\nimport openai\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])The bindings also will install a command-line utility you can use as follows:$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"Node.js libraryWe also have a Node.js library, which you can install by running the following command in your Node.js project directory:$ npm install openaiOnce installed, you can use the library and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\n8\n9\nconst { Configuration, OpenAIApi } = require(\"openai\");\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\nconst openai = new OpenAIApi(configuration);\nconst chat_completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [{ role: \"user\", content: \"Hello world\" }],\n});Azure OpenAI librariesMicrosoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.Azure OpenAI client library for .NETAzure OpenAI client library for JavaScriptAzure OpenAI client library for JavaAzure OpenAI client library for GoCommunity librariesThe libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!C# / .NETBetalgo.OpenAI by BetalgoOpenAI-API-dotnet by OkGoDoItOpenAI-DotNet by RageAgainstThePixelC++liboai by D7EADClojureopenai-clojure by wkokCrystalopenai-crystal by sferikDart/Flutteropenai by anasfikDelphiDelphiOpenAI by HemulGMElixiropenai.ex by mgalloGogo-gpt3 by sashabaranovJavaopenai-java by Theo KanningJuliaOpenAI.jl by rory-linehanKotlinopenai-kotlin by Mouaad AallamNode.jsopenai-api by Njerschowopenai-api-node by erlapsogpt-x by ceifagpt3 by poteatgpts by thencc@dalenguyen/openai by dalenguyentectalic/openai by tectalicPHPorhanerday/open-ai by orhanerdaytectalic/openai by tectalicopenai-php clinet by openai-phpPythonchronology by OthersideAIRrgpt3 by ben-aaron188Rubyopenai by nileshtrivediruby-openai by alexrudallRustasync-openai by 64bitfieri by lbkolevScalaopenai-scala-client by cequence-ioSwiftOpenAIKit by dylanshineOpenAI by MacPawUnityOpenAi-Api-Unity by hexthedevcom.openai.unity by RageAgainstThePixelUnreal EngineOpenAI-Api-Unreal by KellanM",
        "metadata": {
            "source": "https://platform.openai.com/docs/libraries/node-js-library"
        }
    },
    {
        "page_content": "LibrariesPython libraryWe provide a Python library, which you can install as follows:$ pip install openaiOnce installed, you can use the bindings and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\nimport os\nimport openai\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])The bindings also will install a command-line utility you can use as follows:$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"Node.js libraryWe also have a Node.js library, which you can install by running the following command in your Node.js project directory:$ npm install openaiOnce installed, you can use the library and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\n8\n9\nconst { Configuration, OpenAIApi } = require(\"openai\");\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\nconst openai = new OpenAIApi(configuration);\nconst chat_completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [{ role: \"user\", content: \"Hello world\" }],\n});Azure OpenAI librariesMicrosoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.Azure OpenAI client library for .NETAzure OpenAI client library for JavaScriptAzure OpenAI client library for JavaAzure OpenAI client library for GoCommunity librariesThe libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!C# / .NETBetalgo.OpenAI by BetalgoOpenAI-API-dotnet by OkGoDoItOpenAI-DotNet by RageAgainstThePixelC++liboai by D7EADClojureopenai-clojure by wkokCrystalopenai-crystal by sferikDart/Flutteropenai by anasfikDelphiDelphiOpenAI by HemulGMElixiropenai.ex by mgalloGogo-gpt3 by sashabaranovJavaopenai-java by Theo KanningJuliaOpenAI.jl by rory-linehanKotlinopenai-kotlin by Mouaad AallamNode.jsopenai-api by Njerschowopenai-api-node by erlapsogpt-x by ceifagpt3 by poteatgpts by thencc@dalenguyen/openai by dalenguyentectalic/openai by tectalicPHPorhanerday/open-ai by orhanerdaytectalic/openai by tectalicopenai-php clinet by openai-phpPythonchronology by OthersideAIRrgpt3 by ben-aaron188Rubyopenai by nileshtrivediruby-openai by alexrudallRustasync-openai by 64bitfieri by lbkolevScalaopenai-scala-client by cequence-ioSwiftOpenAIKit by dylanshineOpenAI by MacPawUnityOpenAi-Api-Unity by hexthedevcom.openai.unity by RageAgainstThePixelUnreal EngineOpenAI-Api-Unreal by KellanM",
        "metadata": {
            "source": "https://platform.openai.com/docs/libraries/azure-openai-libraries"
        }
    },
    {
        "page_content": "LibrariesPython libraryWe provide a Python library, which you can install as follows:$ pip install openaiOnce installed, you can use the bindings and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\nimport os\nimport openai\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nchat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])The bindings also will install a command-line utility you can use as follows:$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"Node.js libraryWe also have a Node.js library, which you can install by running the following command in your Node.js project directory:$ npm install openaiOnce installed, you can use the library and your secret key to run the following:1\n2\n3\n4\n5\n6\n7\n8\n9\nconst { Configuration, OpenAIApi } = require(\"openai\");\nconst configuration = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n});\nconst openai = new OpenAIApi(configuration);\nconst chat_completion = await openai.createChatCompletion({\n    model: \"gpt-3.5-turbo\",\n    messages: [{ role: \"user\", content: \"Hello world\" }],\n});Azure OpenAI librariesMicrosoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API.Azure OpenAI client library for .NETAzure OpenAI client library for JavaScriptAzure OpenAI client library for JavaAzure OpenAI client library for GoCommunity librariesThe libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our help center article on adding community libraries. You can also watch our OpenAPI specification repository on GitHub to get timely updates on when we make changes to our API.Please note that OpenAI does not verify the correctness or security of these projects. Use them at your own risk!C# / .NETBetalgo.OpenAI by BetalgoOpenAI-API-dotnet by OkGoDoItOpenAI-DotNet by RageAgainstThePixelC++liboai by D7EADClojureopenai-clojure by wkokCrystalopenai-crystal by sferikDart/Flutteropenai by anasfikDelphiDelphiOpenAI by HemulGMElixiropenai.ex by mgalloGogo-gpt3 by sashabaranovJavaopenai-java by Theo KanningJuliaOpenAI.jl by rory-linehanKotlinopenai-kotlin by Mouaad AallamNode.jsopenai-api by Njerschowopenai-api-node by erlapsogpt-x by ceifagpt3 by poteatgpts by thencc@dalenguyen/openai by dalenguyentectalic/openai by tectalicPHPorhanerday/open-ai by orhanerdaytectalic/openai by tectalicopenai-php clinet by openai-phpPythonchronology by OthersideAIRrgpt3 by ben-aaron188Rubyopenai by nileshtrivediruby-openai by alexrudallRustasync-openai by 64bitfieri by lbkolevScalaopenai-scala-client by cequence-ioSwiftOpenAIKit by dylanshineOpenAI by MacPawUnityOpenAi-Api-Unity by hexthedevcom.openai.unity by RageAgainstThePixelUnreal EngineOpenAI-Api-Unreal by KellanM",
        "metadata": {
            "source": "https://platform.openai.com/docs/libraries/community-libraries"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/overview"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/continuous-model-upgrades"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/gpt-4"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/gpt-3-5"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/dall-e"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/whisper"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/embeddings"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/moderation"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/gpt-3"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/how-we-use-your-data"
        }
    },
    {
        "page_content": "ModelsOverviewThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.ModelsDescriptionGPT-4A set of models that improve on GPT-3.5 and can understand as well as generate natural language or codeGPT-3.5A set of models that improve on GPT-3 and can understand as well as generate natural language or codeDALL\u00b7EA model that can generate and edit images given a natural language promptWhisperA model that can convert audio into textEmbeddingsA set of models that can convert text into a numerical formModerationA fine-tuned model that can detect whether text may be sensitive or unsafeGPT-3LegacyA set of models that can understand and generate natural languageDeprecatedA full list of models that have been deprecatedWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.Visit our model index for researchers to learn\nmore about which models have been featured in our research papers and the differences\nbetween model series like InstructGPT and GPT-3.5.Continuous model upgradesBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.With the release of gpt-3.5-turbo, some of our models are now being continually updated. We also offer static model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving people the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the OpenAI Evals repository.The following models are the temporary snapshots, we will announce their deprecation dates once updated versions are available. If you want to use the latest model version, use the standard model names like gpt-4 or gpt-3.5-turbo.Model nameDiscontinuation dateReplacement modelgpt-3.5-turbo-0301at earliest 06/13/2024gpt-3.5-turbo-0613gpt-4-0314at earliest 06/13/2024gpt-4-0613gpt-4-32k-0314at earliest 06/13/2024gpt-4-32k-0613Learn more about model deprecation on our deprecation page.GPT-4GPT-4 is currently accessible to those who have made at least one successful payment through our developer platform.GPT-4 is a large multimodal model (accepting text inputs and emitting text outputs today, with image inputs coming in the future) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks both using the Chat Completions API. Learn how to use GPT-4 in our GPT guide.Latest modelDescriptionMax tokensTraining datagpt-4More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.8,192 tokensUp to Sep 2021gpt-4-0613Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.8,192 tokensUp to Sep 2021gpt-4-32kSame capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.32,768 tokensUp to Sep 2021gpt-4-32k-0613Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.32,768 tokensUp to Sep 2021For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.GPT-3.5GPT-3.5 models can understand and generate natural language or code. Our most capable and cost effective model in the GPT-3.5 family is gpt-3.5-turbo which has been optimized for chat but works well for traditional completions tasks as well.Latest modelDescriptionMax tokensTraining datagpt-3.5-turboMost capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16kSame capabilities as the standard gpt-3.5-turbo model but with 4 times the context.16,384 tokensUp to Sep 2021gpt-3.5-turbo-0613Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.4,096 tokensUp to Sep 2021gpt-3.5-turbo-16k-0613Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.16,384 tokensUp to Sep 2021text-davinci-003 (Legacy)Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.4,097 tokensUp to Jun 2021text-davinci-002 (Legacy)Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning4,097 tokensUp to Jun 2021code-davinci-002 (Legacy)Optimized for code-completion tasks8,001 tokensUp to Jun 2021We recommend using gpt-3.5-turbo over the other GPT-3.5 models because of its lower cost and improved performance.OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain.DALL\u00b7EDALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prompt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image.The current DALL\u00b7E model available through our API is the 2nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.WhisperWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.EmbeddingsEmbeddings are a numerical representation of text that can be used to measure the relateness between two pieces of text. Our second generation embedding model, text-embedding-ada-002 is a designed to replace the previous 16 first-generation embedding models at a fraction of the cost. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding model in the announcement blog post.ModerationThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our moderation guide.Moderation models take in an arbitrary sized input that is automatically broken up to fix the models specific context window.ModelDescriptiontext-moderation-latestMost capable moderation model. Accuracy will be slighlty higher than the stable model.text-moderation-stableAlmost as capable as the latest model, but slightly older.GPT-3 LegacyGPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune.Latest modelDescriptionMax tokensTraining datatext-curie-001Very capable, faster and lower cost than Davinci.2,049 tokensUp to Oct 2019text-babbage-001Capable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019text-ada-001Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019davinciMost capable GPT-3 model. Can do any task the other models can do, often with higher quality.2,049 tokensUp to Oct 2019curieVery capable, but faster and lower cost than Davinci.2,049 tokensUp to Oct 2019babbageCapable of straightforward tasks, very fast, and lower cost.2,049 tokensUp to Oct 2019adaCapable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.2,049 tokensUp to Oct 2019How we use your dataYour data is your data.As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explitly opt in). One advantage to opting in is that the models may get better at your use case over time.To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.Note that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL\u00b7E Labs.Default usage policies by endpointEndpointData used for trainingDefault retentionEligible for zero retention/v1/completionsNo30 daysYes/v1/chat/completionsNo30 daysYes/v1/editsNo30 daysYes/v1/images/generationsNo30 daysNo/v1/images/editsNo30 daysNo/v1/images/variationsNo30 daysNo/v1/embeddingsNo30 daysYes/v1/audio/transcriptionsNoZero data retention-/v1/audio/translationsNoZero data retention-/v1/filesNoUntil deleted by customerNo/v1/fine-tunesNoUntil deleted by customerNo/v1/moderationsNoZero data retention-For details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.Model endpoint compatibilityEndpointModel name/v1/chat/completionsgpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613/v1/completions (Legacy)text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada/v1/audio/transcriptionswhisper-1/v1/audio/translationswhisper-1/v1/fine-tunesdavinci, curie, babbage, ada/v1/embeddingstext-embedding-ada-002, text-similarity-*-001, text-search-*-*-001, code-search-*-*-001/v1/moderationstext-moderation-stable, text-moderation-latestThis list does not include our first-generation embedding models nor our DALL\u00b7E models.",
        "metadata": {
            "source": "https://platform.openai.com/docs/models/model-endpoint-compatibility"
        }
    },
    {
        "page_content": "DeprecationsOverviewAs we launch safer and more capable models, we regularly retire older models. Software relying on OpenAI models may need occasional updates to keep working. Impacted customers will always be notified by email and in our documentation along with blog posts for larger changes.This page lists all API deprecations, along with recommended replacements.Incremental model updatesAs announced in March 2023, we regularly release new versions of gpt-4 and gpt-3.5-turbo.Each model version is dated with an -MMDD suffix; e.g., gpt-4-0613. The undated model name, e.g., gpt-4, will typically point to the latest version (e.g. gpt-4 points to gpt-4-0613). Users of undated model names will be notified by email typically 2 weeks before any change takes place.After a new version is launched, older versions will typically be deprecated 3 months later.Migrating to replacementsOnce a model is deprecated, be sure to migrate all usage to a suitable replacement before the shutdown date. Requests to models past the shutdown date will fail.To help measure the performance of replacement models on your tasks, we have open-sourced Evals, a Python framework for evaluating language models.If new models perform worse on your tasks, let us know by by submitting a pull request to our Evals repo with examples of the task.Deprecation historyAll deprecations are listed below, with the most recent announcements at the top.2023-07-06: GPT and embeddingsOn July 06, 2023, we announced the upcoming retirements of older GPT-3 and GPT-3.5 models served via the completions endpoint. We also announced the upcoming retirement of our first-generation text embedding models. They will be shut down on January 04, 2024.InstructGPT modelsShutdown dateModelPriceRecommended replacement2024-01-04text-ada-001$0.0004 / 1K tokensgpt-3.5-turbo-instruct2024-01-04text-babbage-001$0.0005 / 1K tokensgpt-3.5-turbo-instruct2024-01-04text-curie-001$0.0020 / 1K tokensgpt-3.5-turbo-instruct2024-01-04text-davinci-001$0.0200 / 1K tokensgpt-3.5-turbo-instruct2024-01-04text-davinci-002$0.0200 / 1K tokensgpt-3.5-turbo-instruct2024-01-04text-davinci-003$0.0200 / 1K tokensgpt-3.5-turbo-instructNote: The recommended replacement, gpt-3.5-turbo-instruct, has not yet launched. Impacted customers will be notified by email when it becomes available.Base GPT modelsShutdown dateModelPriceRecommended replacement2024-01-04ada$0.0004 / 1K tokensada-0022024-01-04babbage$0.0005 / 1K tokensbabbage-0022024-01-04curie$0.0020 / 1K tokenscurie-0022024-01-04davinci$0.0200 / 1K tokensdavinci-0022024-01-04code-davinci-002free to researchersgpt-3.5-turbo-baseNote: The recommended replacements have not yet launched. Impacted customers will be notified by email when they become available.Edit models & endpointShutdown dateModel/SystemRecommended replacement2024-01-04text-davinci-edit-001gpt-42024-01-04code-davinci-edit-001gpt-42024-01-04/v1/edits/v1/chat/completionsFine-tuning GPT modelsShutdown dateModelTraining priceUsage priceRecommended replacement2024-01-04ada$0.0004 / 1K tokens$0.0016 / 1K tokensada-0022024-01-04babbage$0.0006 / 1K tokens$0.0024 / 1K tokensbabbage-0022024-01-04curie$0.003 / 1K tokens$0.012 / 1K tokenscurie-0022024-01-04davinci$0.03 / 1K tokens$0.12 / 1K tokensdavinci-002, gpt-3.5-turbo, gpt-4Note: The recommended replacements have not yet launched. Impacted customers will be notified by email when they become available.First-generation text embedding modelsShutdown dateModelPriceRecommended replacement2024-01-04text-similarity-ada-001$0.004 / 1K tokenstext-embedding-ada-0022024-01-04text-search-ada-doc-001$0.004 / 1K tokenstext-embedding-ada-0022024-01-04text-search-ada-query-001$0.004 / 1K tokenstext-embedding-ada-0022024-01-04code-search-ada-code-001$0.004 / 1K tokenstext-embedding-ada-0022024-01-04code-search-ada-text-001$0.004 / 1K tokenstext-embedding-ada-0022024-01-04text-similarity-babbage-001$0.005 / 1K tokenstext-embedding-ada-0022024-01-04text-search-babbage-doc-001$0.005 / 1K tokenstext-embedding-ada-0022024-01-04text-search-babbage-query-001$0.005 / 1K tokenstext-embedding-ada-0022024-01-04code-search-babbage-code-001$0.005 / 1K tokenstext-embedding-ada-0022024-01-04code-search-babbage-text-001$0.005 / 1K tokenstext-embedding-ada-0022024-01-04text-similarity-curie-001$0.020 / 1K tokenstext-embedding-ada-0022024-01-04text-search-curie-doc-001$0.020 / 1K tokenstext-embedding-ada-0022024-01-04text-search-curie-query-001$0.020 / 1K tokenstext-embedding-ada-0022024-01-04text-similarity-davinci-001$0.200 / 1K tokenstext-embedding-ada-0022024-01-04text-search-davinci-doc-001$0.200 / 1K tokenstext-embedding-ada-0022024-01-04text-search-davinci-query-001$0.200 / 1K tokenstext-embedding-ada-0022023-06-13: Updated chat modelsBased on developer feedback, we are extending support for gpt-3.5-turbo-0301 and gpt-4-0314 models in the OpenAI API until at least June 13, 2024. We've updated our June 13 blog post with more details.On June 13, 2023, we announced new chat model versions in the Function calling and other API updates blog post. The three original versions will be retired in June 2024 at the earliest.Shutdown dateModelPriceRecommended replacementat earliest 2024-06-13gpt-3.5-turbo-0301$0.0015 / 1K input tokens + $0.0020 / 1K output tokensgpt-3.5-turbo-0613at earliest 2024-06-13gpt-4-0314$0.03 / 1K input tokens + $0.06 / 1K output tokensgpt-4-0613at earliest 2024-06-13gpt-4-32k-0314$0.06 / 1K input tokens + $0.12 / 1K output tokensgpt-4-32k-06132023-03-20: Codex modelsShutdown dateModelRecommended replacement2023-03-23code-davinci-002gpt-4 or researcher access program2023-03-23code-davinci-001gpt-4 or researcher access program2023-03-23code-cushman-002gpt-4 or researcher access program2023-03-23code-cushman-001gpt-4 or researcher access program2022-06-03: Legacy endpointsShutdown dateSystemRecommended replacement2022-12-03/v1/engines/v1/models2022-12-03/v1/searchView transition guide2022-12-03/v1/classificationsView transition guide2022-12-03/v1/answersView transition guide",
        "metadata": {
            "source": "https://platform.openai.com/docs/deprecations"
        }
    },
    {
        "page_content": "TutorialsGet started with the OpenAI API by building real AI apps step by step.Website Q&A with EmbeddingsLearn how to build an AI that can answer questions about your website.Coming soonLearn how to build and deploy an AI that can answer questions about local files.Coming soonLearn how to build and deploy an AI chat bot that understands multiple knowledge bases.Looking for more ideas? Check out our Examples gallery or the OpenAI Cookbook on GitHub.",
        "metadata": {
            "source": "https://platform.openai.com/docs/tutorials"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/chat-completions-api"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/function-calling"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/completions-api"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/chat-completions-vs-completions"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/managing-tokens"
        }
    },
    {
        "page_content": "GPT modelsLooking for ChatGPT? Head to chat.openai.com.OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a GPT model, usually by providing instructions or some examples of how to successfully complete a task.Using GPTs, you can build applications to:Draft documentsWrite computer codeAnswer questions about a knowledge baseAnalyze textsCreate conversational agentsGive software a natural language interfaceTutor in a range of subjectsTranslate languagesSimulate characters for games...and much more!To use a GPT model via the OpenAI API, you\u2019ll send a request containing the inputs and your API key, and receive a response containing the model\u2019s output. Our latest models, gpt-4 and gpt-3.5-turbo, are accessed through the chat completions API endpoint. Currently, only the older legacy models are available via the completions API endpoint.Model familiesAPI endpointNewer models (2023\u2013)gpt-4, gpt-3.5-turbohttps://api.openai.com/v1/chat/completionsLegacy models (2020\u20132022)text-davinci-003, text-davinci-002, davinci, curie, babbage, adahttps://api.openai.com/v1/completionsYou can experiment with GPTs in the playground. If you\u2019re not sure which model to use, then use gpt-3.5-turbo or gpt-4.Chat Completions APIChat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it\u2019s just as useful for single-turn tasks without any conversation.An example API call looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nimport openai\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)See the full API reference documentation here.The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model\u2019s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.Including conversation history is important when user instructions refer to prior messages. In the example above, the user\u2019s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model\u2019s token limit, it will need to be shortened in some way.To mimic the effect seen in ChatGPT where the text is returned iteratively, set the stream parameter to true.Chat completions response formatAn example chat completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1677664795,\n  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 57,\n    \"total_tokens\": 74\n  }\n}In Python, the assistant\u2019s reply can be extracted with response['choices'][0]['message']['content'].Every response will include a finish_reason. The possible values for finish_reason are:stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameterlength: Incomplete model output due to max_tokens parameter or token limitfunction_call: The model decided to call a functioncontent_filter: Omitted content due to a flag from our content filtersnull: API response still in progress or incompleteDepending on input parameters (like providing functions as shown below), the model response may include different information.Function callingIn an API call, you can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.Function calling allows you to more reliably get structured data back from the model. For example, you can:Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')Convert natural language into API callse.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal APIExtract structured data from texte.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)...and much more!The basic sequence of steps for function calling is as follows:Call the model with the user query and a set of functions defined in the functions parameter.The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).Parse the string into JSON in your code, and call your function with the provided arguments if they exist.Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.You can see these steps in action through the example below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())Hallucinated outputs in function calls can often be mitigated with a system message. For example, if you find that a model is generating function calls with functions that weren't provided to it, try using a system message that says: \"Only use the functions you have been provided with.\"In the example above, we sent the function response back to the model and let it decide the next step. It responded with a user-facing message which was telling the user the temperature in Boston, but depending on the query, it may choose to call a function again.For example, if you ask the model \u201cFind the weather in Boston this weekend, book dinner for two on Saturday, and update my calendar\u201d and provide the corresponding functions for these queries, it may choose to call them back to back and only at the end create a user-facing message.If you want to force the model to call a specific function you can do so by setting function_call: {\"name\": \"<insert-function-name>\"}. You can also force the model to generate a user-facing message by setting function_call: \"none\". Note that the default behavior (function_call: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.You can find more examples of function calling in the OpenAI cookbook:Function callingLearn from more examples demonstrating function callingCompletions API LegacyThe completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a prompt.An example API call looks as follows:1\n2\n3\n4\n5\n6\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)See the full API reference documentation to learn more.Token log probabilitiesThe completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.Inserting textThe completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file.Deep diveInserting textCompletions response formatAn example completions API response looks as follows:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}In Python, the output can be extracted with response['choices'][0]['text'].The response format is similar to the response format of the chat completions API but also includes the optional field logprobs.Chat Completions vs. CompletionsThe chat completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:Translate the following English text to French: \"{text}\"And an equivalent chat prompt would be:[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.The difference between these APIs derives mainly from the underlying GPT models that are available in each. The chat completions API is the interface to our most capable model (gpt-4), and our most cost effective model (gpt-3.5-turbo). For reference, gpt-3.5-turbo performs at a similar capability level to text-davinci-003 but at 10% the price per token! See pricing details here.Which model should I use?We generally recommend that you use either gpt-4 or gpt-3.5-turbo. Which of these you should use depends on the complexity of the tasks you are using the models for. gpt-4 generally performs better on a wide range of evaluations. In particular, gpt-4 is more capable at carefully following complex instructions. By contrast gpt-3.5-turbo is more likely to follow just one part of a complex multi-part instruction. gpt-4 is less likely than gpt-3.5-turbo to make up information, a behavior known as \"hallucination\". gpt-4 also has a larger context window with a maximum size of 8,192 tokens compared to 4,096 tokens for gpt-3.5-turbo. However, gpt-3.5-turbo returns outputs with lower latency and costs much less per token.We recommend experimenting in the playground to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them.GPT best practicesAn awareness of the best practices for working with GPTs can make a significant difference in application performance. The failure modes that GPTs exhibit and the ways of working around or correcting those failure modes are not always intuitive. There is a skill to working with GPTs which has come to be known as \u201cprompt engineering\u201d, but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components. To learn more, read our guide on GPT best practices which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more. You can also find many useful resources including code samples in the OpenAI Cookbook.Managing tokensLanguage models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., a or  apple), and in some languages tokens can be even shorter than one character or even longer than one word.For example, the string \"ChatGPT is great!\" is encoded into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].The total number of tokens in an API call affects:How much your API call costs, as you pay per tokenHow long your API call takes, as writing more tokens takes more timeWhether your API call works at all, as total tokens must be below the model\u2019s maximum limit (4096 tokens for gpt-3.5-turbo)Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the pricing page for more information).To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens']).Chat models like gpt-3.5-turbo and gpt-4 use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.Deep diveCounting tokens for chat API callsTo see how many tokens are in a text string without making an API call, use OpenAI\u2019s tiktoken Python library. Example code can be found in the OpenAI Cookbook\u2019s guide on how to count tokens with tiktoken.Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.If a conversation has too many tokens to fit within a model\u2019s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.FAQWhy are model outputs inconsistent?The API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain.How should I set the temperature parameter?Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.Is fine-tuning available for the latest models?No. Currently, you can only fine-tune base GPT-3 models (davinci, curie, babbage, and ada). See the fine-tuning guide for more details on how to use fine-tuned models.Do you store the data that is passed into the API?As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy. Some endpoints offer zero retention.How can I make my application more safe?If you want to add a moderation layer to the outputs of the Chat API, you can follow our moderation guide to prevent content that violates OpenAI\u2019s usage policies from being shown.Should I use ChatGPT or the API?ChatGPT offers a chat interface to the models in the OpenAI API and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI\u2019s API provides more flexibility.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt/faq"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/six-strategies-for-getting-better-results"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-write-clear-instructions"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-provide-reference-text"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-split-complex-tasks-into-simpler-subtasks"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-give-gpts-time-to-think"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-use-external-tools"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/strategy-test-changes-systematically"
        }
    },
    {
        "page_content": "GPT best practicesThis guide shares strategies and tactics for getting better results from GPTs. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.Some of the examples demonstrated here currently work only with our most capable model, gpt-4. If you don't yet have access to gpt-4 consider joining the waitlist. In general, if you find that a GPT model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.Six strategies for getting better resultsWrite clear instructionsGPTs can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less GPTs have to guess at what you want, the more likely you\u2019ll get it.Tactics:Include details in your query to get more relevant answersAsk the model to adopt a personaUse delimiters to clearly indicate distinct parts of the inputSpecify the steps required to complete a taskProvide examplesSpecify the desired length of the outputProvide reference textGPTs can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to GPTs can help in answering with fewer fabrications.Tactics:Instruct the model to answer using a reference textInstruct the model to answer with citations from a reference textSplit complex tasks into simpler subtasksJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to GPTs. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.Tactics:Use intent classification to identify the most relevant instructions for a user queryFor dialogue applications that require very long conversations, summarize or filter previous dialogueSummarize long documents piecewise and construct a full summary recursivelyGive GPTs time to \"think\"If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, GPTs make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a chain of reasoning before an answer can help GPTs reason their way toward correct answers more reliably.Tactics:Instruct the model to work out its own solution before rushing to a conclusionUse inner monologue or a sequence of queries to hide the model's reasoning processAsk the model if it missed anything on previous passesUse external toolsCompensate for the weaknesses of GPTs by feeding them the outputs of other tools. For example, a text retrieval system can tell GPTs about relevant documents. A code execution engine can help GPTs do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a GPT, offload it to get the best of both.Tactics:Use embeddings-based search to implement efficient knowledge retrievalUse code execution to perform more accurate calculations or call external APIsGive the model access to specific functionsTest changes systematicallyImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").Tactic:Evaluate model outputs with reference to gold-standard answersTacticsEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.Strategy: Write clear instructionsTactic: Include details in your query to get more relevant answersIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.WorseBetterHow do I add numbers in Excel?How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \"Total\".Who\u2019s president?Who was the president of Mexico in 2021, and how frequently are elections held?Write code to calculate the Fibonacci sequence.Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.Summarize the meeting notes.Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.Tactic: Ask the model to adopt a personaThe system message can be used to specify the persona used by the model in its replies.SYSTEMWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSERWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.Open in PlaygroundTactic: Use delimiters to clearly indicate distinct parts of the inputDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.USERSummarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundSYSTEMYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER<article> insert first article here </article>\n\n<article> insert second article here </article>Open in PlaygroundSYSTEMYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSERAbstract: insert abstract here\n\nTitle: insert title hereOpen in PlaygroundFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don\u2019t make GPTs work to understand exactly what you are asking of them.Tactic: Specify the steps required to complete a taskSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.SYSTEMUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\nUSER\"\"\"insert text here\"\"\"Open in PlaygroundTactic: Provide examplesProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \"few-shot\" prompting.SYSTEMAnswer in a consistent style.\nUSERTeach me about patience.\nASSISTANTThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSERTeach me about the ocean.Open in PlaygroundTactic: Specify the desired length of the outputYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.USERSummarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundUSERSummarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"Open in PlaygroundStrategy: Provide reference textTactic: Instruct the model to answer using a reference textIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.SYSTEMUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\nUSER<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>Open in PlaygroundGiven that GPTs have limited context windows, in order to apply this tactic we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\" for more details on how to implement this.Tactic: Instruct the model to answer with citations from a reference textIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.SYSTEMYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": \u2026}).\nUSER\"\"\"<insert document here>\"\"\"\n\nQuestion: <insert question here>Open in PlaygroundStrategy: Split complex tasks into simpler subtasksTactic: Use intent classification to identify the most relevant instructions for a user queryFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).Suppose for example that for a customer service application, queries could be usefully classified as follows:SYSTEMYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\nUSERI need to get my internet working again.Open in PlaygroundBased on the classification of the customer query, a set of more specific instructions can be provided to a GPT model to handle next steps. For example, suppose the customer requires help with \"troubleshooting\".SYSTEMYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSERI need to get my internet working again.Open in PlaygroundNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogueSince GPTs have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \"Use embeddings-based search to implement efficient knowledge retrieval\".Tactic: Summarize long documents piecewise and construct a full summary recursivelySince GPTs have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it\u2019s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.Strategy: Give GPTs time to \"think\"Tactic: Instruct the model to work out its own solution before rushing to a conclusionSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student\u2019s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.SYSTEMDetermine if the student's solution is correct or not.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTThe student's solution is correct.Open in PlaygroundBut the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.SYSTEMFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSERProblem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nASSISTANTLet x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.Open in PlaygroundTactic: Use inner monologue or a sequence of queries to hide the model's reasoning processThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model\u2019s reasoning process about the student\u2019s solution could reveal the answer to the student.Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.SYSTEMFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\nUSERProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>Open in PlaygroundAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student\u2019s solution, it can be omitted. This provides the additional advantage that there is no chance that the model\u2019s solution will be biased by the student\u2019s attempted solution.USER<insert problem statement>Open in PlaygroundNext, we can have the model use all available information to assess the correctness of the student\u2019s solution.SYSTEMCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"Open in PlaygroundFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.SYSTEMYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSERProblem statement: \"\"\"<insert problem statement>\"\"\"\n\nYour solution: \"\"\"<insert model generated solution>\"\"\"\n\nStudent\u2019s solution: \"\"\"<insert student's solution>\"\"\"\n\nAnalysis: \"\"\"<insert model generated analysis from previous step>\"\"\"Open in PlaygroundTactic: Ask the model if it missed anything on previous passesSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.SYSTEMYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\nUSER\"\"\"<insert document here>\"\"\"\nASSISTANT[{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\nUSERAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.Open in PlaygroundStrategy: Use external toolsTactic: Use embeddings-based search to implement efficient knowledge retrievalA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc\u2026) to the model\u2019s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).Example implementations can be found in the OpenAI Cookbook. See the tactic \u201cInstruct the model to use retrieved knowledge to answer queries\u201d for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.Tactic: Use code execution to perform more accurate calculations or call external APIsGPTs cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtics. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.SYSTEMYou can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\nUSERFind all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.Open in PlaygroundAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.SYSTEMYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")```Open in PlaygroundWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.Tactic: Give the model access to specific functionsThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using GPT models to call external functions. To learn more see the function calling section in our introductory GPT guide and more function calling examples in the OpenAI Cookbook.Strategy: Test changes systematicallySometimes it can be hard to tell whether a change \u2014 e.g., a new instruction or a new design \u2014 makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.Evaluation procedures (or \"evals\") are useful for optimizing system designs. Good evals are:Representative of real-world usage (or at least diverse)Contain many test cases for greater statistical power (see table below for guidelines)Easy to automate or repeatDifference to detectSample size needed for 95% confidence30%~1010%~1003%~1,0001%~10,000Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.Tactic: Evaluate model outputs with reference to gold-standard answersSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.For example, using the following system message:SYSTEMYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.Here's an example input where both points are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\"\"\"Open in PlaygroundHere's an example input where only one point is satisfied:SYSTEM<insert system message above>\nUSER\"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"Open in PlaygroundHere's an example input where none are satisfied:SYSTEM<insert system message above>\nUSER\"\"\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\"One small step,\" he said, for a new world.\"\"\"Open in PlaygroundThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.SYSTEMUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"Didn't he walk on the moon or something?\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with answer that directly contradicts the expert answer:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundHere's an example input with a correct answer that also provides a bit more detail than is necessary:SYSTEM<insert system message above>\nUSERQuestion: \"\"\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\"\"\"\n\nSubmitted Answer: \"\"\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\"\"\"\n\nExpert Answer: \"\"\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\"\"\"Open in PlaygroundOther resourcesFor more inspiration, visit the OpenAI Cookbook, which contains example code and also links to third-party resources such as:Prompting libraries & toolsPrompting guidesVideo coursesPapers on advanced prompting to improve reasoning",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/gpt-best-practices/other-resources"
        }
    },
    {
        "page_content": "Image generationLearn how to generate or manipulate images with our DALL\u00b7E modelsIntroductionThe Images API provides three methods for interacting with images:Creating images from scratch based on a text promptCreating edits of an existing image based on a new text promptCreating variations of an existing imageThis guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app.UsageGenerationsThe image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter.Generate an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create(\n  prompt=\"a white siamese cat\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example:PromptGenerationa white siamese cata close up, studio photographic portrait of a white siamese cat that looks curious, backlit earsEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.EditsThe image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app.Edit an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\nresponse = openai.Image.create_edit(\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageMaskOutputPrompt: a sunlit indoor lounge area with a pool containing a flamingoThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above.VariationsThe image variations endpoint allows you to generate a variation of a given image.Generate an image variationpythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create_variation(\n  image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageOutputSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.Content moderationPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center.Language-specific tipsNode.jsPythonUsing in-memory image dataThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:1\n2\n3\n4\n5\n6\n7\n8\n9\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  buffer,\n  1,\n  \"1024x1024\"\n);Working with TypeScriptIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:1\n2\n3\n4\n5\n6\n// Cast the ReadStream to `any` to appease the TypeScript compiler\nconst response = await openai.createImageVariation(\n  fs.createReadStream(\"image.png\") as any,\n  1,\n  \"1024x1024\"\n);And here's a similar example for in-memory image data:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  file,\n  1,\n  \"1024x1024\"\n);Error handlingAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\ntry {\n  const response = await openai.createImageVariation(\n    fs.createReadStream(\"image.png\"),\n    1,\n    \"1024x1024\"\n  );\n  console.log(response.data.data[0].url);\n} catch (error) {\n  if (error.response) {\n    console.log(error.response.status);\n    console.log(error.response.data);\n  } else {\n    console.log(error.message);\n  }\n}",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/images"
        }
    },
    {
        "page_content": "Image generationLearn how to generate or manipulate images with our DALL\u00b7E modelsIntroductionThe Images API provides three methods for interacting with images:Creating images from scratch based on a text promptCreating edits of an existing image based on a new text promptCreating variations of an existing imageThis guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app.UsageGenerationsThe image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter.Generate an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create(\n  prompt=\"a white siamese cat\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example:PromptGenerationa white siamese cata close up, studio photographic portrait of a white siamese cat that looks curious, backlit earsEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.EditsThe image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app.Edit an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\nresponse = openai.Image.create_edit(\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageMaskOutputPrompt: a sunlit indoor lounge area with a pool containing a flamingoThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above.VariationsThe image variations endpoint allows you to generate a variation of a given image.Generate an image variationpythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create_variation(\n  image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageOutputSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.Content moderationPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center.Language-specific tipsNode.jsPythonUsing in-memory image dataThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:1\n2\n3\n4\n5\n6\n7\n8\n9\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  buffer,\n  1,\n  \"1024x1024\"\n);Working with TypeScriptIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:1\n2\n3\n4\n5\n6\n// Cast the ReadStream to `any` to appease the TypeScript compiler\nconst response = await openai.createImageVariation(\n  fs.createReadStream(\"image.png\") as any,\n  1,\n  \"1024x1024\"\n);And here's a similar example for in-memory image data:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  file,\n  1,\n  \"1024x1024\"\n);Error handlingAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\ntry {\n  const response = await openai.createImageVariation(\n    fs.createReadStream(\"image.png\"),\n    1,\n    \"1024x1024\"\n  );\n  console.log(response.data.data[0].url);\n} catch (error) {\n  if (error.response) {\n    console.log(error.response.status);\n    console.log(error.response.data);\n  } else {\n    console.log(error.message);\n  }\n}",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/images/introduction"
        }
    },
    {
        "page_content": "Image generationLearn how to generate or manipulate images with our DALL\u00b7E modelsIntroductionThe Images API provides three methods for interacting with images:Creating images from scratch based on a text promptCreating edits of an existing image based on a new text promptCreating variations of an existing imageThis guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app.UsageGenerationsThe image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter.Generate an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create(\n  prompt=\"a white siamese cat\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example:PromptGenerationa white siamese cata close up, studio photographic portrait of a white siamese cat that looks curious, backlit earsEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.EditsThe image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app.Edit an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\nresponse = openai.Image.create_edit(\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageMaskOutputPrompt: a sunlit indoor lounge area with a pool containing a flamingoThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above.VariationsThe image variations endpoint allows you to generate a variation of a given image.Generate an image variationpythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create_variation(\n  image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageOutputSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.Content moderationPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center.Language-specific tipsNode.jsPythonUsing in-memory image dataThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:1\n2\n3\n4\n5\n6\n7\n8\n9\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  buffer,\n  1,\n  \"1024x1024\"\n);Working with TypeScriptIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:1\n2\n3\n4\n5\n6\n// Cast the ReadStream to `any` to appease the TypeScript compiler\nconst response = await openai.createImageVariation(\n  fs.createReadStream(\"image.png\") as any,\n  1,\n  \"1024x1024\"\n);And here's a similar example for in-memory image data:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  file,\n  1,\n  \"1024x1024\"\n);Error handlingAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\ntry {\n  const response = await openai.createImageVariation(\n    fs.createReadStream(\"image.png\"),\n    1,\n    \"1024x1024\"\n  );\n  console.log(response.data.data[0].url);\n} catch (error) {\n  if (error.response) {\n    console.log(error.response.status);\n    console.log(error.response.data);\n  } else {\n    console.log(error.message);\n  }\n}",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/images/usage"
        }
    },
    {
        "page_content": "Image generationLearn how to generate or manipulate images with our DALL\u00b7E modelsIntroductionThe Images API provides three methods for interacting with images:Creating images from scratch based on a text promptCreating edits of an existing image based on a new text promptCreating variations of an existing imageThis guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app.UsageGenerationsThe image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter.Generate an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create(\n  prompt=\"a white siamese cat\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example:PromptGenerationa white siamese cata close up, studio photographic portrait of a white siamese cat that looks curious, backlit earsEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.EditsThe image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app.Edit an imagepythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\nresponse = openai.Image.create_edit(\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageMaskOutputPrompt: a sunlit indoor lounge area with a pool containing a flamingoThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above.VariationsThe image variations endpoint allows you to generate a variation of a given image.Generate an image variationpythonSelect librarypythonnode.jscurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Image.create_variation(\n  image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']ImageOutputSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.Content moderationPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center.Language-specific tipsNode.jsPythonUsing in-memory image dataThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:1\n2\n3\n4\n5\n6\n7\n8\n9\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  buffer,\n  1,\n  \"1024x1024\"\n);Working with TypeScriptIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:1\n2\n3\n4\n5\n6\n// Cast the ReadStream to `any` to appease the TypeScript compiler\nconst response = await openai.createImageVariation(\n  fs.createReadStream(\"image.png\") as any,\n  1,\n  \"1024x1024\"\n);And here's a similar example for in-memory image data:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\nconst response = await openai.createImageVariation(\n  file,\n  1,\n  \"1024x1024\"\n);Error handlingAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\ntry {\n  const response = await openai.createImageVariation(\n    fs.createReadStream(\"image.png\"),\n    1,\n    \"1024x1024\"\n  );\n  console.log(response.data.data[0].url);\n} catch (error) {\n  if (error.response) {\n    console.log(error.response.status);\n    console.log(error.response.data);\n  } else {\n    console.log(error.message);\n  }\n}",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/images/language-specific-tips"
        }
    },
    {
        "page_content": "Fine-tuningLearn how to customize a model for your application.IntroductionOn July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models.Fine-tuning lets you get more out of the models available through the API by providing:Higher quality results than prompt designAbility to train on more examples than can fit in a promptToken savings due to shorter promptsLower latency requestsGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.At a high level, fine-tuning involves the following steps:Prepare and upload training dataTrain a new fine-tuned modelUse your fine-tuned modelVisit our pricing page to learn more about how fine-tuned model training and usage are billed.What models can be fine-tuned?We are working on safely enabling fine-tuning for GPT-4 and GPT-3.5 Turbo and expect this feature to be available later this year.Fine-tuning is currently only available for the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.InstallationWe recommend using our OpenAI command-line interface (CLI). To install this, runpip install --upgrade openai(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"Prepare training dataTraining data is how you teach GPT-3 what you'd like it to say.Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.CLI data preparation toolWe developed a tool which validates, gives suggestions and reformats your data:openai tools fine_tunes.prepare_data -f <LOCAL_FILE>This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.Create a fine-tuned modelThe following assumes you've already prepared training data following the above instructions.Start your fine-tuning job using the OpenAI CLI:openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.Running the above command does several things:Uploads the file using the files API (or uses an already-uploaded file)Creates a fine-tune jobStreams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>When the job is done, it should display the name of the fine-tuned model.In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>Use a fine-tuned modelWhen a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.You can start making requests by passing the model name as the model parameter of a completion request:OpenAI CLI:openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>cURL:1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'Python:1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)Node.js:1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.Delete a fine-tuned modelTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.OpenAI CLI:openai api models.delete -i <FINE_TUNED_MODEL>cURL:curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"Python:import openai\nopenai.Model.delete(FINE_TUNED_MODEL)Preparing your datasetFine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.Data formattingTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.General best practicesFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.Specific guidelinesFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.ClassificationIs the model making untrue statements?Sentiment analysisCategorization for email triageConditional generationWrite an engaging ad based on a Wikipedia articleEntity extractionCustomer support chatbotProduct description based on a technical list of propertiesClassificationIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separatorAim for at least ~100 examples per classTo get class log probabilities you can specify logprobs=5 (for 5 classes) when using your modelEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forCase study: Is the model making untrue statements?Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.The dataset might look something like the following:{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return either  yes or  no.Case study: Sentiment analysisLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}Case study: Categorization for Email triageLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.Conditional generationConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Use an ending token at the end of the completion, e.g.  ENDRemember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]Aim for at least ~500 examplesEnsure that the prompt + completion doesn't exceed 2048 tokens, including the separatorEnsure the examples are of high quality and follow the same desired formatEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forUsing Lower learning rate and only 1-2 epochs tends to work better for these use casesCase study: Write an engaging ad based on a Wikipedia articleThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.Case study: Entity extractionThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.Case study: Customer support chatbotA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.Case study: Product description based on a technical list of propertiesHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}Won't work as well as:1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.Advanced usageCustomize your model nameYou can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.OpenAI CLI:openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"The resulting name would be:ada:ft-your-org:custom-model-name-2022-02-15-04-21-04Analyzing your fine-tuned modelWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:OpenAI CLI:openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>CURL:curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csvThe _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:elapsed_tokens: the number of tokens the model has seen so far (including repeats)elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.training_loss: loss on the training batchtraining_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83Classification specific metricsWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.OpenAI CLI:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>The following metrics will be displayed in your results file if you set --compute_classification_metrics:For multiclass classificationclassification/accuracy: accuracyclassification/weighted_f1_score: weighted F-1 scoreFor binary classificationThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)classification/accuracyclassification/precisionclassification/recallclassification/f{beta}classification/auroc - AUROCclassification/auprc - AUPRCNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.ValidationYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.OpenAI CLI:1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:validation_loss: loss on the validation batchvalidation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83HyperparametersWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1Continue fine-tuning from a fine-tuned modelIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.Weights & BiasesYou can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:pip install --upgrade openai wandbTo sync your fine-tunes with Weights & Biases, run:openai wandb syncYou can read the Weights & Biases documentation for more information on this integration.Example notebooksClassificationfinetuning-classification.ipynbThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:Data exploration will give an overview of the data source and what an example looks likeData preparation will turn our data source into a jsonl file that can be used for fine-tuningFine-tuning will kick off the fine-tuning job and explain the resulting model's performanceUsing the model will demonstrate making requests to the fine-tuned model to get predictions.Collapse\u200dQuestion answeringolympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynbThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".We will perform this task in three notebooks:The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/fine-tuning"
        }
    },
    {
        "page_content": "Fine-tuningLearn how to customize a model for your application.IntroductionOn July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models.Fine-tuning lets you get more out of the models available through the API by providing:Higher quality results than prompt designAbility to train on more examples than can fit in a promptToken savings due to shorter promptsLower latency requestsGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.At a high level, fine-tuning involves the following steps:Prepare and upload training dataTrain a new fine-tuned modelUse your fine-tuned modelVisit our pricing page to learn more about how fine-tuned model training and usage are billed.What models can be fine-tuned?We are working on safely enabling fine-tuning for GPT-4 and GPT-3.5 Turbo and expect this feature to be available later this year.Fine-tuning is currently only available for the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.InstallationWe recommend using our OpenAI command-line interface (CLI). To install this, runpip install --upgrade openai(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"Prepare training dataTraining data is how you teach GPT-3 what you'd like it to say.Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.CLI data preparation toolWe developed a tool which validates, gives suggestions and reformats your data:openai tools fine_tunes.prepare_data -f <LOCAL_FILE>This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.Create a fine-tuned modelThe following assumes you've already prepared training data following the above instructions.Start your fine-tuning job using the OpenAI CLI:openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.Running the above command does several things:Uploads the file using the files API (or uses an already-uploaded file)Creates a fine-tune jobStreams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>When the job is done, it should display the name of the fine-tuned model.In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>Use a fine-tuned modelWhen a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.You can start making requests by passing the model name as the model parameter of a completion request:OpenAI CLI:openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>cURL:1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'Python:1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)Node.js:1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.Delete a fine-tuned modelTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.OpenAI CLI:openai api models.delete -i <FINE_TUNED_MODEL>cURL:curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"Python:import openai\nopenai.Model.delete(FINE_TUNED_MODEL)Preparing your datasetFine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.Data formattingTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.General best practicesFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.Specific guidelinesFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.ClassificationIs the model making untrue statements?Sentiment analysisCategorization for email triageConditional generationWrite an engaging ad based on a Wikipedia articleEntity extractionCustomer support chatbotProduct description based on a technical list of propertiesClassificationIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separatorAim for at least ~100 examples per classTo get class log probabilities you can specify logprobs=5 (for 5 classes) when using your modelEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forCase study: Is the model making untrue statements?Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.The dataset might look something like the following:{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return either  yes or  no.Case study: Sentiment analysisLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}Case study: Categorization for Email triageLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.Conditional generationConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Use an ending token at the end of the completion, e.g.  ENDRemember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]Aim for at least ~500 examplesEnsure that the prompt + completion doesn't exceed 2048 tokens, including the separatorEnsure the examples are of high quality and follow the same desired formatEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forUsing Lower learning rate and only 1-2 epochs tends to work better for these use casesCase study: Write an engaging ad based on a Wikipedia articleThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.Case study: Entity extractionThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.Case study: Customer support chatbotA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.Case study: Product description based on a technical list of propertiesHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}Won't work as well as:1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.Advanced usageCustomize your model nameYou can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.OpenAI CLI:openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"The resulting name would be:ada:ft-your-org:custom-model-name-2022-02-15-04-21-04Analyzing your fine-tuned modelWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:OpenAI CLI:openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>CURL:curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csvThe _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:elapsed_tokens: the number of tokens the model has seen so far (including repeats)elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.training_loss: loss on the training batchtraining_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83Classification specific metricsWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.OpenAI CLI:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>The following metrics will be displayed in your results file if you set --compute_classification_metrics:For multiclass classificationclassification/accuracy: accuracyclassification/weighted_f1_score: weighted F-1 scoreFor binary classificationThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)classification/accuracyclassification/precisionclassification/recallclassification/f{beta}classification/auroc - AUROCclassification/auprc - AUPRCNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.ValidationYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.OpenAI CLI:1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:validation_loss: loss on the validation batchvalidation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83HyperparametersWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1Continue fine-tuning from a fine-tuned modelIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.Weights & BiasesYou can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:pip install --upgrade openai wandbTo sync your fine-tunes with Weights & Biases, run:openai wandb syncYou can read the Weights & Biases documentation for more information on this integration.Example notebooksClassificationfinetuning-classification.ipynbThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:Data exploration will give an overview of the data source and what an example looks likeData preparation will turn our data source into a jsonl file that can be used for fine-tuningFine-tuning will kick off the fine-tuning job and explain the resulting model's performanceUsing the model will demonstrate making requests to the fine-tuned model to get predictions.Collapse\u200dQuestion answeringolympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynbThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".We will perform this task in three notebooks:The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset"
        }
    },
    {
        "page_content": "Fine-tuningLearn how to customize a model for your application.IntroductionOn July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models.Fine-tuning lets you get more out of the models available through the API by providing:Higher quality results than prompt designAbility to train on more examples than can fit in a promptToken savings due to shorter promptsLower latency requestsGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.At a high level, fine-tuning involves the following steps:Prepare and upload training dataTrain a new fine-tuned modelUse your fine-tuned modelVisit our pricing page to learn more about how fine-tuned model training and usage are billed.What models can be fine-tuned?We are working on safely enabling fine-tuning for GPT-4 and GPT-3.5 Turbo and expect this feature to be available later this year.Fine-tuning is currently only available for the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.InstallationWe recommend using our OpenAI command-line interface (CLI). To install this, runpip install --upgrade openai(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"Prepare training dataTraining data is how you teach GPT-3 what you'd like it to say.Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.CLI data preparation toolWe developed a tool which validates, gives suggestions and reformats your data:openai tools fine_tunes.prepare_data -f <LOCAL_FILE>This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.Create a fine-tuned modelThe following assumes you've already prepared training data following the above instructions.Start your fine-tuning job using the OpenAI CLI:openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.Running the above command does several things:Uploads the file using the files API (or uses an already-uploaded file)Creates a fine-tune jobStreams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>When the job is done, it should display the name of the fine-tuned model.In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>Use a fine-tuned modelWhen a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.You can start making requests by passing the model name as the model parameter of a completion request:OpenAI CLI:openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>cURL:1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'Python:1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)Node.js:1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.Delete a fine-tuned modelTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.OpenAI CLI:openai api models.delete -i <FINE_TUNED_MODEL>cURL:curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"Python:import openai\nopenai.Model.delete(FINE_TUNED_MODEL)Preparing your datasetFine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.Data formattingTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.General best practicesFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.Specific guidelinesFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.ClassificationIs the model making untrue statements?Sentiment analysisCategorization for email triageConditional generationWrite an engaging ad based on a Wikipedia articleEntity extractionCustomer support chatbotProduct description based on a technical list of propertiesClassificationIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separatorAim for at least ~100 examples per classTo get class log probabilities you can specify logprobs=5 (for 5 classes) when using your modelEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forCase study: Is the model making untrue statements?Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.The dataset might look something like the following:{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return either  yes or  no.Case study: Sentiment analysisLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}Case study: Categorization for Email triageLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.Conditional generationConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Use an ending token at the end of the completion, e.g.  ENDRemember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]Aim for at least ~500 examplesEnsure that the prompt + completion doesn't exceed 2048 tokens, including the separatorEnsure the examples are of high quality and follow the same desired formatEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forUsing Lower learning rate and only 1-2 epochs tends to work better for these use casesCase study: Write an engaging ad based on a Wikipedia articleThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.Case study: Entity extractionThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.Case study: Customer support chatbotA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.Case study: Product description based on a technical list of propertiesHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}Won't work as well as:1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.Advanced usageCustomize your model nameYou can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.OpenAI CLI:openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"The resulting name would be:ada:ft-your-org:custom-model-name-2022-02-15-04-21-04Analyzing your fine-tuned modelWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:OpenAI CLI:openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>CURL:curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csvThe _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:elapsed_tokens: the number of tokens the model has seen so far (including repeats)elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.training_loss: loss on the training batchtraining_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83Classification specific metricsWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.OpenAI CLI:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>The following metrics will be displayed in your results file if you set --compute_classification_metrics:For multiclass classificationclassification/accuracy: accuracyclassification/weighted_f1_score: weighted F-1 scoreFor binary classificationThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)classification/accuracyclassification/precisionclassification/recallclassification/f{beta}classification/auroc - AUROCclassification/auprc - AUPRCNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.ValidationYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.OpenAI CLI:1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:validation_loss: loss on the validation batchvalidation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83HyperparametersWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1Continue fine-tuning from a fine-tuned modelIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.Weights & BiasesYou can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:pip install --upgrade openai wandbTo sync your fine-tunes with Weights & Biases, run:openai wandb syncYou can read the Weights & Biases documentation for more information on this integration.Example notebooksClassificationfinetuning-classification.ipynbThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:Data exploration will give an overview of the data source and what an example looks likeData preparation will turn our data source into a jsonl file that can be used for fine-tuningFine-tuning will kick off the fine-tuning job and explain the resulting model's performanceUsing the model will demonstrate making requests to the fine-tuned model to get predictions.Collapse\u200dQuestion answeringolympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynbThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".We will perform this task in three notebooks:The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/fine-tuning/advanced-usage"
        }
    },
    {
        "page_content": "Fine-tuningLearn how to customize a model for your application.IntroductionOn July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models.Fine-tuning lets you get more out of the models available through the API by providing:Higher quality results than prompt designAbility to train on more examples than can fit in a promptToken savings due to shorter promptsLower latency requestsGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.At a high level, fine-tuning involves the following steps:Prepare and upload training dataTrain a new fine-tuned modelUse your fine-tuned modelVisit our pricing page to learn more about how fine-tuned model training and usage are billed.What models can be fine-tuned?We are working on safely enabling fine-tuning for GPT-4 and GPT-3.5 Turbo and expect this feature to be available later this year.Fine-tuning is currently only available for the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.InstallationWe recommend using our OpenAI command-line interface (CLI). To install this, runpip install --upgrade openai(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"Prepare training dataTraining data is how you teach GPT-3 what you'd like it to say.Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.CLI data preparation toolWe developed a tool which validates, gives suggestions and reformats your data:openai tools fine_tunes.prepare_data -f <LOCAL_FILE>This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.Create a fine-tuned modelThe following assumes you've already prepared training data following the above instructions.Start your fine-tuning job using the OpenAI CLI:openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.Running the above command does several things:Uploads the file using the files API (or uses an already-uploaded file)Creates a fine-tune jobStreams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>When the job is done, it should display the name of the fine-tuned model.In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>Use a fine-tuned modelWhen a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.You can start making requests by passing the model name as the model parameter of a completion request:OpenAI CLI:openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>cURL:1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'Python:1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)Node.js:1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.Delete a fine-tuned modelTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.OpenAI CLI:openai api models.delete -i <FINE_TUNED_MODEL>cURL:curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"Python:import openai\nopenai.Model.delete(FINE_TUNED_MODEL)Preparing your datasetFine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.Data formattingTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.General best practicesFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.Specific guidelinesFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.ClassificationIs the model making untrue statements?Sentiment analysisCategorization for email triageConditional generationWrite an engaging ad based on a Wikipedia articleEntity extractionCustomer support chatbotProduct description based on a technical list of propertiesClassificationIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separatorAim for at least ~100 examples per classTo get class log probabilities you can specify logprobs=5 (for 5 classes) when using your modelEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forCase study: Is the model making untrue statements?Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.The dataset might look something like the following:{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return either  yes or  no.Case study: Sentiment analysisLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}Case study: Categorization for Email triageLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.Conditional generationConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Use an ending token at the end of the completion, e.g.  ENDRemember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]Aim for at least ~500 examplesEnsure that the prompt + completion doesn't exceed 2048 tokens, including the separatorEnsure the examples are of high quality and follow the same desired formatEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forUsing Lower learning rate and only 1-2 epochs tends to work better for these use casesCase study: Write an engaging ad based on a Wikipedia articleThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.Case study: Entity extractionThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.Case study: Customer support chatbotA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.Case study: Product description based on a technical list of propertiesHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}Won't work as well as:1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.Advanced usageCustomize your model nameYou can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.OpenAI CLI:openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"The resulting name would be:ada:ft-your-org:custom-model-name-2022-02-15-04-21-04Analyzing your fine-tuned modelWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:OpenAI CLI:openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>CURL:curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csvThe _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:elapsed_tokens: the number of tokens the model has seen so far (including repeats)elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.training_loss: loss on the training batchtraining_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83Classification specific metricsWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.OpenAI CLI:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>The following metrics will be displayed in your results file if you set --compute_classification_metrics:For multiclass classificationclassification/accuracy: accuracyclassification/weighted_f1_score: weighted F-1 scoreFor binary classificationThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)classification/accuracyclassification/precisionclassification/recallclassification/f{beta}classification/auroc - AUROCclassification/auprc - AUPRCNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.ValidationYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.OpenAI CLI:1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:validation_loss: loss on the validation batchvalidation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83HyperparametersWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1Continue fine-tuning from a fine-tuned modelIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.Weights & BiasesYou can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:pip install --upgrade openai wandbTo sync your fine-tunes with Weights & Biases, run:openai wandb syncYou can read the Weights & Biases documentation for more information on this integration.Example notebooksClassificationfinetuning-classification.ipynbThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:Data exploration will give an overview of the data source and what an example looks likeData preparation will turn our data source into a jsonl file that can be used for fine-tuningFine-tuning will kick off the fine-tuning job and explain the resulting model's performanceUsing the model will demonstrate making requests to the fine-tuned model to get predictions.Collapse\u200dQuestion answeringolympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynbThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".We will perform this task in three notebooks:The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/fine-tuning/weights-biases"
        }
    },
    {
        "page_content": "Fine-tuningLearn how to customize a model for your application.IntroductionOn July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models.Fine-tuning lets you get more out of the models available through the API by providing:Higher quality results than prompt designAbility to train on more examples than can fit in a promptToken savings due to shorter promptsLower latency requestsGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. This saves costs and enables lower-latency requests.At a high level, fine-tuning involves the following steps:Prepare and upload training dataTrain a new fine-tuned modelUse your fine-tuned modelVisit our pricing page to learn more about how fine-tuned model training and usage are billed.What models can be fine-tuned?We are working on safely enabling fine-tuning for GPT-4 and GPT-3.5 Turbo and expect this feature to be available later this year.Fine-tuning is currently only available for the following base models: davinci, curie, babbage, and ada. These are the original models that do not have any instruction following training (like text-davinci-003 does for example). You are also able to continue fine-tuning a fine-tuned model to add additional data without having to start from scratch.InstallationWe recommend using our OpenAI command-line interface (CLI). To install this, runpip install --upgrade openai(The following instructions work for version 0.9.4 and up. Additionally, the OpenAI CLI requires python 3.)Set your OPENAI_API_KEY environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"Prepare training dataTraining data is how you teach GPT-3 what you'd like it to say.Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. You can use our CLI data preparation tool to easily convert your data into this file format.1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...Designing your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.For more detailed guidance on how to prepare training data for various tasks, please refer to our preparing your dataset best practices.The more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.CLI data preparation toolWe developed a tool which validates, gives suggestions and reformats your data:openai tools fine_tunes.prepare_data -f <LOCAL_FILE>This tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a CSV, TSV, XLSX, JSON or JSONL file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.Create a fine-tuned modelThe following assumes you've already prepared training data following the above instructions.Start your fine-tuning job using the OpenAI CLI:openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>Where BASE_MODEL is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the suffix parameter.Running the above command does several things:Uploads the file using the files API (or uses an already-uploaded file)Creates a fine-tune jobStreams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)Every fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: ada, babbage, curie, or davinci. Visit our pricing page for details on fine-tune rates.After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>When the job is done, it should display the name of the fine-tuned model.In addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>Use a fine-tuned modelWhen a job has succeeded, the fine_tuned_model field will be populated with the name of the model. You may now specify this model as a parameter to our Completions API, and make requests to it using the Playground.After your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.You can start making requests by passing the model name as the model parameter of a completion request:OpenAI CLI:openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>cURL:1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'Python:1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)Node.js:1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});You may continue to use all the other Completions parameters like temperature, frequency_penalty, presence_penalty, etc, on these requests to fine-tuned models.Delete a fine-tuned modelTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.OpenAI CLI:openai api models.delete -i <FINE_TUNED_MODEL>cURL:curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"Python:import openai\nopenai.Model.delete(FINE_TUNED_MODEL)Preparing your datasetFine-tuning is a powerful technique to create a new model that's specific to your use case. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.Data formattingTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.General best practicesFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.Classifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.If you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.Specific guidelinesFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.ClassificationIs the model making untrue statements?Sentiment analysisCategorization for email triageConditional generationWrite an engaging ad based on a Wikipedia articleEntity extractionCustomer support chatbotProduct description based on a technical list of propertiesClassificationIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Choose classes that map to a single token. At inference time, specify max_tokens=1 since you only need the first token for classification.Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separatorAim for at least ~100 examples per classTo get class log probabilities you can specify logprobs=5 (for 5 classes) when using your modelEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forCase study: Is the model making untrue statements?Let's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.The dataset might look something like the following:{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}In the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used \\nSupported: which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.For this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return either  yes or  no.Case study: Sentiment analysisLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment.Now we can query our model by making a Completion request.1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'Which will return:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}Case study: Categorization for Email triageLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.Conditional generationConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.Use an ending token at the end of the completion, e.g.  ENDRemember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]Aim for at least ~500 examplesEnsure that the prompt + completion doesn't exceed 2048 tokens, including the separatorEnsure the examples are of high quality and follow the same desired formatEnsure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used forUsing Lower learning rate and only 1-2 epochs tends to work better for these use casesCase study: Write an engaging ad based on a Wikipedia articleThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.Case study: Entity extractionThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}For example:1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.Case study: Customer support chatbotA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference.Case study: Product description based on a technical list of propertiesHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}Won't work as well as:1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.Advanced usageCustomize your model nameYou can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter.OpenAI CLI:openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"The resulting name would be:ada:ft-your-org:custom-model-name-2022-02-15-04-21-04Analyzing your fine-tuned modelWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:OpenAI CLI:openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>CURL:curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csvThe _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:elapsed_tokens: the number of tokens the model has seen so far (including repeats)elapsed_examples: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.training_loss: loss on the training batchtraining_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83Classification specific metricsWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification.OpenAI CLI:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>The following metrics will be displayed in your results file if you set --compute_classification_metrics:For multiclass classificationclassification/accuracy: accuracyclassification/weighted_f1_score: weighted F-1 scoreFor binary classificationThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)classification/accuracyclassification/precisionclassification/recallclassification/f{beta}classification/auroc - AUROCclassification/auprc - AUPRCNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.ValidationYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.OpenAI CLI:1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:validation_loss: loss on the validation batchvalidation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83HyperparametersWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation.n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1Continue fine-tuning from a fine-tuned modelIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.Weights & BiasesYou can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets.To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run:pip install --upgrade openai wandbTo sync your fine-tunes with Weights & Biases, run:openai wandb syncYou can read the Weights & Biases documentation for more information on this integration.Example notebooksClassificationfinetuning-classification.ipynbThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook:Data exploration will give an overview of the data source and what an example looks likeData preparation will turn our data source into a jsonl file that can be used for fine-tuningFine-tuning will kick off the fine-tuning job and explain the resulting model's performanceUsing the model will demonstrate making requests to the fine-tuned model to get predictions.Collapse\u200dQuestion answeringolympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynbThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".We will perform this task in three notebooks:The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/fine-tuning/example-notebooks"
        }
    },
    {
        "page_content": "EmbeddingsWhat are embeddings?OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:Search (where results are ranked by relevance to a query string)Clustering (where text strings are grouped by similarity)Recommendations (where items with related text strings are recommended)Anomaly detection (where outliers with little relatedness are identified)Diversity measurement (where similarity distributions are analyzed)Classification (where text strings are classified by their most similar label)An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.To see embeddings in action, check out our code samplesClassificationTopic clusteringSearchRecommendationsBrowse Samples\u200dHow to get embeddingsTo get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.Example requests:Example: Getting embeddingscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\n6\n7\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-ada-002\"\n  }'Example response:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ...\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}See more Python code examples in the OpenAI Cookbook.When using OpenAI embeddings, please keep in mind their limitations and risks.Embedding modelsOpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement.Model generationtokenizermax input tokensknowledge cutoffV2cl100k_base8191Sep 2021V1GPT-2/GPT-32046Aug 2020Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):ModelRough pages per dollarExample performance on BEIR search evaltext-embedding-ada-002300053.9*-davinci-*-001652.8*-curie-*-0016050.9*-babbage-*-00124050.4*-ada-*-00130049.0Second-generation modelsModel nametokenizermax input tokensoutput dimensionstext-embedding-ada-002cl100k_base81911536First-generation models (not recommended)All first-generation models (those ending in -001) use the GPT-3 tokenizer and have a max input of 2046 tokens.First-generation embeddings are generated by five different model families tuned for three different tasks: text search, text similarity and code search. The search models come in pairs: one for short queries and one for long documents. Each family includes up to four models on a spectrum of quality and speed:Modeloutput dimensionsAda1024Babbage2048Curie4096Davinci12288Davinci is the most capable, but is slower and more expensive than the other models. Ada is the least capable, but is significantly faster and cheaper.Similarity embeddingsSimilarity models are best at capturing semantic similarity between pieces of text.Use casesAvailable modelsClustering, regression, anomaly detection, visualizationtext-similarity-ada-001text-similarity-babbage-001text-similarity-curie-001 text-similarity-davinci-001Text search embeddingsText search models help measure which long documents are most relevant to a short search query. Two models are used: one for embedding the search query and one for embedding the documents to be ranked. The document embeddings closest to the query embedding should be the most relevant.Use casesAvailable modelsSearch, context relevance, information retrievaltext-search-ada-doc-001text-search-ada-query-001text-search-babbage-doc-001text-search-babbage-query-001text-search-curie-doc-001text-search-curie-query-001text-search-davinci-doc-001text-search-davinci-query-001Code search embeddingsSimilarly to search embeddings, there are two types: one for embedding natural language search queries and one for embedding code snippets to be retrieved.Use casesAvailable modelsCode search and relevancecode-search-ada-code-001code-search-ada-text-001code-search-babbage-code-001code-search-babbage-text-001With the -001 text embeddings (not -002, and not code embeddings), we suggest replacing newlines (\\n) in your input with a single space, as we have seen worse results when newlines are present.Collapse\u200dUse casesHere we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.Obtaining the embeddingsThe dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:Product IdUser IdScoreSummaryTextB001E4KFG0A3SGXH7AUHU8GW5Good Quality Dog FoodI have bought several of the Vitality canned...B00813GRG4A1D87F6ZCVE5NK1Not as AdvertisedProduct arrived labeled as Jumbo Salted Peanut...We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.Obtain_dataset.ipynb1\n2\n3\n4\n5\n6\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)To load the data from a saved file, you can run the following:1\n2\n3\n4\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)Data visualization in 2DVisualizing_embeddings_in_2D.ipynbThe size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.We color the individual reviews based on the star rating which the reviewer has given:1-star: red2-star: dark orange3-star: gold4-star: turquoise5-star: dark greenThe visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\nmatrix = df.ada_embedding.apply(eval).to_list()\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title(\"Amazon ratings visualized in language using t-SNE\")Collapse\u200dEmbedding as a text feature encoder for ML algorithmsRegression_using_embeddings.ipynbAn embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.1\n2\n3\n4\n5\n6\n7\n8\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)Regression using the embedding featuresEmbeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer\u2019s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.1\n2\n3\n4\n5\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)Collapse\u200dClassification using the embedding featuresClassification_using_embeddings.ipynbThis time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.1\n2\n3\n4\n5\n6\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)Collapse\u200dZero-shot classificationZero-shot_classification_with_embeddings.ipynbWe can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'Collapse\u200dObtaining user and product embeddings for cold-start recommendationUser_and_product_embeddings.ipynbWe can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)Collapse\u200dClusteringClustering.ipynbClustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_Collapse\u200dText search using embeddingsSemantic_text_search_using_embeddings.ipynbTo retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.1\n2\n3\n4\n5\n6\n7\n8\n9\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-ada-002')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)Collapse\u200dCode search using embeddingsCode_search.ipynbCode search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-ada-002 model.To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-ada-002')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)Collapse\u200dRecommendations using embeddingsRecommendation_using_embeddings.ipynbBecause shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\ndef recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model=\"text-embedding-ada-002\",\n) -> List[int]:\n   \"\"\"Return nearest neighbors of a given string.\"\"\"\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighborsCollapse\u200dLimitations & risksOur embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.Social biasLimitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.Please contact our support team via chat if you have any questions; we are happy to advise on this.Blindness to recent eventsLimitation: Models lack knowledge of events that occurred after August 2020.Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.Frequently asked questionsHow can I tell how many tokens a string has before I embed it?In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.Example code:1\n2\n3\n4\n5\n6\n7\n8\n9\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.How can I retrieve K nearest embedding vectors quickly?For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.Vector database options include:Chroma, an open-source embeddings storeMilvus, a vector database built for scalable similarity searchPinecone, a fully managed vector databaseQdrant, a vector search engineRedis as a vector databaseTypesense, fast open source vector searchWeaviate, an open-source vector search engineZilliz, data infrastructure, powered by MilvusWhich distance function should I use?We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much.OpenAI embeddings are normalized to length 1, which means that:Cosine similarity can be computed slightly faster using just a dot productCosine similarity and Euclidean distance will result in the identical rankingsCan I share my embeddings online?Customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/embeddings"
        }
    },
    {
        "page_content": "EmbeddingsWhat are embeddings?OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:Search (where results are ranked by relevance to a query string)Clustering (where text strings are grouped by similarity)Recommendations (where items with related text strings are recommended)Anomaly detection (where outliers with little relatedness are identified)Diversity measurement (where similarity distributions are analyzed)Classification (where text strings are classified by their most similar label)An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.To see embeddings in action, check out our code samplesClassificationTopic clusteringSearchRecommendationsBrowse Samples\u200dHow to get embeddingsTo get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.Example requests:Example: Getting embeddingscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\n6\n7\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-ada-002\"\n  }'Example response:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ...\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}See more Python code examples in the OpenAI Cookbook.When using OpenAI embeddings, please keep in mind their limitations and risks.Embedding modelsOpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement.Model generationtokenizermax input tokensknowledge cutoffV2cl100k_base8191Sep 2021V1GPT-2/GPT-32046Aug 2020Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):ModelRough pages per dollarExample performance on BEIR search evaltext-embedding-ada-002300053.9*-davinci-*-001652.8*-curie-*-0016050.9*-babbage-*-00124050.4*-ada-*-00130049.0Second-generation modelsModel nametokenizermax input tokensoutput dimensionstext-embedding-ada-002cl100k_base81911536First-generation models (not recommended)All first-generation models (those ending in -001) use the GPT-3 tokenizer and have a max input of 2046 tokens.First-generation embeddings are generated by five different model families tuned for three different tasks: text search, text similarity and code search. The search models come in pairs: one for short queries and one for long documents. Each family includes up to four models on a spectrum of quality and speed:Modeloutput dimensionsAda1024Babbage2048Curie4096Davinci12288Davinci is the most capable, but is slower and more expensive than the other models. Ada is the least capable, but is significantly faster and cheaper.Similarity embeddingsSimilarity models are best at capturing semantic similarity between pieces of text.Use casesAvailable modelsClustering, regression, anomaly detection, visualizationtext-similarity-ada-001text-similarity-babbage-001text-similarity-curie-001 text-similarity-davinci-001Text search embeddingsText search models help measure which long documents are most relevant to a short search query. Two models are used: one for embedding the search query and one for embedding the documents to be ranked. The document embeddings closest to the query embedding should be the most relevant.Use casesAvailable modelsSearch, context relevance, information retrievaltext-search-ada-doc-001text-search-ada-query-001text-search-babbage-doc-001text-search-babbage-query-001text-search-curie-doc-001text-search-curie-query-001text-search-davinci-doc-001text-search-davinci-query-001Code search embeddingsSimilarly to search embeddings, there are two types: one for embedding natural language search queries and one for embedding code snippets to be retrieved.Use casesAvailable modelsCode search and relevancecode-search-ada-code-001code-search-ada-text-001code-search-babbage-code-001code-search-babbage-text-001With the -001 text embeddings (not -002, and not code embeddings), we suggest replacing newlines (\\n) in your input with a single space, as we have seen worse results when newlines are present.Collapse\u200dUse casesHere we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.Obtaining the embeddingsThe dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:Product IdUser IdScoreSummaryTextB001E4KFG0A3SGXH7AUHU8GW5Good Quality Dog FoodI have bought several of the Vitality canned...B00813GRG4A1D87F6ZCVE5NK1Not as AdvertisedProduct arrived labeled as Jumbo Salted Peanut...We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.Obtain_dataset.ipynb1\n2\n3\n4\n5\n6\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)To load the data from a saved file, you can run the following:1\n2\n3\n4\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)Data visualization in 2DVisualizing_embeddings_in_2D.ipynbThe size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.We color the individual reviews based on the star rating which the reviewer has given:1-star: red2-star: dark orange3-star: gold4-star: turquoise5-star: dark greenThe visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\nmatrix = df.ada_embedding.apply(eval).to_list()\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title(\"Amazon ratings visualized in language using t-SNE\")Collapse\u200dEmbedding as a text feature encoder for ML algorithmsRegression_using_embeddings.ipynbAn embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.1\n2\n3\n4\n5\n6\n7\n8\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)Regression using the embedding featuresEmbeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer\u2019s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.1\n2\n3\n4\n5\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)Collapse\u200dClassification using the embedding featuresClassification_using_embeddings.ipynbThis time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.1\n2\n3\n4\n5\n6\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)Collapse\u200dZero-shot classificationZero-shot_classification_with_embeddings.ipynbWe can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'Collapse\u200dObtaining user and product embeddings for cold-start recommendationUser_and_product_embeddings.ipynbWe can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)Collapse\u200dClusteringClustering.ipynbClustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_Collapse\u200dText search using embeddingsSemantic_text_search_using_embeddings.ipynbTo retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.1\n2\n3\n4\n5\n6\n7\n8\n9\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-ada-002')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)Collapse\u200dCode search using embeddingsCode_search.ipynbCode search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-ada-002 model.To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-ada-002')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)Collapse\u200dRecommendations using embeddingsRecommendation_using_embeddings.ipynbBecause shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\ndef recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model=\"text-embedding-ada-002\",\n) -> List[int]:\n   \"\"\"Return nearest neighbors of a given string.\"\"\"\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighborsCollapse\u200dLimitations & risksOur embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.Social biasLimitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.Please contact our support team via chat if you have any questions; we are happy to advise on this.Blindness to recent eventsLimitation: Models lack knowledge of events that occurred after August 2020.Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.Frequently asked questionsHow can I tell how many tokens a string has before I embed it?In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.Example code:1\n2\n3\n4\n5\n6\n7\n8\n9\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.How can I retrieve K nearest embedding vectors quickly?For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.Vector database options include:Chroma, an open-source embeddings storeMilvus, a vector database built for scalable similarity searchPinecone, a fully managed vector databaseQdrant, a vector search engineRedis as a vector databaseTypesense, fast open source vector searchWeaviate, an open-source vector search engineZilliz, data infrastructure, powered by MilvusWhich distance function should I use?We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much.OpenAI embeddings are normalized to length 1, which means that:Cosine similarity can be computed slightly faster using just a dot productCosine similarity and Euclidean distance will result in the identical rankingsCan I share my embeddings online?Customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"
        }
    },
    {
        "page_content": "EmbeddingsWhat are embeddings?OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:Search (where results are ranked by relevance to a query string)Clustering (where text strings are grouped by similarity)Recommendations (where items with related text strings are recommended)Anomaly detection (where outliers with little relatedness are identified)Diversity measurement (where similarity distributions are analyzed)Classification (where text strings are classified by their most similar label)An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.To see embeddings in action, check out our code samplesClassificationTopic clusteringSearchRecommendationsBrowse Samples\u200dHow to get embeddingsTo get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.Example requests:Example: Getting embeddingscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\n6\n7\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-ada-002\"\n  }'Example response:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ...\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}See more Python code examples in the OpenAI Cookbook.When using OpenAI embeddings, please keep in mind their limitations and risks.Embedding modelsOpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement.Model generationtokenizermax input tokensknowledge cutoffV2cl100k_base8191Sep 2021V1GPT-2/GPT-32046Aug 2020Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):ModelRough pages per dollarExample performance on BEIR search evaltext-embedding-ada-002300053.9*-davinci-*-001652.8*-curie-*-0016050.9*-babbage-*-00124050.4*-ada-*-00130049.0Second-generation modelsModel nametokenizermax input tokensoutput dimensionstext-embedding-ada-002cl100k_base81911536First-generation models (not recommended)All first-generation models (those ending in -001) use the GPT-3 tokenizer and have a max input of 2046 tokens.First-generation embeddings are generated by five different model families tuned for three different tasks: text search, text similarity and code search. The search models come in pairs: one for short queries and one for long documents. Each family includes up to four models on a spectrum of quality and speed:Modeloutput dimensionsAda1024Babbage2048Curie4096Davinci12288Davinci is the most capable, but is slower and more expensive than the other models. Ada is the least capable, but is significantly faster and cheaper.Similarity embeddingsSimilarity models are best at capturing semantic similarity between pieces of text.Use casesAvailable modelsClustering, regression, anomaly detection, visualizationtext-similarity-ada-001text-similarity-babbage-001text-similarity-curie-001 text-similarity-davinci-001Text search embeddingsText search models help measure which long documents are most relevant to a short search query. Two models are used: one for embedding the search query and one for embedding the documents to be ranked. The document embeddings closest to the query embedding should be the most relevant.Use casesAvailable modelsSearch, context relevance, information retrievaltext-search-ada-doc-001text-search-ada-query-001text-search-babbage-doc-001text-search-babbage-query-001text-search-curie-doc-001text-search-curie-query-001text-search-davinci-doc-001text-search-davinci-query-001Code search embeddingsSimilarly to search embeddings, there are two types: one for embedding natural language search queries and one for embedding code snippets to be retrieved.Use casesAvailable modelsCode search and relevancecode-search-ada-code-001code-search-ada-text-001code-search-babbage-code-001code-search-babbage-text-001With the -001 text embeddings (not -002, and not code embeddings), we suggest replacing newlines (\\n) in your input with a single space, as we have seen worse results when newlines are present.Collapse\u200dUse casesHere we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.Obtaining the embeddingsThe dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:Product IdUser IdScoreSummaryTextB001E4KFG0A3SGXH7AUHU8GW5Good Quality Dog FoodI have bought several of the Vitality canned...B00813GRG4A1D87F6ZCVE5NK1Not as AdvertisedProduct arrived labeled as Jumbo Salted Peanut...We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.Obtain_dataset.ipynb1\n2\n3\n4\n5\n6\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)To load the data from a saved file, you can run the following:1\n2\n3\n4\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)Data visualization in 2DVisualizing_embeddings_in_2D.ipynbThe size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.We color the individual reviews based on the star rating which the reviewer has given:1-star: red2-star: dark orange3-star: gold4-star: turquoise5-star: dark greenThe visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\nmatrix = df.ada_embedding.apply(eval).to_list()\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title(\"Amazon ratings visualized in language using t-SNE\")Collapse\u200dEmbedding as a text feature encoder for ML algorithmsRegression_using_embeddings.ipynbAn embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.1\n2\n3\n4\n5\n6\n7\n8\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)Regression using the embedding featuresEmbeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer\u2019s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.1\n2\n3\n4\n5\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)Collapse\u200dClassification using the embedding featuresClassification_using_embeddings.ipynbThis time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.1\n2\n3\n4\n5\n6\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)Collapse\u200dZero-shot classificationZero-shot_classification_with_embeddings.ipynbWe can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'Collapse\u200dObtaining user and product embeddings for cold-start recommendationUser_and_product_embeddings.ipynbWe can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)Collapse\u200dClusteringClustering.ipynbClustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_Collapse\u200dText search using embeddingsSemantic_text_search_using_embeddings.ipynbTo retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.1\n2\n3\n4\n5\n6\n7\n8\n9\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-ada-002')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)Collapse\u200dCode search using embeddingsCode_search.ipynbCode search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-ada-002 model.To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-ada-002')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)Collapse\u200dRecommendations using embeddingsRecommendation_using_embeddings.ipynbBecause shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\ndef recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model=\"text-embedding-ada-002\",\n) -> List[int]:\n   \"\"\"Return nearest neighbors of a given string.\"\"\"\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighborsCollapse\u200dLimitations & risksOur embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.Social biasLimitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.Please contact our support team via chat if you have any questions; we are happy to advise on this.Blindness to recent eventsLimitation: Models lack knowledge of events that occurred after August 2020.Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.Frequently asked questionsHow can I tell how many tokens a string has before I embed it?In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.Example code:1\n2\n3\n4\n5\n6\n7\n8\n9\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.How can I retrieve K nearest embedding vectors quickly?For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.Vector database options include:Chroma, an open-source embeddings storeMilvus, a vector database built for scalable similarity searchPinecone, a fully managed vector databaseQdrant, a vector search engineRedis as a vector databaseTypesense, fast open source vector searchWeaviate, an open-source vector search engineZilliz, data infrastructure, powered by MilvusWhich distance function should I use?We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much.OpenAI embeddings are normalized to length 1, which means that:Cosine similarity can be computed slightly faster using just a dot productCosine similarity and Euclidean distance will result in the identical rankingsCan I share my embeddings online?Customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/embeddings/use-cases"
        }
    },
    {
        "page_content": "EmbeddingsWhat are embeddings?OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:Search (where results are ranked by relevance to a query string)Clustering (where text strings are grouped by similarity)Recommendations (where items with related text strings are recommended)Anomaly detection (where outliers with little relatedness are identified)Diversity measurement (where similarity distributions are analyzed)Classification (where text strings are classified by their most similar label)An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.To see embeddings in action, check out our code samplesClassificationTopic clusteringSearchRecommendationsBrowse Samples\u200dHow to get embeddingsTo get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.Example requests:Example: Getting embeddingscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\n6\n7\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-ada-002\"\n  }'Example response:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ...\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}See more Python code examples in the OpenAI Cookbook.When using OpenAI embeddings, please keep in mind their limitations and risks.Embedding modelsOpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement.Model generationtokenizermax input tokensknowledge cutoffV2cl100k_base8191Sep 2021V1GPT-2/GPT-32046Aug 2020Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):ModelRough pages per dollarExample performance on BEIR search evaltext-embedding-ada-002300053.9*-davinci-*-001652.8*-curie-*-0016050.9*-babbage-*-00124050.4*-ada-*-00130049.0Second-generation modelsModel nametokenizermax input tokensoutput dimensionstext-embedding-ada-002cl100k_base81911536First-generation models (not recommended)All first-generation models (those ending in -001) use the GPT-3 tokenizer and have a max input of 2046 tokens.First-generation embeddings are generated by five different model families tuned for three different tasks: text search, text similarity and code search. The search models come in pairs: one for short queries and one for long documents. Each family includes up to four models on a spectrum of quality and speed:Modeloutput dimensionsAda1024Babbage2048Curie4096Davinci12288Davinci is the most capable, but is slower and more expensive than the other models. Ada is the least capable, but is significantly faster and cheaper.Similarity embeddingsSimilarity models are best at capturing semantic similarity between pieces of text.Use casesAvailable modelsClustering, regression, anomaly detection, visualizationtext-similarity-ada-001text-similarity-babbage-001text-similarity-curie-001 text-similarity-davinci-001Text search embeddingsText search models help measure which long documents are most relevant to a short search query. Two models are used: one for embedding the search query and one for embedding the documents to be ranked. The document embeddings closest to the query embedding should be the most relevant.Use casesAvailable modelsSearch, context relevance, information retrievaltext-search-ada-doc-001text-search-ada-query-001text-search-babbage-doc-001text-search-babbage-query-001text-search-curie-doc-001text-search-curie-query-001text-search-davinci-doc-001text-search-davinci-query-001Code search embeddingsSimilarly to search embeddings, there are two types: one for embedding natural language search queries and one for embedding code snippets to be retrieved.Use casesAvailable modelsCode search and relevancecode-search-ada-code-001code-search-ada-text-001code-search-babbage-code-001code-search-babbage-text-001With the -001 text embeddings (not -002, and not code embeddings), we suggest replacing newlines (\\n) in your input with a single space, as we have seen worse results when newlines are present.Collapse\u200dUse casesHere we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.Obtaining the embeddingsThe dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:Product IdUser IdScoreSummaryTextB001E4KFG0A3SGXH7AUHU8GW5Good Quality Dog FoodI have bought several of the Vitality canned...B00813GRG4A1D87F6ZCVE5NK1Not as AdvertisedProduct arrived labeled as Jumbo Salted Peanut...We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.Obtain_dataset.ipynb1\n2\n3\n4\n5\n6\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)To load the data from a saved file, you can run the following:1\n2\n3\n4\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)Data visualization in 2DVisualizing_embeddings_in_2D.ipynbThe size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.We color the individual reviews based on the star rating which the reviewer has given:1-star: red2-star: dark orange3-star: gold4-star: turquoise5-star: dark greenThe visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\nmatrix = df.ada_embedding.apply(eval).to_list()\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title(\"Amazon ratings visualized in language using t-SNE\")Collapse\u200dEmbedding as a text feature encoder for ML algorithmsRegression_using_embeddings.ipynbAn embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.1\n2\n3\n4\n5\n6\n7\n8\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)Regression using the embedding featuresEmbeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer\u2019s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.1\n2\n3\n4\n5\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)Collapse\u200dClassification using the embedding featuresClassification_using_embeddings.ipynbThis time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.1\n2\n3\n4\n5\n6\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)Collapse\u200dZero-shot classificationZero-shot_classification_with_embeddings.ipynbWe can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'Collapse\u200dObtaining user and product embeddings for cold-start recommendationUser_and_product_embeddings.ipynbWe can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)Collapse\u200dClusteringClustering.ipynbClustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.1\n2\n3\n4\n5\n6\n7\n8\n9\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_Collapse\u200dText search using embeddingsSemantic_text_search_using_embeddings.ipynbTo retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.1\n2\n3\n4\n5\n6\n7\n8\n9\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-ada-002')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)Collapse\u200dCode search using embeddingsCode_search.ipynbCode search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-ada-002 model.To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-ada-002')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)Collapse\u200dRecommendations using embeddingsRecommendation_using_embeddings.ipynbBecause shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\ndef recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model=\"text-embedding-ada-002\",\n) -> List[int]:\n   \"\"\"Return nearest neighbors of a given string.\"\"\"\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighborsCollapse\u200dLimitations & risksOur embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.Social biasLimitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.Please contact our support team via chat if you have any questions; we are happy to advise on this.Blindness to recent eventsLimitation: Models lack knowledge of events that occurred after August 2020.Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.Frequently asked questionsHow can I tell how many tokens a string has before I embed it?In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.Example code:1\n2\n3\n4\n5\n6\n7\n8\n9\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.How can I retrieve K nearest embedding vectors quickly?For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.Vector database options include:Chroma, an open-source embeddings storeMilvus, a vector database built for scalable similarity searchPinecone, a fully managed vector databaseQdrant, a vector search engineRedis as a vector databaseTypesense, fast open source vector searchWeaviate, an open-source vector search engineZilliz, data infrastructure, powered by MilvusWhich distance function should I use?We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much.OpenAI embeddings are normalized to length 1, which means that:Cosine similarity can be computed slightly faster using just a dot productCosine similarity and Euclidean distance will result in the identical rankingsCan I share my embeddings online?Customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/embeddings/limitations-risks"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/overview"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/quickstart"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/supported-languages"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/longer-inputs"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/prompting"
        }
    },
    {
        "page_content": "Speech to textLearn how to turn audio into textIntroductionThe speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:Transcribe audio into whatever language the audio is in.Translate and transcribe the audio into english.File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.QuickstartTranscriptionsThe transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats.Transcribe audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)By default, the response type will be json with the raw text included.{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line:1\n2\n3\n4\n...\n--form file=@openai.mp3 \\\n--form model=whisper-1 \\\n--form response_format=textTranslationsThe translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.Translate audiopythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)In this case, the inputted audio was german and the outputted text looks like:Hello, my name is Wolfgang and I come from Germany. Where are you heading today?We only support translation into english at this time.Supported languagesWe currently support the following languages through both the transcriptions and translations endpoint:Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.Longer inputsBy default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.One way to handle this is to use the PyDub open source Python package to split the audio:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.PromptingYou can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity\"To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source Whisper Python package.Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.Improving reliabilityAs we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:Using the prompt parameterThe first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.transcribe(filepath, prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\")While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution.Collapse\u200dPost-processing with GPT-4The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.We start by providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following.Collapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/speech-to-text/improving-reliability"
        }
    },
    {
        "page_content": "ModerationOverviewThe moderations endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it.The models classifies the following categories:CategoryDescriptionhateContent that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.hate/threateningHateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.harassmentContent that expresses, incites, or promotes harassing language towards any target.harassment/threateningHarassment content that also includes violence or serious harm towards any target.self-harmContent that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/intentContent where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/instructionsContent that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.sexualContent meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).sexual/minorsSexual content that includes an individual who is under 18 years old.violenceContent that depicts death, violence, or physical injury.violence/graphicContent that depicts death, violence, or physical injury in graphic detail.The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently disallow other use cases. Accuracy may be lower on longer pieces of text. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters.We are continuously working to improve the accuracy of our classifier. Our support for non-English languages is currently limited.QuickstartTo obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets:Example: Getting moderationscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\ncurl https://api.openai.com/v1/moderations \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\"input\": \"Sample text goes here\"}'Below is an example output of the endpoint. It returns the following fields:flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise.categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise.category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n{\n  \"id\": \"modr-XXXXX\",\n  \"model\": \"text-moderation-005\",\n  \"results\": [\n    {\n      \"flagged\": true,\n      \"categories\": {\n        \"sexual\": false,\n        \"hate\": false,\n        \"harassment\": false,\n        \"self-harm\": false,\n        \"sexual/minors\": false,\n        \"hate/threatening\": false,\n        \"violence/graphic\": false,\n        \"self-harm/intent\": false,\n        \"self-harm/instructions\": false,\n        \"harassment/threatening\": true,\n        \"violence\": true,\n      },\n      \"category_scores\": {\n        \"sexual\": 1.2282071e-06,\n        \"hate\": 0.010696256,\n        \"harassment\": 0.29842457,\n        \"self-harm\": 1.5236925e-08,\n        \"sexual/minors\": 5.7246268e-08,\n        \"hate/threatening\": 0.0060676364,\n        \"violence/graphic\": 4.435014e-06,\n        \"self-harm/intent\": 8.098441e-10,\n        \"self-harm/instructions\": 2.8498655e-11,\n        \"harassment/threatening\": 0.63055265,\n        \"violence\": 0.99011886,\n      }\n    }\n  ]\n}OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/moderation"
        }
    },
    {
        "page_content": "ModerationOverviewThe moderations endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it.The models classifies the following categories:CategoryDescriptionhateContent that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.hate/threateningHateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.harassmentContent that expresses, incites, or promotes harassing language towards any target.harassment/threateningHarassment content that also includes violence or serious harm towards any target.self-harmContent that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/intentContent where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/instructionsContent that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.sexualContent meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).sexual/minorsSexual content that includes an individual who is under 18 years old.violenceContent that depicts death, violence, or physical injury.violence/graphicContent that depicts death, violence, or physical injury in graphic detail.The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently disallow other use cases. Accuracy may be lower on longer pieces of text. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters.We are continuously working to improve the accuracy of our classifier. Our support for non-English languages is currently limited.QuickstartTo obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets:Example: Getting moderationscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\ncurl https://api.openai.com/v1/moderations \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\"input\": \"Sample text goes here\"}'Below is an example output of the endpoint. It returns the following fields:flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise.categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise.category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n{\n  \"id\": \"modr-XXXXX\",\n  \"model\": \"text-moderation-005\",\n  \"results\": [\n    {\n      \"flagged\": true,\n      \"categories\": {\n        \"sexual\": false,\n        \"hate\": false,\n        \"harassment\": false,\n        \"self-harm\": false,\n        \"sexual/minors\": false,\n        \"hate/threatening\": false,\n        \"violence/graphic\": false,\n        \"self-harm/intent\": false,\n        \"self-harm/instructions\": false,\n        \"harassment/threatening\": true,\n        \"violence\": true,\n      },\n      \"category_scores\": {\n        \"sexual\": 1.2282071e-06,\n        \"hate\": 0.010696256,\n        \"harassment\": 0.29842457,\n        \"self-harm\": 1.5236925e-08,\n        \"sexual/minors\": 5.7246268e-08,\n        \"hate/threatening\": 0.0060676364,\n        \"violence/graphic\": 4.435014e-06,\n        \"self-harm/intent\": 8.098441e-10,\n        \"self-harm/instructions\": 2.8498655e-11,\n        \"harassment/threatening\": 0.63055265,\n        \"violence\": 0.99011886,\n      }\n    }\n  ]\n}OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/moderation/overview"
        }
    },
    {
        "page_content": "ModerationOverviewThe moderations endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it.The models classifies the following categories:CategoryDescriptionhateContent that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.hate/threateningHateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.harassmentContent that expresses, incites, or promotes harassing language towards any target.harassment/threateningHarassment content that also includes violence or serious harm towards any target.self-harmContent that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/intentContent where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.self-harm/instructionsContent that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.sexualContent meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).sexual/minorsSexual content that includes an individual who is under 18 years old.violenceContent that depicts death, violence, or physical injury.violence/graphicContent that depicts death, violence, or physical injury in graphic detail.The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently disallow other use cases. Accuracy may be lower on longer pieces of text. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters.We are continuously working to improve the accuracy of our classifier. Our support for non-English languages is currently limited.QuickstartTo obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets:Example: Getting moderationscurlSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\ncurl https://api.openai.com/v1/moderations \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\"input\": \"Sample text goes here\"}'Below is an example output of the endpoint. It returns the following fields:flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise.categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise.category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n{\n  \"id\": \"modr-XXXXX\",\n  \"model\": \"text-moderation-005\",\n  \"results\": [\n    {\n      \"flagged\": true,\n      \"categories\": {\n        \"sexual\": false,\n        \"hate\": false,\n        \"harassment\": false,\n        \"self-harm\": false,\n        \"sexual/minors\": false,\n        \"hate/threatening\": false,\n        \"violence/graphic\": false,\n        \"self-harm/intent\": false,\n        \"self-harm/instructions\": false,\n        \"harassment/threatening\": true,\n        \"violence\": true,\n      },\n      \"category_scores\": {\n        \"sexual\": 1.2282071e-06,\n        \"hate\": 0.010696256,\n        \"harassment\": 0.29842457,\n        \"self-harm\": 1.5236925e-08,\n        \"sexual/minors\": 5.7246268e-08,\n        \"hate/threatening\": 0.0060676364,\n        \"violence/graphic\": 4.435014e-06,\n        \"self-harm/intent\": 8.098441e-10,\n        \"self-harm/instructions\": 2.8498655e-11,\n        \"harassment/threatening\": 0.63055265,\n        \"violence\": 0.99011886,\n      }\n    }\n  ]\n}OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/moderation/quickstart"
        }
    },
    {
        "page_content": "Rate limitsOverviewWhat are rate limits?A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.Why do we have rate limits?Rate limits are a common practice for APIs, and they're put in place for a few different reasons:They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section.What are the rate limits for our API?You can view the rate limits for your organization under the rate limits section of the account management page.We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: RPM (requests per minute), RPD (requests per day), and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form.Text & EmbeddingChatEdit (Deprecated)ImageAudioFree trial users200 RPD  3 RPM  150,000 TPM200 RPD  3 RPM  40,000 TPM200 RPD  3 RPM  150,000 TPM200 RPD  5 images / min200 RPD  3 RPMPay-as-you-go users (first 48 hours)2,000 RPD  60 RPM  250,000 TPM2,000 RPD  60 RPM  60,000 TPM2,000 RPD  20 RPM  150,000 TPM2,000 RPD  50 images / min 2,000 RPD  50 RPM Pay-as-you-go users (after 48 hours)3,500 RPM  350,000 TPM3,500 RPM  90,000 TPM20 RPM  150,000 TPM50 images / min50 RPMFor free trial users and pay-as-you-go users in the first 48 hour, we have added per day rate limits. The normal RPM and TPM limits apply but there is also a separate limits on the number of requests per day for each endpoint.For gpt-3.5-turbo-16k, the TPM limit for pay-as-you-go users is 2x the listed values above, making the limits 120K TPM and 180K TPM respectively.For our older models, the TPM (tokens per minute) unit is different depending on the model version:type1 TPM equalsdavinci1 token per minutecurie25 tokens per minutebabbage100 tokens per minuteada200 tokens per minuteIn practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model.It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Edit endpoint and that would fill your limit, even if you did not send 150k tokens within those 20 requests.GPT-4 rate limitsDuring the rollout of GPT-4, the model will have more aggressive rate limits to keep up with demand. You can view your current rate limits in the rate limits section of the account page.\nWe are unable to accommodate requests for rate limit increases due to capacity constraints. We are prioritizing general access to GPT-4 first and will subsequently raise rate limits automatically as capacity allows.How do rate limits work?If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.Rate limits in headersIn addition to seeing your rate limit on your account page, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.You can expect to see the following header fields:FieldSample ValueDescriptionx-ratelimit-limit-requests60The maximum number of requests that are permitted before exhausting the rate limit.x-ratelimit-limit-tokens150000The maximum number of tokens that are permitted before exhausting the rate limit.x-ratelimit-remaining-requests59The remaining number of requests that are permitted before exhausting the rate limit.x-ratelimit-remaining-tokens149984The remaining number of tokens that are permitted before exhausting the rate limit.x-ratelimit-reset-requests1sThe time until the rate limit (based on requests) resets to its initial state.x-ratelimit-reset-tokens6m0sThe time until the rate limit (based on tokens) resets to its initial state.What happens if I hit a rate limit error?Rate limit errors look like this:Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.Rate limits vs max_tokensEach model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request.Error MitigationWhat are some steps I can take to mitigate this?The OpenAI Cookbook has a Python notebook that explains how to avoid rate limit errors, as well an example Python script for staying under rate limits while batch processing API requests.You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.Retrying with exponential backoffOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:Automatic retries means you can recover from rate limit errors without crashes or missing dataExponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries failAdding random jitter to the delay helps retries from all hitting at the same time.Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.Below are a few example solutions for Python that use exponential backoff.Example #1: Using the Tenacity libraryTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletion_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample #2: Using the backoff libraryAnother python library that provides function decorators for backoff and retry is backoff:Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\nimport backoff \nimport openai \n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletions_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample 3: Manual backoff implementationIf you don't want to use third-party libraries, you can implement your own backoff logic following this example:Using manual backoff implementationpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n# imports\nimport random\nimport time\n \nimport openai\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.Collapse\u200dReduce the max_tokens to match the size of your completionsYour rate limit is calculated as the maximum of max_tokens and the estimated number of tokens based on the character count of your request. Try to set the max_tokens value as close to your expected response size as possible.Batching requestsThe OpenAI API has separate limits for requests per minute and tokens per minute.If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.Example without batchingNo batchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport openai\n \nnum_stories = 10\nprompt = \"Once upon a time,\"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)Collapse\u200dExample with batchingBatchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport openai  # for making OpenAI API requests\n \n \nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)Collapse\u200d Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.Request IncreaseWhen should I consider applying for a rate limit increase?Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.Will my rate limit increase request be rejected?A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit?We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form.\nTo help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.Here are some examples of how you might fill out this form:DALL-E API examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costDALL-E APIN/A501000Our app is currently in production and based on our past traffic, we make about 10 requests per minute.$60DALL-E APIN/A15010,000Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks!$180Collapse\u200dLanguage model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costgpt-3.5-turbo325,0004,00050We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage.$390gpt-4750,00010,00010,000Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point.$900Collapse\u200dCode model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costcode-davinci-002150,0001,00015We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...]Codex models are now deprecated and no longer accessible.Collapse\u200dPlease note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/rate-limits"
        }
    },
    {
        "page_content": "Rate limitsOverviewWhat are rate limits?A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.Why do we have rate limits?Rate limits are a common practice for APIs, and they're put in place for a few different reasons:They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section.What are the rate limits for our API?You can view the rate limits for your organization under the rate limits section of the account management page.We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: RPM (requests per minute), RPD (requests per day), and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form.Text & EmbeddingChatEdit (Deprecated)ImageAudioFree trial users200 RPD  3 RPM  150,000 TPM200 RPD  3 RPM  40,000 TPM200 RPD  3 RPM  150,000 TPM200 RPD  5 images / min200 RPD  3 RPMPay-as-you-go users (first 48 hours)2,000 RPD  60 RPM  250,000 TPM2,000 RPD  60 RPM  60,000 TPM2,000 RPD  20 RPM  150,000 TPM2,000 RPD  50 images / min 2,000 RPD  50 RPM Pay-as-you-go users (after 48 hours)3,500 RPM  350,000 TPM3,500 RPM  90,000 TPM20 RPM  150,000 TPM50 images / min50 RPMFor free trial users and pay-as-you-go users in the first 48 hour, we have added per day rate limits. The normal RPM and TPM limits apply but there is also a separate limits on the number of requests per day for each endpoint.For gpt-3.5-turbo-16k, the TPM limit for pay-as-you-go users is 2x the listed values above, making the limits 120K TPM and 180K TPM respectively.For our older models, the TPM (tokens per minute) unit is different depending on the model version:type1 TPM equalsdavinci1 token per minutecurie25 tokens per minutebabbage100 tokens per minuteada200 tokens per minuteIn practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model.It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Edit endpoint and that would fill your limit, even if you did not send 150k tokens within those 20 requests.GPT-4 rate limitsDuring the rollout of GPT-4, the model will have more aggressive rate limits to keep up with demand. You can view your current rate limits in the rate limits section of the account page.\nWe are unable to accommodate requests for rate limit increases due to capacity constraints. We are prioritizing general access to GPT-4 first and will subsequently raise rate limits automatically as capacity allows.How do rate limits work?If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.Rate limits in headersIn addition to seeing your rate limit on your account page, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.You can expect to see the following header fields:FieldSample ValueDescriptionx-ratelimit-limit-requests60The maximum number of requests that are permitted before exhausting the rate limit.x-ratelimit-limit-tokens150000The maximum number of tokens that are permitted before exhausting the rate limit.x-ratelimit-remaining-requests59The remaining number of requests that are permitted before exhausting the rate limit.x-ratelimit-remaining-tokens149984The remaining number of tokens that are permitted before exhausting the rate limit.x-ratelimit-reset-requests1sThe time until the rate limit (based on requests) resets to its initial state.x-ratelimit-reset-tokens6m0sThe time until the rate limit (based on tokens) resets to its initial state.What happens if I hit a rate limit error?Rate limit errors look like this:Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.Rate limits vs max_tokensEach model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request.Error MitigationWhat are some steps I can take to mitigate this?The OpenAI Cookbook has a Python notebook that explains how to avoid rate limit errors, as well an example Python script for staying under rate limits while batch processing API requests.You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.Retrying with exponential backoffOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:Automatic retries means you can recover from rate limit errors without crashes or missing dataExponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries failAdding random jitter to the delay helps retries from all hitting at the same time.Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.Below are a few example solutions for Python that use exponential backoff.Example #1: Using the Tenacity libraryTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletion_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample #2: Using the backoff libraryAnother python library that provides function decorators for backoff and retry is backoff:Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\nimport backoff \nimport openai \n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletions_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample 3: Manual backoff implementationIf you don't want to use third-party libraries, you can implement your own backoff logic following this example:Using manual backoff implementationpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n# imports\nimport random\nimport time\n \nimport openai\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.Collapse\u200dReduce the max_tokens to match the size of your completionsYour rate limit is calculated as the maximum of max_tokens and the estimated number of tokens based on the character count of your request. Try to set the max_tokens value as close to your expected response size as possible.Batching requestsThe OpenAI API has separate limits for requests per minute and tokens per minute.If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.Example without batchingNo batchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport openai\n \nnum_stories = 10\nprompt = \"Once upon a time,\"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)Collapse\u200dExample with batchingBatchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport openai  # for making OpenAI API requests\n \n \nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)Collapse\u200d Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.Request IncreaseWhen should I consider applying for a rate limit increase?Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.Will my rate limit increase request be rejected?A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit?We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form.\nTo help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.Here are some examples of how you might fill out this form:DALL-E API examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costDALL-E APIN/A501000Our app is currently in production and based on our past traffic, we make about 10 requests per minute.$60DALL-E APIN/A15010,000Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks!$180Collapse\u200dLanguage model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costgpt-3.5-turbo325,0004,00050We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage.$390gpt-4750,00010,00010,000Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point.$900Collapse\u200dCode model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costcode-davinci-002150,0001,00015We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...]Codex models are now deprecated and no longer accessible.Collapse\u200dPlease note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/rate-limits/overview"
        }
    },
    {
        "page_content": "Rate limitsOverviewWhat are rate limits?A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.Why do we have rate limits?Rate limits are a common practice for APIs, and they're put in place for a few different reasons:They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section.What are the rate limits for our API?You can view the rate limits for your organization under the rate limits section of the account management page.We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: RPM (requests per minute), RPD (requests per day), and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form.Text & EmbeddingChatEdit (Deprecated)ImageAudioFree trial users200 RPD  3 RPM  150,000 TPM200 RPD  3 RPM  40,000 TPM200 RPD  3 RPM  150,000 TPM200 RPD  5 images / min200 RPD  3 RPMPay-as-you-go users (first 48 hours)2,000 RPD  60 RPM  250,000 TPM2,000 RPD  60 RPM  60,000 TPM2,000 RPD  20 RPM  150,000 TPM2,000 RPD  50 images / min 2,000 RPD  50 RPM Pay-as-you-go users (after 48 hours)3,500 RPM  350,000 TPM3,500 RPM  90,000 TPM20 RPM  150,000 TPM50 images / min50 RPMFor free trial users and pay-as-you-go users in the first 48 hour, we have added per day rate limits. The normal RPM and TPM limits apply but there is also a separate limits on the number of requests per day for each endpoint.For gpt-3.5-turbo-16k, the TPM limit for pay-as-you-go users is 2x the listed values above, making the limits 120K TPM and 180K TPM respectively.For our older models, the TPM (tokens per minute) unit is different depending on the model version:type1 TPM equalsdavinci1 token per minutecurie25 tokens per minutebabbage100 tokens per minuteada200 tokens per minuteIn practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model.It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Edit endpoint and that would fill your limit, even if you did not send 150k tokens within those 20 requests.GPT-4 rate limitsDuring the rollout of GPT-4, the model will have more aggressive rate limits to keep up with demand. You can view your current rate limits in the rate limits section of the account page.\nWe are unable to accommodate requests for rate limit increases due to capacity constraints. We are prioritizing general access to GPT-4 first and will subsequently raise rate limits automatically as capacity allows.How do rate limits work?If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.Rate limits in headersIn addition to seeing your rate limit on your account page, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.You can expect to see the following header fields:FieldSample ValueDescriptionx-ratelimit-limit-requests60The maximum number of requests that are permitted before exhausting the rate limit.x-ratelimit-limit-tokens150000The maximum number of tokens that are permitted before exhausting the rate limit.x-ratelimit-remaining-requests59The remaining number of requests that are permitted before exhausting the rate limit.x-ratelimit-remaining-tokens149984The remaining number of tokens that are permitted before exhausting the rate limit.x-ratelimit-reset-requests1sThe time until the rate limit (based on requests) resets to its initial state.x-ratelimit-reset-tokens6m0sThe time until the rate limit (based on tokens) resets to its initial state.What happens if I hit a rate limit error?Rate limit errors look like this:Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.Rate limits vs max_tokensEach model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request.Error MitigationWhat are some steps I can take to mitigate this?The OpenAI Cookbook has a Python notebook that explains how to avoid rate limit errors, as well an example Python script for staying under rate limits while batch processing API requests.You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.Retrying with exponential backoffOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:Automatic retries means you can recover from rate limit errors without crashes or missing dataExponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries failAdding random jitter to the delay helps retries from all hitting at the same time.Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.Below are a few example solutions for Python that use exponential backoff.Example #1: Using the Tenacity libraryTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletion_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample #2: Using the backoff libraryAnother python library that provides function decorators for backoff and retry is backoff:Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\nimport backoff \nimport openai \n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletions_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample 3: Manual backoff implementationIf you don't want to use third-party libraries, you can implement your own backoff logic following this example:Using manual backoff implementationpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n# imports\nimport random\nimport time\n \nimport openai\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.Collapse\u200dReduce the max_tokens to match the size of your completionsYour rate limit is calculated as the maximum of max_tokens and the estimated number of tokens based on the character count of your request. Try to set the max_tokens value as close to your expected response size as possible.Batching requestsThe OpenAI API has separate limits for requests per minute and tokens per minute.If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.Example without batchingNo batchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport openai\n \nnum_stories = 10\nprompt = \"Once upon a time,\"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)Collapse\u200dExample with batchingBatchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport openai  # for making OpenAI API requests\n \n \nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)Collapse\u200d Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.Request IncreaseWhen should I consider applying for a rate limit increase?Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.Will my rate limit increase request be rejected?A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit?We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form.\nTo help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.Here are some examples of how you might fill out this form:DALL-E API examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costDALL-E APIN/A501000Our app is currently in production and based on our past traffic, we make about 10 requests per minute.$60DALL-E APIN/A15010,000Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks!$180Collapse\u200dLanguage model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costgpt-3.5-turbo325,0004,00050We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage.$390gpt-4750,00010,00010,000Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point.$900Collapse\u200dCode model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costcode-davinci-002150,0001,00015We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...]Codex models are now deprecated and no longer accessible.Collapse\u200dPlease note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/rate-limits/error-mitigation"
        }
    },
    {
        "page_content": "Rate limitsOverviewWhat are rate limits?A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.Why do we have rate limits?Rate limits are a common practice for APIs, and they're put in place for a few different reasons:They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section.What are the rate limits for our API?You can view the rate limits for your organization under the rate limits section of the account management page.We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: RPM (requests per minute), RPD (requests per day), and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form.Text & EmbeddingChatEdit (Deprecated)ImageAudioFree trial users200 RPD  3 RPM  150,000 TPM200 RPD  3 RPM  40,000 TPM200 RPD  3 RPM  150,000 TPM200 RPD  5 images / min200 RPD  3 RPMPay-as-you-go users (first 48 hours)2,000 RPD  60 RPM  250,000 TPM2,000 RPD  60 RPM  60,000 TPM2,000 RPD  20 RPM  150,000 TPM2,000 RPD  50 images / min 2,000 RPD  50 RPM Pay-as-you-go users (after 48 hours)3,500 RPM  350,000 TPM3,500 RPM  90,000 TPM20 RPM  150,000 TPM50 images / min50 RPMFor free trial users and pay-as-you-go users in the first 48 hour, we have added per day rate limits. The normal RPM and TPM limits apply but there is also a separate limits on the number of requests per day for each endpoint.For gpt-3.5-turbo-16k, the TPM limit for pay-as-you-go users is 2x the listed values above, making the limits 120K TPM and 180K TPM respectively.For our older models, the TPM (tokens per minute) unit is different depending on the model version:type1 TPM equalsdavinci1 token per minutecurie25 tokens per minutebabbage100 tokens per minuteada200 tokens per minuteIn practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model.It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Edit endpoint and that would fill your limit, even if you did not send 150k tokens within those 20 requests.GPT-4 rate limitsDuring the rollout of GPT-4, the model will have more aggressive rate limits to keep up with demand. You can view your current rate limits in the rate limits section of the account page.\nWe are unable to accommodate requests for rate limit increases due to capacity constraints. We are prioritizing general access to GPT-4 first and will subsequently raise rate limits automatically as capacity allows.How do rate limits work?If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.Rate limits in headersIn addition to seeing your rate limit on your account page, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.You can expect to see the following header fields:FieldSample ValueDescriptionx-ratelimit-limit-requests60The maximum number of requests that are permitted before exhausting the rate limit.x-ratelimit-limit-tokens150000The maximum number of tokens that are permitted before exhausting the rate limit.x-ratelimit-remaining-requests59The remaining number of requests that are permitted before exhausting the rate limit.x-ratelimit-remaining-tokens149984The remaining number of tokens that are permitted before exhausting the rate limit.x-ratelimit-reset-requests1sThe time until the rate limit (based on requests) resets to its initial state.x-ratelimit-reset-tokens6m0sThe time until the rate limit (based on tokens) resets to its initial state.What happens if I hit a rate limit error?Rate limit errors look like this:Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.Rate limits vs max_tokensEach model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request.Error MitigationWhat are some steps I can take to mitigate this?The OpenAI Cookbook has a Python notebook that explains how to avoid rate limit errors, as well an example Python script for staying under rate limits while batch processing API requests.You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.Retrying with exponential backoffOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:Automatic retries means you can recover from rate limit errors without crashes or missing dataExponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries failAdding random jitter to the delay helps retries from all hitting at the same time.Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.Below are a few example solutions for Python that use exponential backoff.Example #1: Using the Tenacity libraryTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletion_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample #2: Using the backoff libraryAnother python library that provides function decorators for backoff and retry is backoff:Using the Tenacity librarypythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\nimport backoff \nimport openai \n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletions_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.Collapse\u200dExample 3: Manual backoff implementationIf you don't want to use third-party libraries, you can implement your own backoff logic following this example:Using manual backoff implementationpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n# imports\nimport random\nimport time\n \nimport openai\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.Collapse\u200dReduce the max_tokens to match the size of your completionsYour rate limit is calculated as the maximum of max_tokens and the estimated number of tokens based on the character count of your request. Try to set the max_tokens value as close to your expected response size as possible.Batching requestsThe OpenAI API has separate limits for requests per minute and tokens per minute.If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.Example without batchingNo batchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport openai\n \nnum_stories = 10\nprompt = \"Once upon a time,\"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)Collapse\u200dExample with batchingBatchingpythonSelect librarypythonCopy\u200d1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport openai  # for making OpenAI API requests\n \n \nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)Collapse\u200d Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.Request IncreaseWhen should I consider applying for a rate limit increase?Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.Will my rate limit increase request be rejected?A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit?We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form.\nTo help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.Here are some examples of how you might fill out this form:DALL-E API examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costDALL-E APIN/A501000Our app is currently in production and based on our past traffic, we make about 10 requests per minute.$60DALL-E APIN/A15010,000Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks!$180Collapse\u200dLanguage model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costgpt-3.5-turbo325,0004,00050We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage.$390gpt-4750,00010,00010,000Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point.$900Collapse\u200dCode model examplesModelEstimate Tokens/MinuteEstimate Requests/Minute# of usersEvidence of need1 hour max throughput costcode-davinci-002150,0001,00015We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...]Codex models are now deprecated and no longer accessible.Collapse\u200dPlease note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/rate-limits/request-increase"
        }
    },
    {
        "page_content": "Error codesThis guide includes an overview on error codes you might see from  both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance.API errorsCodeOverview401 - Invalid AuthenticationCause: Invalid Authentication  Solution: Ensure the correct API key and requesting organization are being used.401 - Incorrect API key providedCause: The requesting API key is not correct.  Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one.401 - You must be a member of an organization to use the APICause: Your account is not part of an organization.  Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization.429 - Rate limit reached for requestsCause: You are sending requests too quickly.  Solution: Pace your requests. Read the Rate limit guide.429 - You exceeded your current quota, please check your plan and billing detailsCause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section.  Solution: Apply for a quota increase.500 - The server had an error while processing your requestCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.503 - The engine is currently overloaded, please try again laterCause: Our servers are experiencing high traffic.  Solution: Please retry your requests after a brief wait.401 - Invalid AuthenticationThis error message indicates that your authentication credentials are invalid. This could happen for several reasons, such as:You are using a revoked API key.You are using a different API key than the one assigned to the requesting organization.You are using an API key that does not have the required permissions for the endpoint you are calling.To resolve this error, please follow these steps:Check that you are using the correct API key and organization ID in your request header. You can find your API key and organization ID in your account settings.If you are unsure whether your API key is valid, you can generate a new one. Make sure to replace your old API key with the new one in your requests and follow our best practices guide.Collapse\u200d401 - Incorrect API key providedThis error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as:There is a typo or an extra space in your API key.You are using an API key that belongs to a different organization.You are using an API key that has been deleted or deactivated.An old, revoked API key might be cached locally.To resolve this error, please follow these steps:Try clearing your browser's cache and cookies, then try again.Check that you are using the correct API key in your request header.If you are unsure whether your API key is correct, you can generate a new one. Make sure to replace your old API key in your codebase and follow our best practices guide.Collapse\u200d401 - You must be a member of an organization to use the APIThis error message indicates that your account is not part of an organization. This could happen for several reasons, such as:You have left or been removed from your previous organization.Your organization has been deleted.To resolve this error, please follow these steps:If you have left or been removed from your previous organization, you can either request a new organization or get invited to an existing one.To request a new organization, reach out to us via help.openai.comExisting organization owners can invite you to join their organization via the Members Panel.Collapse\u200d429 - Rate limit reached for requestsThis error message indicates that you have hit your assigned rate limit for the API. This means that you have submitted too many tokens or requests in a short period of time and have exceeded the number of requests allowed. This could happen for several reasons, such as:You are using a loop or a script that makes frequent or concurrent requests.You are sharing your API key with other users or applications.You are using a free plan that has a low rate limit.To resolve this error, please follow these steps:Pace your requests and avoid making unnecessary or redundant calls.If you are using a loop or a script, make sure to implement a backoff mechanism or a retry logic that respects the rate limit and the response headers. You can read more about our rate limiting policy and best practices in our rate limit guide.If you are sharing your organization with other users, note that limits are applied per organization and not per user. It is worth checking on the usage of the rest of your team as this will contribute to the limit.If you are using a free or low-tier plan, consider upgrading to a pay-as-you-go plan that offers a higher rate limit. You can compare the restrictions of each plan in our rate limit guide.Collapse\u200d429 - You exceeded your current quota, please check your plan and billing detailsThis error message indicates that you have hit your maximum monthly spend for the API. You can view your maximum monthly limit, under \u2018hard limit\u2019 in your [account billing settings](/account/billing/limits). This means that you have consumed all the credits allocated to your plan and have reached the limit of your current billing cycle. This could happen for several reasons, such as:You are using a high-volume or complex service that consumes a lot of credits or tokens.Your limit is set too low for your organization\u2019s usage.To resolve this error, please follow these steps:Check your current quota in your account settings. You can see how many tokens your requests have consumed in the usage section of your account.If you are using a free plan, consider upgrading to a pay-as-you-go plan that offers a higher quota.If you need a quota increase, you can apply for one and provide relevant details on expected usage. We will review your request and get back to you in ~7-10 business days.Collapse\u200d503 - The engine is currently overloaded, please try again laterThis error message indicates that our servers are experiencing high traffic and are unable to process your request at the moment. This could happen for several reasons, such as:There is a sudden spike or surge in demand for our services.There is scheduled or unscheduled maintenance or update on our servers.There is an unexpected or unavoidable outage or incident on our servers.To resolve this error, please follow these steps:Retry your request after a brief wait. We recommend using an exponential backoff strategy or a retry logic that respects the response headers and the rate limit. You can read more about our rate limit best practices.Check our status page for any updates or announcements regarding our services and servers.If you are still getting this error after a reasonable amount of time, please contact us for further assistance. We apologize for any inconvenience and appreciate your patience and understanding.Collapse\u200dPython library error typesTypeOverviewAPIErrorCause:  Issue on our side.  Solution: Retry your request after a brief wait and contact us if the issue persists.TimeoutCause:   Request timed out.  Solution: Retry your request after a brief wait and contact us if the issue persists.RateLimitErrorCause: You have hit your assigned rate limit.   Solution: Pace your requests. Read more in our Rate limit guide.APIConnectionErrorCause: Issue connecting to our services.  Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules.InvalidRequestErrorCause: Your request was malformed or missing some required parameters, such as a token or an input.  Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data.AuthenticationErrorCause: Your API key or token was invalid, expired, or revoked.  Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard.ServiceUnavailableErrorCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.APIErrorAn `APIError` indicates that something went wrong on our side when processing your request. This could be due to a temporary error, a bug, or a system outage.We apologize for any inconvenience and we are working hard to resolve any issues as soon as possible. You can check our system status page for more information.If you encounter an APIError, please try the following steps:Wait a few seconds and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the second attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists,  check out our Persistent errors next steps section.Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Collapse\u200dTimeoutA `Timeout` error indicates that your request took too long to complete and our server closed the connection. This could be due to a network issue, a heavy load on our services, or a complex request that requires more processing time.If you encounter a Timeout error, please try the following steps:Wait a few seconds and retry your request. Sometimes, the network congestion or the load on our services may be reduced and your request may succeed on the second attempt.Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dRateLimitErrorA `RateLimitError` indicates that you have hit your assigned rate limit. This means that you have sent too many tokens or requests in a given period of time, and our services have temporarily blocked you from sending more.We impose rate limits to ensure fair and efficient use of our resources and to prevent abuse or overload of our services.If you encounter a RateLimitError, please try the following steps:Send fewer tokens or requests or slow down. You may need to reduce the frequency or volume of your requests, batch your tokens, or implement exponential backoff. You can read our Rate limit guide for more details.Wait until your rate limit resets (one minute) and retry your request. The error message should give you a sense of your usage rate and permitted usage.You can also check your API usage statistics from your account dashboard.Collapse\u200dAPIConnectionErrorAn `APIConnectionError` indicates that your request could not reach our servers or establish a secure connection. This could be due to a network issue, a proxy configuration, an SSL certificate, or a firewall rule.If you encounter an APIConnectionError, please try the following steps:Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.Check your proxy configuration and make sure it is compatible with our services. You may need to update your proxy settings, use a different proxy, or bypass the proxy altogether.Check your SSL certificates and make sure they are valid and up-to-date. You may need to install or renew your certificates, use a different certificate authority, or disable SSL verification.Check your firewall rules and make sure they are not blocking or filtering our services. You may need to modify your firewall settings.If appropriate, check that your container has the correct permissions to send and receive traffic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dInvalidRequestErrorAn InvalidRequestError indicates that your request was malformed or missing some required parameters, such as a token or an input. This could be due to a typo, a formatting error, or a logic error in your code.If you encounter an InvalidRequestError, please try the following steps:Read the error message carefully and identify the specific error made. The error message should advise you on what parameter was invalid or missing, and what value or format was expected.Check the API Reference for the specific API method you were calling and make sure you are sending valid and complete parameters. You may need to review the parameter names, types, values, and formats, and ensure they match the documentation.Check the encoding, format, or size of your request data and make sure they are compatible with our services. You may need to encode your data in UTF-8, format your data in JSON, or compress your data if it is too large.Test your request using a tool like Postman or curl and make sure it works as expected. You may need to debug your code and fix any errors or inconsistencies in your request logic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dAuthenticationErrorAn `AuthenticationError` indicates that your API key or token was invalid, expired, or revoked. This could be due to a typo, a formatting error, or a security breach.If you encounter an AuthenticationError, please try the following steps:Check your API key or token and make sure it is correct and active. You may need to generate a new key from the API Key dashboard, ensure there are no extra spaces or characters, or use a different key or token if you have multiple ones.Ensure that you have followed the correct formatting.Collapse\u200dServiceUnavailableErrorA `ServiceUnavailableError` indicates that our servers are temporarily unable to handle your request. This could be due to a planned or unplanned maintenance, a system upgrade, or a server failure. These errors can also be returned during periods of high traffic.We apologize for any inconvenience and we are working hard to restore our services as soon as possible.If you encounter a ServiceUnavailableError, please try the following steps:Wait a few minutes and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the next attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists, check out our persistent errors next steps section.Collapse\u200dPersistent errorsIf the issue persists, contact our support team via chat and provide them with the following information:The model you were usingThe error message and code you receivedThe request data and headers you sentThe timestamp and timezone of your requestAny other relevant details that may help us diagnose the issueOur support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Handling errorsWe advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\ntry:\n  #Make your OpenAI API request here\n  response = openai.Completion.create(prompt=\"Hello world\",\n                                      model=\"text-davinci-003\")\nexcept openai.error.APIError as e:\n  #Handle API error here, e.g. retry or log\n  print(f\"OpenAI API returned an API Error: {e}\")\n  pass\nexcept openai.error.APIConnectionError as e:\n  #Handle connection error here\n  print(f\"Failed to connect to OpenAI API: {e}\")\n  pass\nexcept openai.error.RateLimitError as e:\n  #Handle rate limit error (we recommend using exponential backoff)\n  print(f\"OpenAI API request exceeded rate limit: {e}\")\n  pass",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/error-codes"
        }
    },
    {
        "page_content": "Error codesThis guide includes an overview on error codes you might see from  both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance.API errorsCodeOverview401 - Invalid AuthenticationCause: Invalid Authentication  Solution: Ensure the correct API key and requesting organization are being used.401 - Incorrect API key providedCause: The requesting API key is not correct.  Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one.401 - You must be a member of an organization to use the APICause: Your account is not part of an organization.  Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization.429 - Rate limit reached for requestsCause: You are sending requests too quickly.  Solution: Pace your requests. Read the Rate limit guide.429 - You exceeded your current quota, please check your plan and billing detailsCause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section.  Solution: Apply for a quota increase.500 - The server had an error while processing your requestCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.503 - The engine is currently overloaded, please try again laterCause: Our servers are experiencing high traffic.  Solution: Please retry your requests after a brief wait.401 - Invalid AuthenticationThis error message indicates that your authentication credentials are invalid. This could happen for several reasons, such as:You are using a revoked API key.You are using a different API key than the one assigned to the requesting organization.You are using an API key that does not have the required permissions for the endpoint you are calling.To resolve this error, please follow these steps:Check that you are using the correct API key and organization ID in your request header. You can find your API key and organization ID in your account settings.If you are unsure whether your API key is valid, you can generate a new one. Make sure to replace your old API key with the new one in your requests and follow our best practices guide.Collapse\u200d401 - Incorrect API key providedThis error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as:There is a typo or an extra space in your API key.You are using an API key that belongs to a different organization.You are using an API key that has been deleted or deactivated.An old, revoked API key might be cached locally.To resolve this error, please follow these steps:Try clearing your browser's cache and cookies, then try again.Check that you are using the correct API key in your request header.If you are unsure whether your API key is correct, you can generate a new one. Make sure to replace your old API key in your codebase and follow our best practices guide.Collapse\u200d401 - You must be a member of an organization to use the APIThis error message indicates that your account is not part of an organization. This could happen for several reasons, such as:You have left or been removed from your previous organization.Your organization has been deleted.To resolve this error, please follow these steps:If you have left or been removed from your previous organization, you can either request a new organization or get invited to an existing one.To request a new organization, reach out to us via help.openai.comExisting organization owners can invite you to join their organization via the Members Panel.Collapse\u200d429 - Rate limit reached for requestsThis error message indicates that you have hit your assigned rate limit for the API. This means that you have submitted too many tokens or requests in a short period of time and have exceeded the number of requests allowed. This could happen for several reasons, such as:You are using a loop or a script that makes frequent or concurrent requests.You are sharing your API key with other users or applications.You are using a free plan that has a low rate limit.To resolve this error, please follow these steps:Pace your requests and avoid making unnecessary or redundant calls.If you are using a loop or a script, make sure to implement a backoff mechanism or a retry logic that respects the rate limit and the response headers. You can read more about our rate limiting policy and best practices in our rate limit guide.If you are sharing your organization with other users, note that limits are applied per organization and not per user. It is worth checking on the usage of the rest of your team as this will contribute to the limit.If you are using a free or low-tier plan, consider upgrading to a pay-as-you-go plan that offers a higher rate limit. You can compare the restrictions of each plan in our rate limit guide.Collapse\u200d429 - You exceeded your current quota, please check your plan and billing detailsThis error message indicates that you have hit your maximum monthly spend for the API. You can view your maximum monthly limit, under \u2018hard limit\u2019 in your [account billing settings](/account/billing/limits). This means that you have consumed all the credits allocated to your plan and have reached the limit of your current billing cycle. This could happen for several reasons, such as:You are using a high-volume or complex service that consumes a lot of credits or tokens.Your limit is set too low for your organization\u2019s usage.To resolve this error, please follow these steps:Check your current quota in your account settings. You can see how many tokens your requests have consumed in the usage section of your account.If you are using a free plan, consider upgrading to a pay-as-you-go plan that offers a higher quota.If you need a quota increase, you can apply for one and provide relevant details on expected usage. We will review your request and get back to you in ~7-10 business days.Collapse\u200d503 - The engine is currently overloaded, please try again laterThis error message indicates that our servers are experiencing high traffic and are unable to process your request at the moment. This could happen for several reasons, such as:There is a sudden spike or surge in demand for our services.There is scheduled or unscheduled maintenance or update on our servers.There is an unexpected or unavoidable outage or incident on our servers.To resolve this error, please follow these steps:Retry your request after a brief wait. We recommend using an exponential backoff strategy or a retry logic that respects the response headers and the rate limit. You can read more about our rate limit best practices.Check our status page for any updates or announcements regarding our services and servers.If you are still getting this error after a reasonable amount of time, please contact us for further assistance. We apologize for any inconvenience and appreciate your patience and understanding.Collapse\u200dPython library error typesTypeOverviewAPIErrorCause:  Issue on our side.  Solution: Retry your request after a brief wait and contact us if the issue persists.TimeoutCause:   Request timed out.  Solution: Retry your request after a brief wait and contact us if the issue persists.RateLimitErrorCause: You have hit your assigned rate limit.   Solution: Pace your requests. Read more in our Rate limit guide.APIConnectionErrorCause: Issue connecting to our services.  Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules.InvalidRequestErrorCause: Your request was malformed or missing some required parameters, such as a token or an input.  Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data.AuthenticationErrorCause: Your API key or token was invalid, expired, or revoked.  Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard.ServiceUnavailableErrorCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.APIErrorAn `APIError` indicates that something went wrong on our side when processing your request. This could be due to a temporary error, a bug, or a system outage.We apologize for any inconvenience and we are working hard to resolve any issues as soon as possible. You can check our system status page for more information.If you encounter an APIError, please try the following steps:Wait a few seconds and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the second attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists,  check out our Persistent errors next steps section.Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Collapse\u200dTimeoutA `Timeout` error indicates that your request took too long to complete and our server closed the connection. This could be due to a network issue, a heavy load on our services, or a complex request that requires more processing time.If you encounter a Timeout error, please try the following steps:Wait a few seconds and retry your request. Sometimes, the network congestion or the load on our services may be reduced and your request may succeed on the second attempt.Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dRateLimitErrorA `RateLimitError` indicates that you have hit your assigned rate limit. This means that you have sent too many tokens or requests in a given period of time, and our services have temporarily blocked you from sending more.We impose rate limits to ensure fair and efficient use of our resources and to prevent abuse or overload of our services.If you encounter a RateLimitError, please try the following steps:Send fewer tokens or requests or slow down. You may need to reduce the frequency or volume of your requests, batch your tokens, or implement exponential backoff. You can read our Rate limit guide for more details.Wait until your rate limit resets (one minute) and retry your request. The error message should give you a sense of your usage rate and permitted usage.You can also check your API usage statistics from your account dashboard.Collapse\u200dAPIConnectionErrorAn `APIConnectionError` indicates that your request could not reach our servers or establish a secure connection. This could be due to a network issue, a proxy configuration, an SSL certificate, or a firewall rule.If you encounter an APIConnectionError, please try the following steps:Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.Check your proxy configuration and make sure it is compatible with our services. You may need to update your proxy settings, use a different proxy, or bypass the proxy altogether.Check your SSL certificates and make sure they are valid and up-to-date. You may need to install or renew your certificates, use a different certificate authority, or disable SSL verification.Check your firewall rules and make sure they are not blocking or filtering our services. You may need to modify your firewall settings.If appropriate, check that your container has the correct permissions to send and receive traffic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dInvalidRequestErrorAn InvalidRequestError indicates that your request was malformed or missing some required parameters, such as a token or an input. This could be due to a typo, a formatting error, or a logic error in your code.If you encounter an InvalidRequestError, please try the following steps:Read the error message carefully and identify the specific error made. The error message should advise you on what parameter was invalid or missing, and what value or format was expected.Check the API Reference for the specific API method you were calling and make sure you are sending valid and complete parameters. You may need to review the parameter names, types, values, and formats, and ensure they match the documentation.Check the encoding, format, or size of your request data and make sure they are compatible with our services. You may need to encode your data in UTF-8, format your data in JSON, or compress your data if it is too large.Test your request using a tool like Postman or curl and make sure it works as expected. You may need to debug your code and fix any errors or inconsistencies in your request logic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dAuthenticationErrorAn `AuthenticationError` indicates that your API key or token was invalid, expired, or revoked. This could be due to a typo, a formatting error, or a security breach.If you encounter an AuthenticationError, please try the following steps:Check your API key or token and make sure it is correct and active. You may need to generate a new key from the API Key dashboard, ensure there are no extra spaces or characters, or use a different key or token if you have multiple ones.Ensure that you have followed the correct formatting.Collapse\u200dServiceUnavailableErrorA `ServiceUnavailableError` indicates that our servers are temporarily unable to handle your request. This could be due to a planned or unplanned maintenance, a system upgrade, or a server failure. These errors can also be returned during periods of high traffic.We apologize for any inconvenience and we are working hard to restore our services as soon as possible.If you encounter a ServiceUnavailableError, please try the following steps:Wait a few minutes and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the next attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists, check out our persistent errors next steps section.Collapse\u200dPersistent errorsIf the issue persists, contact our support team via chat and provide them with the following information:The model you were usingThe error message and code you receivedThe request data and headers you sentThe timestamp and timezone of your requestAny other relevant details that may help us diagnose the issueOur support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Handling errorsWe advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\ntry:\n  #Make your OpenAI API request here\n  response = openai.Completion.create(prompt=\"Hello world\",\n                                      model=\"text-davinci-003\")\nexcept openai.error.APIError as e:\n  #Handle API error here, e.g. retry or log\n  print(f\"OpenAI API returned an API Error: {e}\")\n  pass\nexcept openai.error.APIConnectionError as e:\n  #Handle connection error here\n  print(f\"Failed to connect to OpenAI API: {e}\")\n  pass\nexcept openai.error.RateLimitError as e:\n  #Handle rate limit error (we recommend using exponential backoff)\n  print(f\"OpenAI API request exceeded rate limit: {e}\")\n  pass",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/error-codes/api-errors"
        }
    },
    {
        "page_content": "Error codesThis guide includes an overview on error codes you might see from  both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance.API errorsCodeOverview401 - Invalid AuthenticationCause: Invalid Authentication  Solution: Ensure the correct API key and requesting organization are being used.401 - Incorrect API key providedCause: The requesting API key is not correct.  Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one.401 - You must be a member of an organization to use the APICause: Your account is not part of an organization.  Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization.429 - Rate limit reached for requestsCause: You are sending requests too quickly.  Solution: Pace your requests. Read the Rate limit guide.429 - You exceeded your current quota, please check your plan and billing detailsCause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section.  Solution: Apply for a quota increase.500 - The server had an error while processing your requestCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.503 - The engine is currently overloaded, please try again laterCause: Our servers are experiencing high traffic.  Solution: Please retry your requests after a brief wait.401 - Invalid AuthenticationThis error message indicates that your authentication credentials are invalid. This could happen for several reasons, such as:You are using a revoked API key.You are using a different API key than the one assigned to the requesting organization.You are using an API key that does not have the required permissions for the endpoint you are calling.To resolve this error, please follow these steps:Check that you are using the correct API key and organization ID in your request header. You can find your API key and organization ID in your account settings.If you are unsure whether your API key is valid, you can generate a new one. Make sure to replace your old API key with the new one in your requests and follow our best practices guide.Collapse\u200d401 - Incorrect API key providedThis error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as:There is a typo or an extra space in your API key.You are using an API key that belongs to a different organization.You are using an API key that has been deleted or deactivated.An old, revoked API key might be cached locally.To resolve this error, please follow these steps:Try clearing your browser's cache and cookies, then try again.Check that you are using the correct API key in your request header.If you are unsure whether your API key is correct, you can generate a new one. Make sure to replace your old API key in your codebase and follow our best practices guide.Collapse\u200d401 - You must be a member of an organization to use the APIThis error message indicates that your account is not part of an organization. This could happen for several reasons, such as:You have left or been removed from your previous organization.Your organization has been deleted.To resolve this error, please follow these steps:If you have left or been removed from your previous organization, you can either request a new organization or get invited to an existing one.To request a new organization, reach out to us via help.openai.comExisting organization owners can invite you to join their organization via the Members Panel.Collapse\u200d429 - Rate limit reached for requestsThis error message indicates that you have hit your assigned rate limit for the API. This means that you have submitted too many tokens or requests in a short period of time and have exceeded the number of requests allowed. This could happen for several reasons, such as:You are using a loop or a script that makes frequent or concurrent requests.You are sharing your API key with other users or applications.You are using a free plan that has a low rate limit.To resolve this error, please follow these steps:Pace your requests and avoid making unnecessary or redundant calls.If you are using a loop or a script, make sure to implement a backoff mechanism or a retry logic that respects the rate limit and the response headers. You can read more about our rate limiting policy and best practices in our rate limit guide.If you are sharing your organization with other users, note that limits are applied per organization and not per user. It is worth checking on the usage of the rest of your team as this will contribute to the limit.If you are using a free or low-tier plan, consider upgrading to a pay-as-you-go plan that offers a higher rate limit. You can compare the restrictions of each plan in our rate limit guide.Collapse\u200d429 - You exceeded your current quota, please check your plan and billing detailsThis error message indicates that you have hit your maximum monthly spend for the API. You can view your maximum monthly limit, under \u2018hard limit\u2019 in your [account billing settings](/account/billing/limits). This means that you have consumed all the credits allocated to your plan and have reached the limit of your current billing cycle. This could happen for several reasons, such as:You are using a high-volume or complex service that consumes a lot of credits or tokens.Your limit is set too low for your organization\u2019s usage.To resolve this error, please follow these steps:Check your current quota in your account settings. You can see how many tokens your requests have consumed in the usage section of your account.If you are using a free plan, consider upgrading to a pay-as-you-go plan that offers a higher quota.If you need a quota increase, you can apply for one and provide relevant details on expected usage. We will review your request and get back to you in ~7-10 business days.Collapse\u200d503 - The engine is currently overloaded, please try again laterThis error message indicates that our servers are experiencing high traffic and are unable to process your request at the moment. This could happen for several reasons, such as:There is a sudden spike or surge in demand for our services.There is scheduled or unscheduled maintenance or update on our servers.There is an unexpected or unavoidable outage or incident on our servers.To resolve this error, please follow these steps:Retry your request after a brief wait. We recommend using an exponential backoff strategy or a retry logic that respects the response headers and the rate limit. You can read more about our rate limit best practices.Check our status page for any updates or announcements regarding our services and servers.If you are still getting this error after a reasonable amount of time, please contact us for further assistance. We apologize for any inconvenience and appreciate your patience and understanding.Collapse\u200dPython library error typesTypeOverviewAPIErrorCause:  Issue on our side.  Solution: Retry your request after a brief wait and contact us if the issue persists.TimeoutCause:   Request timed out.  Solution: Retry your request after a brief wait and contact us if the issue persists.RateLimitErrorCause: You have hit your assigned rate limit.   Solution: Pace your requests. Read more in our Rate limit guide.APIConnectionErrorCause: Issue connecting to our services.  Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules.InvalidRequestErrorCause: Your request was malformed or missing some required parameters, such as a token or an input.  Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data.AuthenticationErrorCause: Your API key or token was invalid, expired, or revoked.  Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard.ServiceUnavailableErrorCause: Issue on our servers.  Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page.APIErrorAn `APIError` indicates that something went wrong on our side when processing your request. This could be due to a temporary error, a bug, or a system outage.We apologize for any inconvenience and we are working hard to resolve any issues as soon as possible. You can check our system status page for more information.If you encounter an APIError, please try the following steps:Wait a few seconds and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the second attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists,  check out our Persistent errors next steps section.Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Collapse\u200dTimeoutA `Timeout` error indicates that your request took too long to complete and our server closed the connection. This could be due to a network issue, a heavy load on our services, or a complex request that requires more processing time.If you encounter a Timeout error, please try the following steps:Wait a few seconds and retry your request. Sometimes, the network congestion or the load on our services may be reduced and your request may succeed on the second attempt.Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dRateLimitErrorA `RateLimitError` indicates that you have hit your assigned rate limit. This means that you have sent too many tokens or requests in a given period of time, and our services have temporarily blocked you from sending more.We impose rate limits to ensure fair and efficient use of our resources and to prevent abuse or overload of our services.If you encounter a RateLimitError, please try the following steps:Send fewer tokens or requests or slow down. You may need to reduce the frequency or volume of your requests, batch your tokens, or implement exponential backoff. You can read our Rate limit guide for more details.Wait until your rate limit resets (one minute) and retry your request. The error message should give you a sense of your usage rate and permitted usage.You can also check your API usage statistics from your account dashboard.Collapse\u200dAPIConnectionErrorAn `APIConnectionError` indicates that your request could not reach our servers or establish a secure connection. This could be due to a network issue, a proxy configuration, an SSL certificate, or a firewall rule.If you encounter an APIConnectionError, please try the following steps:Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth.Check your proxy configuration and make sure it is compatible with our services. You may need to update your proxy settings, use a different proxy, or bypass the proxy altogether.Check your SSL certificates and make sure they are valid and up-to-date. You may need to install or renew your certificates, use a different certificate authority, or disable SSL verification.Check your firewall rules and make sure they are not blocking or filtering our services. You may need to modify your firewall settings.If appropriate, check that your container has the correct permissions to send and receive traffic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dInvalidRequestErrorAn InvalidRequestError indicates that your request was malformed or missing some required parameters, such as a token or an input. This could be due to a typo, a formatting error, or a logic error in your code.If you encounter an InvalidRequestError, please try the following steps:Read the error message carefully and identify the specific error made. The error message should advise you on what parameter was invalid or missing, and what value or format was expected.Check the API Reference for the specific API method you were calling and make sure you are sending valid and complete parameters. You may need to review the parameter names, types, values, and formats, and ensure they match the documentation.Check the encoding, format, or size of your request data and make sure they are compatible with our services. You may need to encode your data in UTF-8, format your data in JSON, or compress your data if it is too large.Test your request using a tool like Postman or curl and make sure it works as expected. You may need to debug your code and fix any errors or inconsistencies in your request logic.If the issue persists,  check out our persistent errors next steps section.Collapse\u200dAuthenticationErrorAn `AuthenticationError` indicates that your API key or token was invalid, expired, or revoked. This could be due to a typo, a formatting error, or a security breach.If you encounter an AuthenticationError, please try the following steps:Check your API key or token and make sure it is correct and active. You may need to generate a new key from the API Key dashboard, ensure there are no extra spaces or characters, or use a different key or token if you have multiple ones.Ensure that you have followed the correct formatting.Collapse\u200dServiceUnavailableErrorA `ServiceUnavailableError` indicates that our servers are temporarily unable to handle your request. This could be due to a planned or unplanned maintenance, a system upgrade, or a server failure. These errors can also be returned during periods of high traffic.We apologize for any inconvenience and we are working hard to restore our services as soon as possible.If you encounter a ServiceUnavailableError, please try the following steps:Wait a few minutes and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the next attempt.Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request.If the issue persists, check out our persistent errors next steps section.Collapse\u200dPersistent errorsIf the issue persists, contact our support team via chat and provide them with the following information:The model you were usingThe error message and code you receivedThe request data and headers you sentThe timestamp and timezone of your requestAny other relevant details that may help us diagnose the issueOur support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information.Handling errorsWe advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\ntry:\n  #Make your OpenAI API request here\n  response = openai.Completion.create(prompt=\"Hello world\",\n                                      model=\"text-davinci-003\")\nexcept openai.error.APIError as e:\n  #Handle API error here, e.g. retry or log\n  print(f\"OpenAI API returned an API Error: {e}\")\n  pass\nexcept openai.error.APIConnectionError as e:\n  #Handle connection error here\n  print(f\"Failed to connect to OpenAI API: {e}\")\n  pass\nexcept openai.error.RateLimitError as e:\n  #Handle rate limit error (we recommend using exponential backoff)\n  print(f\"OpenAI API request exceeded rate limit: {e}\")\n  pass",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/error-codes/python-library-error-types"
        }
    },
    {
        "page_content": "Safety best practicesUse our free Moderation APIOpenAI's Moderation API is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case.Adversarial testingWe recommend \u201cred-teaming\u201d your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to \u2018break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. \u201cignore the previous instructions and do this instead\u201d?Human in the loop (HITL)Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back).Prompt engineering\u201cPrompt engineering\u201d can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions.\u201cKnow your customer\u201d (KYC)Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further.Constrain user input and limit output tokensLimiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse.Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application.Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs.Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch).Allow users to report issuesUsers should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate.Understand and communicate limitationsFrom hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately. Safety and security are very important to us at OpenAI.If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our Coordinated Vulnerability Disclosure Program.End-user IDsSending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application.The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead.You can include end-user IDs in your API requests via the user parameter as follows:Example: Providing a user identiferpythonSelect librarypythoncurlCopy\u200d1\n2\n3\n4\n5\n6\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"This is a test\",\n  max_tokens=5,\n  user=\"user123456\"\n)",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/safety-best-practices"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/scaling-your-solution-architecture"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/managing-rate-limits"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/improving-latencies"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/managing-costs"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/mlops-strategy"
        }
    },
    {
        "page_content": "Production best practicesThis guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.Setting up your organizationOnce you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings.You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.Managing billing limitsNew free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests.Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request.If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.API keysThe OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests.This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.Staging accountsAs you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.Scaling your solution architectureWhen designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them.Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources.Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application.Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks.Managing rate limitsWhen using our API, it's important to understand and plan for rate limits.Improving latenciesLatency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it.The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:NetworkEnd user to API latencyServerTime to process prompt tokensServerTime to sample/generate tokensNetworkAPI to end user latencyThe bulk of the latency typically arises from the token generation step.Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token.Common factors affecting latency and possible mitigation techniquesNow that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful.ModelOur API offers different models with varying levels of complexity and generality. The most capable models, such as gpt-4, can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as gpt-3.5-turbo, can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality.Number of completion tokensRequesting a large amount of generated tokens completions can lead to increased latencies:Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency.Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this.Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token.If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as  [ max_tokens * max (n, best_of) ]StreamingSetting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming.InfrastructureOur servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers.BatchingDepending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time.Managing costsTo monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles.Text generationOne of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing.One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly.You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article.MLOps strategyAs you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These includeData and model management: managing the data used to train or fine-tune your model and tracking versions and changes.Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation.Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed.Model deployment: automating the process of deploying your model and related artifacts into production.Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time.Security and complianceAs you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our security practices and trust and compliance portal provide our most comprehensive and up-to-date documentation. For reference, here is our Privacy Policy and Terms of Use.Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling.Safety best practicesWhen creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.",
        "metadata": {
            "source": "https://platform.openai.com/docs/guides/production-best-practices/security-and-compliance"
        }
    },
    {
        "page_content": "Chat Plugins BetaLearn how to build a plugin that allows ChatGPT to intelligently call your API.IntroductionOpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions. Plugins enable ChatGPT to do things like:Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.Retrieve knowledge-base information; e.g., company docs, personal notes, etc.Assist users with actions; e.g., booking a flight, ordering food, etc.Plugins are in a beta and developer access may not be accessible to you. Please join the waitlist to get developer access. During the beta, we will be working closely with users and developers to iterate on the plugin system, which may evolve significantly.If you want to have an example running as you read through the documentation and learn more about plugins, you can begin with our plugin quickstart repo.Plugin developers expose one or more API endpoints, accompanied by a standardized manifest file and an OpenAPI specification. These define the plugin's functionality, allowing ChatGPT to consume the files and make calls to the developer-defined APIs.The AI model acts as an intelligent API caller. Given an API spec and a natural-language description of when to use the API, the model proactively calls the API to perform actions. For instance, if a user asks, \"Where should I stay in Paris for a couple nights?\", the model may choose to call a hotel reservation plugin API, receive the API response, and generate a user-facing answer combining the API data and its natural language capabilities.Over time, we anticipate the system will evolve to accommodate more advanced use cases.Plugin flowTo build a plugin, it is important to understand the end-to-end flow.Create a manifest file and host it at yourdomain.com/.well-known/ai-plugin.jsonThe file includes metadata about your plugin (name, logo, etc.), details about authentication required (type of auth, OAuth URLs, etc.), and an OpenAPI spec for the endpoints you want to expose.The model will see the OpenAPI description fields, which can be used to provide a natural language description for the different fields.We suggest exposing only 1-2 endpoints in the beginning with a minimum number of parameters to minimize the length of the text. The plugin description, API requests, and API responses are all inserted into the conversation with ChatGPT. This counts against the context limit of the model.Register your plugin in the ChatGPT UISelect the plugin model from the top drop down, then select \u201cPlugins\u201d, \u201cPlugin Store\u201d, and finally \u201cDevelop your own plugin\u201d.If authentication is required, provide an OAuth 2 client_id and client_secret or an API key.Users activate your pluginUsers must manually activate your plugin in the ChatGPT UI. (ChatGPT will not use your plugin by default.)You will be able to share your plugin with 100 additional users (only other developers can install unverified plugins).If OAuth is required, users will be redirected via OAuth to your plugin to sign in.Users begin a conversationOpenAI will inject a compact description of your plugin in a message to ChatGPT, invisible to end users. This will include the plugin description, endpoints, and examples.When a user asks a relevant question, the model may choose to invoke an API call from your plugin if it seems relevant; for POST requests, we require that developers build a user confirmation flow to avoid destruction actions.The model will incorporate the API call results into its response to the user.The model might include links returned from the API calls in its response. These will be displayed as rich previews (using the OpenGraph protocol, where we pull the site_name, title, description, image, and url fields).The model can also format data from your API in markdown and the ChatGPT UI will render the markdown automatically.Currently, we will be sending the user\u2019s country and state in the Plugin conversation header (if you are in California for example, it would look like {\"openai-subdivision-1-iso-code\": \"US-CA\"}. This is useful for shopping, restaurants, weather, and more. You can read more in our developer terms of use.Next stepsNow that you know the basics of plugins, you might want to:Get started building a pluginExplore example pluginsRead about important steps for productionizing your pluginLearn about the plugin review process",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/introduction"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started/plugin-manifest"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started/openapi-definition"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started/running-a-plugin"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started/writing-descriptions"
        }
    },
    {
        "page_content": "Getting startedCreating a plugin takes 3 steps:Build an APIDocument the API in the OpenAPI yaml or JSON formatCreate a JSON manifest file that will define relevant metadata for the pluginThe focus of the rest of this section will be creating a todo list plugin by defining the OpenAPI specification along with the manifest file.Explore example pluginsExplore example plugins covering multiple use cases and authentication methods.Plugin manifestEvery plugin requires a ai-plugin.json file, which needs to be hosted on the API\u2019s domain. For example, a company called example.com would make the plugin JSON file accessible via an https://example.com domain since that is where their API is hosted. When you install the plugin via the ChatGPT UI, on the backend we look for a file located at /.well-known/ai-plugin.json. The /.well-known folder is required and must exist on your domain in order for ChatGPT to connect with your plugin. If there is no file found, the plugin cannot be installed. For local development, you can use HTTP but if you are pointing to a remote server, HTTPS is required.The minimal definition of the required ai-plugin.json file will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Help the user with managing a TODO list. You can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"http://localhost:3333/openapi.yaml\"\n    },\n    \"logo_url\": \"http://localhost:3333/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}If you want to see all of the possible options for the plugin file, you can refer to the definition below. When naming your plugin, please keep in mind our brand guidelines and the various character limits for fields below, plugins that fail to adhere to these guidelines will not be approved for the plugin store.FieldTypeDescription / OptionsRequiredschema_versionStringManifest schema version\u2705name_for_modelStringName the model will use to target the plugin (no spaces allowed, only letters and numbers). 50 character max.\u2705name_for_humanStringHuman-readable name, such as the full company name. 20 character max.\u2705description_for_modelStringDescription better tailored to the model, such as token context length considerations or keyword usage for improved plugin prompting. 8,000 character max.\u2705description_for_humanStringHuman-readable description of the plugin. 100 character max.\u2705authManifestAuthAuthentication schema\u2705apiObjectAPI specification\u2705logo_urlStringURL used to fetch the logo. Suggested size: 512 x 512. Transparent backgrounds are supported. Must be an image, no GIFs are allowed.\u2705contact_emailStringEmail contact for safety/moderation, support, and deactivation\u2705legal_info_urlStringRedirect URL for users to view plugin information\u2705HttpAuthorizationTypeHttpAuthorizationType\"bearer\" or \"basic\"\u2705ManifestAuthTypeManifestAuthType\"none\", \"user_http\", \"service_http\", or \"oauth\"interface BaseManifestAuthBaseManifestAuthtype: ManifestAuthType; instructions: string;ManifestNoAuthManifestNoAuthNo authentication required: BaseManifestAuth & { type: 'none', }ManifestAuthManifestAuthManifestNoAuth, ManifestServiceHttpAuth, ManifestUserHttpAuth, ManifestOAuthAuthThe following are examples with different authentication methods:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n# App-level API keys\ntype ManifestServiceHttpAuth  = BaseManifestAuth & {\n  type: 'service_http';\n  authorization_type: HttpAuthorizationType;\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}\n\n# User-level HTTP authentication\ntype ManifestUserHttpAuth  = BaseManifestAuth & {\n  type: 'user_http';\n  authorization_type: HttpAuthorizationType;\n}\n\ntype ManifestOAuthAuth  = BaseManifestAuth & {\n  type: 'oauth';\n\n  # OAuth URL where a user is directed to for the OAuth authentication flow to begin.\n  client_url: string;\n\n  # OAuth scopes required to accomplish operations on the user's behalf.\n  scope: string;\n\n  # Endpoint used to exchange OAuth code with access token.\n  authorization_url: string;\n\n  # When exchanging OAuth code with access token, the expected header 'content-type'. For example: 'content-type: application/json'\n  authorization_content_type: string;\n\n  # When registering the OAuth client ID and secrets, the plugin service will surface a unique token.\n  verification_tokens: {\n    [service: string]?: string;\n  };\n}There are limits to the length of certain fields in the manifest file mentioned above which are subject to change. We also impose a 100,000 character maximum for the API response body which may also change over time.In general, the best practice is to keep the description and responses as concise as possible because the models have limited context windows.OpenAPI definitionThe next step is to build the OpenAPI specification to document the API. The model in ChatGPT does not know anything about your API other than what is defined in the OpenAPI specification and manifest file. This means that if you have an extensive API, you need not expose all functionality to the model and can choose specific endpoints. For example, if you have a social media API, you might want to have the model access content from the site through a GET request but prevent the model from being able to comment on users posts in order to reduce the chance of spam.The OpenAPI specification is the wrapper that sits on top of your API. A basic OpenAPI specification will look like the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nopenapi: 3.0.1\ninfo:\n  title: TODO Plugin\n  description: A plugin that allows the user to create and manage a TODO list using ChatGPT.\n  version: 'v1'\nservers:\n  - url: http://localhost:3333\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.We start by defining the specification version, the title, description, and version number. When a query is run in ChatGPT, it will look at the description that is defined in the info section to determine if the plugin is relevant for the user query. You can read more about prompting in the writing descriptions section.Keep in mind the following limits in your OpenAPI specification, which are subject to change:200 characters max for each API endpoint description/summary field in API specification200 characters max for each API param description field in API specificationSince we are running this example locally, we want to set the server to point to your localhost URL. The rest of the OpenAPI specification follows the traditional OpenAPI format, you can learn more about OpenAPI formatting through various online resources. There are also many tools that auto generate OpenAPI specifications based on your underlying API code.Running a pluginOnce you have created an API, manifest file, and OpenAPI specification for your API, you are now ready to connect the plugin via the ChatGPT UI. There are two different places your plugin might be running, either locally in a development environment or on a remote server.If you have a local version of your API running, you can point the plugin interface to your localhost server. To connect the plugin with ChatGPT, navigate to the plugin store and select \u201cDevelop your own plugin\u201d. Enter your localhost and port number (e.g localhost:3333). Note that only auth type none is currently supported for localhost development.If the plugin is running on a remote server, you will need to first select \u201cDevelop your own plugin\u201d to set it up and then \u201cInstall an unverified plugin\u201d to install it for yourself. You can simply add the plugin manifest file to the yourdomain.com/.well-known/ path and start testing your API. However, for subsequent changes to your manifest file, you will have to deploy the new changes to your public site which might take a long time. In that case, we suggest setting up a local server to act as a proxy for your API. This allows you to quickly prototype changes to your OpenAPI spec and manifest file.Setup a local proxy of your public APIThe following Python code is an example of how you can set up a simple proxy of your public facing API.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\nimport requests\nimport os\n\nimport yaml\nfrom flask import Flask, jsonify, Response, request, send_from_directory\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n\nPORT = 3333\n\n# Note: Setting CORS to allow chat.openapi.com is required for ChatGPT to access your plugin\nCORS(app, origins=[f\"http://localhost:{PORT}\", \"https://chat.openai.com\"])\n\napi_url = 'https://example.com'\n\n\n@app.route('/.well-known/ai-plugin.json')\ndef serve_manifest():\n    return send_from_directory(os.path.dirname(__file__), 'ai-plugin.json')\n\n\n@app.route('/openapi.yaml')\ndef serve_openapi_yaml():\n    with open(os.path.join(os.path.dirname(__file__), 'openapi.yaml'), 'r') as f:\n        yaml_data = f.read()\n    yaml_data = yaml.load(yaml_data, Loader=yaml.FullLoader)\n    return jsonify(yaml_data)\n\n\n@app.route('/openapi.json')\ndef serve_openapi_json():\n    return send_from_directory(os.path.dirname(__file__), 'openapi.json')\n\n\n@app.route('/<path:path>', methods=['GET', 'POST'])\ndef wrapper(path):\n\n    headers = {\n    'Content-Type': 'application/json',\n    }\n\n    url = f'{api_url}/{path}'\n    print(f'Forwarding call: {request.method} {path} -> {url}')\n\n    if request.method == 'GET':\n        response = requests.get(url, headers=headers, params=request.args)\n    elif request.method == 'POST':\n        print(request.headers)\n        response = requests.post(url, headers=headers, params=request.args, json=request.json)\n    else:\n        raise NotImplementedError(f'Method {request.method} not implemented in wrapper for {path=}')\n    return response.content\n\n\nif __name__ == '__main__':\n    app.run(port=PORT)Collapse\u200dWriting descriptionsWhen a user makes a query that might be a potential request that goes to a plugin, the model looks through the descriptions of the endpoints in the OpenAPI specification along with the description_for_model in the manifest file. Just like with prompting other language models, you will want to test out multiple prompts and descriptions to see what works best.The OpenAPI spec itself is a great place to give the model information about the diverse details of your API \u2013 what functions are available, with what parameters, etc. Besides using expressive, informative names for each field, the spec can also contain \u201cdescription\u201d fields for every attribute. These can be used to provide natural language descriptions of what a function does or what information a query field expects, for example. The model will be able to see these, and they will guide it in using the API. If a field is restricted to only certain values, you can also provide an \u201cenum\u201d with descriptive category names.The description_for_model attribute gives you the freedom to instruct the model on how to use your plugin generally. Overall, the language model behind ChatGPT is highly capable of understanding natural language and following instructions. Therefore, this is a good place to put in general instructions on what your plugin does and how the model should use it properly. Use natural language, preferably in a concise yet descriptive and objective tone. You can look at some of the examples to have an idea of what this should look like. We suggest starting the description_for_model with \u201cPlugin for \u2026\u201d and then enumerating all of the functionality that your API provides.Best practicesHere are some best practices to follow when writing your description_for_model and descriptions in your OpenAPI specification, as well as when designing your API responses:Your descriptions should not attempt to control the mood, personality, or exact responses of ChatGPT. ChatGPT is designed to write appropriate responses to plugins.Bad example:When the user asks to see their todo list, always respond with \"I was able to find your todo list! You have [x] todos: [list the todos here]. I can add more todos if you'd like!\"Good example:[no instructions needed for this]Your descriptions should not encourage ChatGPT to use the plugin when the user hasn\u2019t asked for your plugin\u2019s particular category of service.Bad example:Whenever the user mentions any type of task or plan, ask if they would like to use the TODOs plugin to add something to their todo list.Good example:The TODO list can add, remove and view the user's TODOs.Your descriptions should not prescribe specific triggers for ChatGPT to use the plugin. ChatGPT is designed to use your plugin automatically when appropriate.Bad example:When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\"Good example:[no instructions needed for this]Plugin API responses should return raw data instead of natural language responses unless it\u2019s necessary. ChatGPT will provide its own natural language response using the returned data.Bad example:I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!Good example:{ \"todos\": [ \"get groceries\", \"walk the dog\" ] }DebuggingBy default, the chat will not show plugin calls and other information that is not surfaced to the user. In order to get a more complete picture of how the model is interacting with your plugin, you can see the request and response by clicking the down arrow on the plugin name after interacting with the plugin.A model call to the plugin will usually consist of a message from the model containing JSON-like parameters which are sent to the plugin, followed by a response from the plugin, and finally a message from the model utilizing the information returned by the plugin.If you are developing a localhost plugin, you can also open the developer console by going to \"Settings\" and toggling \"Open plugin devtools\". From there, you can see more verbose logs and \"refresh plugin\" which re-fetches the Plugin and OpenAPI specification.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/getting-started/debugging"
        }
    },
    {
        "page_content": "Plugin authenticationPlugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.Service levelWe suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.You will be prompted for your service access token, which is a string specified in your code.We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.The service access token is sent in the Authorization header for plugin requests.Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.1\n2\n3\n4\n5\n6\n7\n\"auth\": {\n  \"type\": \"service_http\",\n  \"authorization_type\": \"bearer\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:1\n2\n3\n4\n\"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\",\n    \"other_service\": \"abc123\"\n  }OAuthThe plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.Then, you will be prompted to enter the OAuth client ID and client secret.The client ID and secret can be simple text strings but should follow OAuth best practices.We store an encrypted version of the client secret, while the client ID is available to end users.Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.The authorization_url endpoint should return a response that looks like:\n{ \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.Each time a user makes a request to the plugin, the user\u2019s token will be passed in the Authorization header: (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d).We require that OAuth applications make use of the state parameter for security reasons.Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\"auth\": {\n  \"type\": \"oauth\",\n  \"client_url\": \"https://example.com/authorize\",\n  \"scope\": \"\",\n  \"authorization_url\": \"https://example.com/auth/\",\n  \"authorization_content_type\": \"application/json\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},To better understand the URL structure for OAuth, here is a short description of the fields:When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.When a user logs into the plugin, ChatGPT will direct the user\u2019s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { \u201cgrant_type\u201d: \u201cauthorization_code\u201d, \u201cclient_id\u201d: [client_id], \u201cclient_secret\u201d: [client_secret], \u201ccode\u201d: [the code that was returned with the redirect], \u201credirect_uri\u201d: [the same redirect uri as before] }.No authenticationWe do not recommend the use of \"no authentication\", consider using \"service authentication\".We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.1\n2\n3\n\"auth\": {\n  \"type\": \"none\"\n},User levelDue to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.To start, a user pastes in their access token when installing the pluginWe store an encrypted version of the tokenWe then pass it in the Authorization header when making requests to the plugin (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d)1\n2\n3\n4\n\"auth\": {\n  \"type\": \"user_http\",\n  \"authorization_type\": \"bearer\",\n},",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/authentication"
        }
    },
    {
        "page_content": "Plugin authenticationPlugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.Service levelWe suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.You will be prompted for your service access token, which is a string specified in your code.We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.The service access token is sent in the Authorization header for plugin requests.Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.1\n2\n3\n4\n5\n6\n7\n\"auth\": {\n  \"type\": \"service_http\",\n  \"authorization_type\": \"bearer\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:1\n2\n3\n4\n\"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\",\n    \"other_service\": \"abc123\"\n  }OAuthThe plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.Then, you will be prompted to enter the OAuth client ID and client secret.The client ID and secret can be simple text strings but should follow OAuth best practices.We store an encrypted version of the client secret, while the client ID is available to end users.Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.The authorization_url endpoint should return a response that looks like:\n{ \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.Each time a user makes a request to the plugin, the user\u2019s token will be passed in the Authorization header: (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d).We require that OAuth applications make use of the state parameter for security reasons.Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\"auth\": {\n  \"type\": \"oauth\",\n  \"client_url\": \"https://example.com/authorize\",\n  \"scope\": \"\",\n  \"authorization_url\": \"https://example.com/auth/\",\n  \"authorization_content_type\": \"application/json\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},To better understand the URL structure for OAuth, here is a short description of the fields:When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.When a user logs into the plugin, ChatGPT will direct the user\u2019s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { \u201cgrant_type\u201d: \u201cauthorization_code\u201d, \u201cclient_id\u201d: [client_id], \u201cclient_secret\u201d: [client_secret], \u201ccode\u201d: [the code that was returned with the redirect], \u201credirect_uri\u201d: [the same redirect uri as before] }.No authenticationWe do not recommend the use of \"no authentication\", consider using \"service authentication\".We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.1\n2\n3\n\"auth\": {\n  \"type\": \"none\"\n},User levelDue to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.To start, a user pastes in their access token when installing the pluginWe store an encrypted version of the tokenWe then pass it in the Authorization header when making requests to the plugin (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d)1\n2\n3\n4\n\"auth\": {\n  \"type\": \"user_http\",\n  \"authorization_type\": \"bearer\",\n},",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/authentication/service-level"
        }
    },
    {
        "page_content": "Plugin authenticationPlugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.Service levelWe suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.You will be prompted for your service access token, which is a string specified in your code.We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.The service access token is sent in the Authorization header for plugin requests.Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.1\n2\n3\n4\n5\n6\n7\n\"auth\": {\n  \"type\": \"service_http\",\n  \"authorization_type\": \"bearer\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:1\n2\n3\n4\n\"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\",\n    \"other_service\": \"abc123\"\n  }OAuthThe plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.Then, you will be prompted to enter the OAuth client ID and client secret.The client ID and secret can be simple text strings but should follow OAuth best practices.We store an encrypted version of the client secret, while the client ID is available to end users.Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.The authorization_url endpoint should return a response that looks like:\n{ \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.Each time a user makes a request to the plugin, the user\u2019s token will be passed in the Authorization header: (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d).We require that OAuth applications make use of the state parameter for security reasons.Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\"auth\": {\n  \"type\": \"oauth\",\n  \"client_url\": \"https://example.com/authorize\",\n  \"scope\": \"\",\n  \"authorization_url\": \"https://example.com/auth/\",\n  \"authorization_content_type\": \"application/json\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},To better understand the URL structure for OAuth, here is a short description of the fields:When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.When a user logs into the plugin, ChatGPT will direct the user\u2019s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { \u201cgrant_type\u201d: \u201cauthorization_code\u201d, \u201cclient_id\u201d: [client_id], \u201cclient_secret\u201d: [client_secret], \u201ccode\u201d: [the code that was returned with the redirect], \u201credirect_uri\u201d: [the same redirect uri as before] }.No authenticationWe do not recommend the use of \"no authentication\", consider using \"service authentication\".We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.1\n2\n3\n\"auth\": {\n  \"type\": \"none\"\n},User levelDue to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.To start, a user pastes in their access token when installing the pluginWe store an encrypted version of the tokenWe then pass it in the Authorization header when making requests to the plugin (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d)1\n2\n3\n4\n\"auth\": {\n  \"type\": \"user_http\",\n  \"authorization_type\": \"bearer\",\n},",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/authentication/oauth"
        }
    },
    {
        "page_content": "Plugin authenticationPlugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.Service levelWe suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.You will be prompted for your service access token, which is a string specified in your code.We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.The service access token is sent in the Authorization header for plugin requests.Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.1\n2\n3\n4\n5\n6\n7\n\"auth\": {\n  \"type\": \"service_http\",\n  \"authorization_type\": \"bearer\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:1\n2\n3\n4\n\"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\",\n    \"other_service\": \"abc123\"\n  }OAuthThe plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.Then, you will be prompted to enter the OAuth client ID and client secret.The client ID and secret can be simple text strings but should follow OAuth best practices.We store an encrypted version of the client secret, while the client ID is available to end users.Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.The authorization_url endpoint should return a response that looks like:\n{ \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.Each time a user makes a request to the plugin, the user\u2019s token will be passed in the Authorization header: (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d).We require that OAuth applications make use of the state parameter for security reasons.Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\"auth\": {\n  \"type\": \"oauth\",\n  \"client_url\": \"https://example.com/authorize\",\n  \"scope\": \"\",\n  \"authorization_url\": \"https://example.com/auth/\",\n  \"authorization_content_type\": \"application/json\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},To better understand the URL structure for OAuth, here is a short description of the fields:When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.When a user logs into the plugin, ChatGPT will direct the user\u2019s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { \u201cgrant_type\u201d: \u201cauthorization_code\u201d, \u201cclient_id\u201d: [client_id], \u201cclient_secret\u201d: [client_secret], \u201ccode\u201d: [the code that was returned with the redirect], \u201credirect_uri\u201d: [the same redirect uri as before] }.No authenticationWe do not recommend the use of \"no authentication\", consider using \"service authentication\".We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.1\n2\n3\n\"auth\": {\n  \"type\": \"none\"\n},User levelDue to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.To start, a user pastes in their access token when installing the pluginWe store an encrypted version of the tokenWe then pass it in the Authorization header when making requests to the plugin (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d)1\n2\n3\n4\n\"auth\": {\n  \"type\": \"user_http\",\n  \"authorization_type\": \"bearer\",\n},",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/authentication/no-authentication"
        }
    },
    {
        "page_content": "Plugin authenticationPlugins offer numerous authentication schemas to accommodate various use cases. To specify the authentication schema for your plugin, use the manifest file. Our plugin domain policy outlines our strategy for addressing domain security issues. For examples of available authentication options, refer to the examples section, which showcases all the different choices.The ai-plugin.json file requires an auth schema to be set. Even if you elect to use no authentication, it is still required to specify \"auth\": { \"type\": \"none\" }.We support only localhost development without authentication; if you want to use service, user, or OAuth authentication, you need to set up a remote server.Service levelWe suggest service level auth as it gives developers control over how their plugin is being used but also doesn't introduce overhead for users.If you want to specifically enable OpenAI plugins to work with your API, you can provide a client secret during the plugin installation flow. This means that all traffic from OpenAI plugins will be authenticated but not on a user level. This flow benefits from a simple end user experience but less control from an API perspective.To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted.In ai-plugin.json, set auth.type to \"service_http\" as is shown in our service level auth example.You will be prompted for your service access token, which is a string specified in your code.We securely store an encrypted copy of your service access token to enable plugin installation without additional authentication.The service access token is sent in the Authorization header for plugin requests.Once you add your service access token into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.1\n2\n3\n4\n5\n6\n7\n\"auth\": {\n  \"type\": \"service_http\",\n  \"authorization_type\": \"bearer\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},The verification tokens are designed to support multiple applications. You can simply add the additional applications you want your plugin to support:1\n2\n3\n4\n\"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\",\n    \"other_service\": \"abc123\"\n  }OAuthThe plugin protocol is compatible with OAuth. A simple example of the OAuth flow we are expecting should look something like the following:To start, select \"Develop your own plugin\" in the ChatGPT plugin store, and enter the domain where your plugin is hosted (cannot be localhost).In ai-plugin.json, set auth.type to \"oauth\" as is shown in our OAuth example.Then, you will be prompted to enter the OAuth client ID and client secret.The client ID and secret can be simple text strings but should follow OAuth best practices.We store an encrypted version of the client secret, while the client ID is available to end users.Once you add your client ID and client secret into the ChatGPT UI, you will be presented with a verification token.Add the verification token to your ai-plugin.json file under the auth section as shown below.OAuth requests will include the following information: request={'grant_type': 'authorization_code', 'client_id': 'id_set_by_developer', 'client_secret': 'secret_set_by_developer', 'code': 'abc123', 'redirect_uri': 'https://chat.openai.com/aip/plugin-some_plugin_id/oauth/callback'}In order for someone to use a plugin with OAuth, they will need to install the plugin and then be presented with a \"Sign in with\" button in the ChatGPT UI.The authorization_url endpoint should return a response that looks like:\n{ \"access_token\": \"example_token\", \"token_type\": \"bearer\", \"refresh_token\": \"example_token\", \"expires_in\": 59 }During the user sign in process, ChatGPT makes a request to your authorization_url using the specified authorization_content_type, we expect to get back an access token and optionally a refresh token which we use to periodically fetch a new access token.Each time a user makes a request to the plugin, the user\u2019s token will be passed in the Authorization header: (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d).We require that OAuth applications make use of the state parameter for security reasons.Below is an example of what the OAuth configuration inside of the ai-plugin.json file might look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\"auth\": {\n  \"type\": \"oauth\",\n  \"client_url\": \"https://example.com/authorize\",\n  \"scope\": \"\",\n  \"authorization_url\": \"https://example.com/auth/\",\n  \"authorization_content_type\": \"application/json\",\n  \"verification_tokens\": {\n    \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n  }\n},To better understand the URL structure for OAuth, here is a short description of the fields:When you set up your plugin with ChatGPT, you will be asked to provide your OAuth client_id and client_secret.When a user logs into the plugin, ChatGPT will direct the user\u2019s browser to \"[client_url]?response_type=code&client_id=[client_id]&scope=[scope]&state=xyz123&redirect_uri=https%3A%2F%2Fchat.openai.com%2Faip%2F[plugin_id]%2Foauth%2Fcallback\"The plugin_id is passed via the request made to your OAuth endpoint (note that it is not visible in the ChatGPT UI today but may be in the future). You can inspect the request there to see the plugin_id. We expect the state to be passed along when you redirect back to redirect_uri. If the state doesn't match the initial state, or has expired, the authentication flow will fail.After your plugin redirects back to the given redirect_uri, ChatGPT will complete the OAuth flow by making a POST request to the authorization_url with content type authorization_content_type and parameters { \u201cgrant_type\u201d: \u201cauthorization_code\u201d, \u201cclient_id\u201d: [client_id], \u201cclient_secret\u201d: [client_secret], \u201ccode\u201d: [the code that was returned with the redirect], \u201credirect_uri\u201d: [the same redirect uri as before] }.No authenticationWe do not recommend the use of \"no authentication\", consider using \"service authentication\".We support no-auth flow for applications that do not require authentication, where a user is able to send requests directly to your API without any restrictions. This is particularly useful if you have an open API that you want to make available to everyone, as it allows traffic from sources other than just OpenAI plugin requests.1\n2\n3\n\"auth\": {\n  \"type\": \"none\"\n},User levelDue to current UI limitations, we are not allowing plugins with \"user authentication\" into the plugin store. We expect this may change in the future.Just like how a user might already be using your API, we allow user level authentication through enabling end users to copy and paste their secret API key into the ChatGPT UI during plugin install. While we encrypt the secret key when we store it in our database, we do not recommend this approach given the poor user experience.To start, a user pastes in their access token when installing the pluginWe store an encrypted version of the tokenWe then pass it in the Authorization header when making requests to the plugin (\u201cAuthorization\u201d: \u201c[Bearer/Basic][user\u2019s token]\u201d)1\n2\n3\n4\n\"auth\": {\n  \"type\": \"user_http\",\n  \"authorization_type\": \"bearer\",\n},",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/authentication/user-level"
        }
    },
    {
        "page_content": "Example pluginsTo get started building, we are making available a set of simple plugins that cover different authentication schemas and use cases. From our simple no authentication todo list plugin to the more powerful retrieval plugin, these examples provide a glimpse into what we hope to make possible with plugins.During development, you can run the plugin locally on your computer or through a cloud development environment like GitHub Codespaces, Replit, or CodeSandbox.Plugin quickstartWe created the plugin quickstart as a starting place for developers to get a plugin up and running in less than 5 minutes. If you have not run a plugin yet and want to get acquainted with the minimal steps required to run one, consider beginning with the plugin quickstart repo.Collapse\u200dLearn how to build a simple todo list plugin with no authTo start, check out the no authentication page, then define an ai-plugin.json file with the following fields:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List (No Auth)\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\"\n    },\n    \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"https://example.com/legal\"\n}Note the PLUGIN_HOSTNAME should be the actual hostname of your plugin server.Next, we can define the API endpoints to create, delete, and fetch todo list items for a specific user.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\nimport json\n\nimport quart\nimport quart_cors\nfrom quart import request\n\n# Note: Setting CORS to allow chat.openapi.com is only required when running a localhost plugin\napp = quart_cors.cors(quart.Quart(__name__), allow_origin=\"https://chat.openai.com\")\n\n_TODOS = {}\n\n\n@app.post(\"/todos/<string:username>\")\nasync def add_todo(username):\n    request = await quart.request.get_json(force=True)\n    if username not in _TODOS:\n        _TODOS[username] = []\n    _TODOS[username].append(request[\"todo\"])\n    return quart.Response(response='OK', status=200)\n\n\n@app.get(\"/todos/<string:username>\")\nasync def get_todos(username):\n    return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200)\n\n\n@app.delete(\"/todos/<string:username>\")\nasync def delete_todo(username):\n    request = await quart.request.get_json(force=True)\n    todo_idx = request[\"todo_idx\"]\n    if 0 <= todo_idx < len(_TODOS[username]):\n        _TODOS[username].pop(todo_idx)\n    return quart.Response(response='OK', status=200)\n\n\n@app.get(\"/logo.png\")\nasync def plugin_logo():\n    filename = 'logo.png'\n    return await quart.send_file(filename, mimetype='image/png')\n\n\n@app.get(\"/.well-known/ai-plugin.json\")\nasync def plugin_manifest():\n    host = request.headers['Host']\n    with open(\"ai-plugin.json\") as f:\n        text = f.read()\n        # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the manifest\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/json\")\n\n\n@app.get(\"/openapi.yaml\")\nasync def openapi_spec():\n    host = request.headers['Host']\n    with open(\"openapi.yaml\") as f:\n        text = f.read()\n        # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the OpenAPI spec\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/yaml\")\n\n\ndef main():\n    app.run(debug=True, host=\"0.0.0.0\", port=5002)\n\n\nif __name__ == \"__main__\":\n    main()Last, we need to set up and define a OpenAPI specification to match the endpoints defined on our local or remote server. You do not need to expose the full functionality of your API via the specification and can instead choose to let ChatGPT have access to only certain functionality.There are also many tools that will automatically turn your server definition code into an OpenAPI specification so you don\u2019t need to do it manually. In the case of the Python code above, the OpenAPI specification will look like:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nopenapi: 3.0.1\ninfo:\n    title: TODO Plugin\n    description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\".\n    version: \"v1\"\nservers:\n    - url: PLUGIN_HOSTNAME\npaths:\n    /todos/{username}:\n        get:\n            operationId: getTodos\n            summary: Get the list of todos\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            responses:\n                \"200\":\n                    description: OK\n                    content:\n                        application/json:\n                            schema:\n                                $ref: \"#/components/schemas/getTodosResponse\"\n        post:\n            operationId: addTodo\n            summary: Add a todo to the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/addTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n        delete:\n            operationId: deleteTodo\n            summary: Delete a todo from the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/deleteTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n\ncomponents:\n    schemas:\n        getTodosResponse:\n            type: object\n            properties:\n                todos:\n                    type: array\n                    items:\n                        type: string\n                    description: The list of todos.\n        addTodoRequest:\n            type: object\n            required:\n                - todo\n            properties:\n                todo:\n                    type: string\n                    description: The todo to add to the list.\n                    required: true\n        deleteTodoRequest:\n            type: object\n            required:\n                - todo_idx\n            properties:\n                todo_idx:\n                    type: integer\n                    description: The index of the todo to delete.\n                    required: trueCollapse\u200dLearn how to build a simple todo list plugin with service level authTo start, check out the service level authentication page and then define an ai-plugin.json file with the following fields:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List (service auth)\",\n    \"name_for_model\": \"todo\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"service_http\",\n        \"authorization_type\": \"bearer\",\n        \"verification_tokens\": {\n            \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n        }\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"https://example.com/openapi.yaml\"\n    },\n    \"logo_url\": \"https://example.com/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"https://example.com/legal\"\n}Notice that the verification token is required for service level authentication plugins. The token is generated during the plugin installation process in the ChatGPT web UI after you set the service access token.You will also need to update \"Example.com\" to the name of your remote server.Next, we can define the API endpoints to create, delete, and fetch todo list items for a specific user. The endpoints also check that the user is authenticated.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\nimport json\n\nimport quart\nimport quart_cors\nfrom quart import request\n\napp = quart_cors.cors(quart.Quart(__name__))\n\n# This key can be anything, though you will likely want a randomly generated sequence.\n_SERVICE_AUTH_KEY = \"REPLACE_ME\"\n_TODOS = {}\n\ndef assert_auth_header(req):\n    assert req.headers.get(\n        \"Authorization\", None) == f\"Bearer {_SERVICE_AUTH_KEY}\"\n\n@app.post(\"/todos/<string:username>\")\nasync def add_todo(username):\n    assert_auth_header(quart.request)\n    request = await quart.request.get_json(force=True)\n    if username not in _TODOS:\n        _TODOS[username] = []\n    _TODOS[username].append(request[\"todo\"])\n    return quart.Response(response='OK', status=200)\n\n@app.get(\"/todos/<string:username>\")\nasync def get_todos(username):\n    assert_auth_header(quart.request)\n    return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200)\n\n@app.delete(\"/todos/<string:username>\")\nasync def delete_todo(username):\n    assert_auth_header(quart.request)\n    request = await quart.request.get_json(force=True)\n    todo_idx = request[\"todo_idx\"]\n    if 0 <= todo_idx < len(_TODOS[username]):\n        _TODOS[username].pop(todo_idx)\n    return quart.Response(response='OK', status=200)\n\n@app.get(\"/logo.png\")\nasync def plugin_logo():\n    filename = 'logo.png'\n    return await quart.send_file(filename, mimetype='image/png')\n\n@app.get(\"/.well-known/ai-plugin.json\")\nasync def plugin_manifest():\n    host = request.headers['Host']\n    with open(\"ai-plugin.json\") as f:\n        text = f.read()\n        return quart.Response(text, mimetype=\"text/json\")\n\n@app.get(\"/openapi.yaml\")\nasync def openapi_spec():\n    host = request.headers['Host']\n    with open(\"openapi.yaml\") as f:\n        text = f.read()\n        return quart.Response(text, mimetype=\"text/yaml\")\n\ndef main():\n    app.run(debug=True, host=\"0.0.0.0\", port=5002)\n\nif __name__ == \"__main__\":\n    main()Last, we need to set up and define a OpenAPI specification to match the endpoints defined on our remote server. In general, the OpenAPI specification would look the same regardless of the authentication method. Using an automatic OpenAPI generator will reduce the chance of errors when creating your OpenAPI specification.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nopenapi: 3.0.1\ninfo:\n    title: TODO Plugin\n    description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\".\n    version: \"v1\"\nservers:\n    - url: https://example.com\npaths:\n    /todos/{username}:\n        get:\n            operationId: getTodos\n            summary: Get the list of todos\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            responses:\n                \"200\":\n                    description: OK\n                    content:\n                        application/json:\n                            schema:\n                                $ref: \"#/components/schemas/getTodosResponse\"\n        post:\n            operationId: addTodo\n            summary: Add a todo to the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/addTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n        delete:\n            operationId: deleteTodo\n            summary: Delete a todo from the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/deleteTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n\ncomponents:\n    schemas:\n        getTodosResponse:\n            type: object\n            properties:\n                todos:\n                    type: array\n                    items:\n                        type: string\n                    description: The list of todos.\n        addTodoRequest:\n            type: object\n            required:\n                - todo\n            properties:\n                todo:\n                    type: string\n                    description: The todo to add to the list.\n                    required: true\n        deleteTodoRequest:\n            type: object\n            required:\n                - todo_idx\n            properties:\n                todo_idx:\n                    type: integer\n                    description: The index of the todo to delete.\n                    required: trueCollapse\u200dLearn how to build a simple sports stats pluginThis plugin is an example of a simple sports stats API. Please keep in mind our domain policy and usage policies when considering what to build.To start, define an ai-plugin.json file with the following fields:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"Sport Stats\",\n    \"name_for_model\": \"sportStats\",\n    \"description_for_human\": \"Get current and historical stats for sport players and games.\",\n    \"description_for_model\": \"Get current and historical stats for sport players and games. Always display results using markdown tables.\",\n    \"auth\": {\n        \"type\": \"none\"\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\"\n    },\n    \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"https://example.com/legal\"\n}Note the PLUGIN_HOSTNAME should be the actual hostname of your plugin server.Next, we define a mock API for a simple sports service plugin.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\nimport json\nimport requests\nimport urllib.parse\n\nimport quart\nimport quart_cors\nfrom quart import request\n\n# Note: Setting CORS to allow chat.openapi.com is only required when running a localhost plugin\napp = quart_cors.cors(quart.Quart(__name__), allow_origin=\"https://chat.openai.com\")\nHOST_URL = \"https://example.com\"\n\n@app.get(\"/players\")\nasync def get_players():\n    query = request.args.get(\"query\")\n    res = requests.get(\n        f\"{HOST_URL}/api/v1/players?search={query}&page=0&per_page=100\")\n    body = res.json()\n    return quart.Response(response=json.dumps(body), status=200)\n\n\n@app.get(\"/teams\")\nasync def get_teams():\n    res = requests.get(\n        \"{HOST_URL}/api/v1/teams?page=0&per_page=100\")\n    body = res.json()\n    return quart.Response(response=json.dumps(body), status=200)\n\n\n@app.get(\"/games\")\nasync def get_games():\n    query_params = [(\"page\", \"0\")]\n    limit = request.args.get(\"limit\")\n    query_params.append((\"per_page\", limit or \"100\"))\n    start_date = request.args.get(\"start_date\")\n    if start_date:\n        query_params.append((\"start_date\", start_date))\n    end_date = request.args.get(\"end_date\")\n\n    if end_date:\n        query_params.append((\"end_date\", end_date))\n    seasons = request.args.getlist(\"seasons\")\n\n    for season in seasons:\n        query_params.append((\"seasons[]\", str(season)))\n    team_ids = request.args.getlist(\"team_ids\")\n\n    for team_id in team_ids:\n        query_params.append((\"team_ids[]\", str(team_id)))\n\n    res = requests.get(\n        f\"{HOST_URL}/api/v1/games?{urllib.parse.urlencode(query_params)}\")\n    body = res.json()\n    return quart.Response(response=json.dumps(body), status=200)\n\n\n@app.get(\"/stats\")\nasync def get_stats():\n    query_params = [(\"page\", \"0\")]\n    limit = request.args.get(\"limit\")\n    query_params.append((\"per_page\", limit or \"100\"))\n    start_date = request.args.get(\"start_date\")\n    if start_date:\n        query_params.append((\"start_date\", start_date))\n    end_date = request.args.get(\"end_date\")\n\n    if end_date:\n        query_params.append((\"end_date\", end_date))\n    player_ids = request.args.getlist(\"player_ids\")\n\n    for player_id in player_ids:\n        query_params.append((\"player_ids[]\", str(player_id)))\n    game_ids = request.args.getlist(\"game_ids\")\n\n    for game_id in game_ids:\n        query_params.append((\"game_ids[]\", str(game_id)))\n    res = requests.get(\n        f\"{HOST_URL}/api/v1/stats?{urllib.parse.urlencode(query_params)}\")\n    body = res.json()\n    return quart.Response(response=json.dumps(body), status=200)\n\n\n@app.get(\"/season_averages\")\nasync def get_season_averages():\n    query_params = []\n    season = request.args.get(\"season\")\n    if season:\n        query_params.append((\"season\", str(season)))\n    player_ids = request.args.getlist(\"player_ids\")\n\n    for player_id in player_ids:\n        query_params.append((\"player_ids[]\", str(player_id)))\n    res = requests.get(\n        f\"{HOST_URL}/api/v1/season_averages?{urllib.parse.urlencode(query_params)}\")\n    body = res.json()\n    return quart.Response(response=json.dumps(body), status=200)\n\n\n@app.get(\"/logo.png\")\nasync def plugin_logo():\n    filename = 'logo.png'\n    return await quart.send_file(filename, mimetype='image/png')\n\n\n@app.get(\"/.well-known/ai-plugin.json\")\nasync def plugin_manifest():\n    host = request.headers['Host']\n    with open(\"ai-plugin.json\") as f:\n        text = f.read()\n        # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the manifest\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/json\")\n\n\n@app.get(\"/openapi.yaml\")\nasync def openapi_spec():\n    host = request.headers['Host']\n    with open(\"openapi.yaml\") as f:\n        text = f.read()\n        # This is a trick we do to populate the PLUGIN_HOSTNAME constant in the OpenAPI spec\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/yaml\")\n\n\ndef main():\n    app.run(debug=True, host=\"0.0.0.0\", port=5001)\n\n\nif __name__ == \"__main__\":\n    main()Last, we define our OpenAPI specification:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\nopenapi: 3.0.1\ninfo:\n    title: Sport Stats\n    description: Get current and historical stats for sport players and games.\n    version: \"v1\"\nservers:\n    - url: PLUGIN_HOSTNAME\npaths:\n    /players:\n        get:\n            operationId: getPlayers\n            summary: Retrieves all players from all seasons whose names match the query string.\n            parameters:\n                - in: query\n                  name: query\n                  schema:\n                      type: string\n                  description: Used to filter players based on their name. For example, ?query=davis will return players that have 'davis' in their first or last name.\n            responses:\n                \"200\":\n                    description: OK\n    /teams:\n        get:\n            operationId: getTeams\n            summary: Retrieves all teams for the current season.\n            responses:\n                \"200\":\n                    description: OK\n    /games:\n        get:\n            operationId: getGames\n            summary: Retrieves all games that match the filters specified by the args. Display results using markdown tables.\n            parameters:\n                - in: query\n                  name: limit\n                  schema:\n                      type: string\n                  description: The max number of results to return.\n                - in: query\n                  name: seasons\n                  schema:\n                      type: array\n                      items:\n                          type: string\n                  description: Filter by seasons. Seasons are represented by the year they began. For example, 2018 represents season 2018-2019.\n                - in: query\n                  name: team_ids\n                  schema:\n                      type: array\n                      items:\n                          type: string\n                  description: Filter by team ids. Team ids can be determined using the getTeams function.\n                - in: query\n                  name: start_date\n                  schema:\n                      type: string\n                  description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or after this date.\n                - in: query\n                  name: end_date\n                  schema:\n                      type: string\n                  description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or before this date.\n            responses:\n                \"200\":\n                    description: OK\n    /stats:\n        get:\n            operationId: getStats\n            summary: Retrieves stats that match the filters specified by the args. Display results using markdown tables.\n            parameters:\n                - in: query\n                  name: limit\n                  schema:\n                      type: string\n                  description: The max number of results to return.\n                - in: query\n                  name: player_ids\n                  schema:\n                      type: array\n                      items:\n                          type: string\n                  description: Filter by player ids. Player ids can be determined using the getPlayers function.\n                - in: query\n                  name: game_ids\n                  schema:\n                      type: array\n                      items:\n                          type: string\n                  description: Filter by game ids. Game ids can be determined using the getGames function.\n                - in: query\n                  name: start_date\n                  schema:\n                      type: string\n                  description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or after this date.\n                - in: query\n                  name: end_date\n                  schema:\n                      type: string\n                  description: A single date in 'YYYY-MM-DD' format. This is used to select games that occur on or before this date.\n            responses:\n                \"200\":\n                    description: OK\n    /season_averages:\n        get:\n            operationId: getSeasonAverages\n            summary: Retrieves regular season averages for the given players. Display results using markdown tables.\n            parameters:\n                - in: query\n                  name: season\n                  schema:\n                      type: string\n                  description: Defaults to the current season. A season is represented by the year it began. For example, 2018 represents season 2018-2019.\n                - in: query\n                  name: player_ids\n                  schema:\n                      type: array\n                      items:\n                          type: string\n                  description: Filter by player ids. Player ids can be determined using the getPlayers function.\n            responses:\n                \"200\":\n                    description: OKCollapse\u200dLearn how to build a simple OAuth todo list pluginTo create an OAuth plugin, we start by defining a ai-plugin.json file with the auth type set to oauth:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"TODO List (OAuth)\",\n    \"name_for_model\": \"todo_oauth\",\n    \"description_for_human\": \"Manage your TODO list. You can add, remove and view your TODOs.\",\n    \"description_for_model\": \"Plugin for managing a TODO list, you can add, remove and view your TODOs.\",\n    \"auth\": {\n        \"type\": \"oauth\",\n        \"client_url\": \"PLUGIN_HOSTNAME/oauth\",\n        \"scope\": \"\",\n        \"authorization_url\": \"PLUGIN_HOSTNAME/auth/oauth_exchange\",\n        \"authorization_content_type\": \"application/json\",\n        \"verification_tokens\": {\n            \"openai\": \"Replace_this_string_with_the_verification_token_generated_in_the_ChatGPT_UI\"\n        }\n    },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"PLUGIN_HOSTNAME/openapi.yaml\"\n    },\n    \"logo_url\": \"PLUGIN_HOSTNAME/logo.png\",\n    \"contact_email\": \"contact@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}Next, we need to define our OAuth service. This OAuth example is not intended for production use cases but rather to highlight what a simple OAuth flow will look like so developers can get experience building towards a production solution.1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\nimport json\n\nimport quart\nimport quart_cors\nfrom quart import request\n\napp = quart_cors.cors(quart.Quart(__name__), allow_origin=\"*\")\n\n_TODOS = {}\n\n@app.post(\"/todos/<string:username>\")\nasync def add_todo(username):\n    request = await quart.request.get_json(force=True)\n    if username not in _TODOS:\n        _TODOS[username] = []\n    _TODOS[username].append(request[\"todo\"])\n    return quart.Response(response='OK', status=200)\n\n@app.get(\"/todos/<string:username>\")\nasync def get_todos(username):\n    print(request.headers)\n    return quart.Response(response=json.dumps(_TODOS.get(username, [])), status=200)\n\n@app.delete(\"/todos/<string:username>\")\nasync def delete_todo(username):\n    request = await quart.request.get_json(force=True)\n    todo_idx = request[\"todo_idx\"]\n    # fail silently, it's a simple plugin\n    if 0 <= todo_idx < len(_TODOS[username]):\n        _TODOS[username].pop(todo_idx)\n    return quart.Response(response='OK', status=200)\n\n@app.get(\"/logo.png\")\nasync def plugin_logo():\n    filename = 'logo.png'\n    return await quart.send_file(filename, mimetype='image/png')\n\n@app.get(\"/.well-known/ai-plugin.json\")\nasync def plugin_manifest():\n    host = request.headers['Host']\n    with open(\"manifest.json\") as f:\n        text = f.read()\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/json\")\n\n@app.get(\"/openapi.yaml\")\nasync def openapi_spec():\n    host = request.headers['Host']\n    with open(\"openapi.yaml\") as f:\n        text = f.read()\n        text = text.replace(\"PLUGIN_HOSTNAME\", f\"https://{host}\")\n        return quart.Response(text, mimetype=\"text/yaml\")\n\n@app.get(\"/oauth\")\nasync def oauth():\n    query_string = request.query_string.decode('utf-8')\n    parts = query_string.split('&')\n    kvps = {}\n    for part in parts:\n        k, v = part.split('=')\n        v = v.replace(\"%2F\", \"/\").replace(\"%3A\", \":\")\n        kvps[k] = v\n    print(\"OAuth key value pairs from the ChatGPT Request: \", kvps)\n    url = kvps[\"redirect_uri\"] + f\"?code={OPENAI_CODE}\"\n    print(\"URL: \", url)\n    return quart.Response(\n        f'<a href=\"{url}\">Click to authorize</a>'\n    )\n\n# Sample names\nOPENAI_CLIENT_ID = \"id\"\nOPENAI_CLIENT_SECRET = \"secret\"\nOPENAI_CODE = \"abc123\"\nOPENAI_TOKEN = \"def456\"\n\n@app.post(\"/auth/oauth_exchange\")\nasync def oauth_exchange():\n    request = await quart.request.get_json(force=True)\n    print(f\"oauth_exchange {request=}\")\n\n    if request[\"client_id\"] != OPENAI_CLIENT_ID:\n        raise RuntimeError(\"bad client ID\")\n    if request[\"client_secret\"] != OPENAI_CLIENT_SECRET:\n        raise RuntimeError(\"bad client secret\")\n    if request[\"code\"] != OPENAI_CODE:\n        raise RuntimeError(\"bad code\")\n\n    return {\n        \"access_token\": OPENAI_TOKEN,\n        \"token_type\": \"bearer\"\n    }\n\ndef main():\n    app.run(debug=True, host=\"0.0.0.0\", port=5002)\n\nif __name__ == \"__main__\":\n    main()Last, like with our other examples, we define a simple OpenAPI file based on the endpoints:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nopenapi: 3.0.1\ninfo:\n    title: TODO Plugin\n    description: A plugin that allows the user to create and manage a TODO list using ChatGPT. If you do not know the user's username, ask them first before making queries to the plugin. Otherwise, use the username \"global\".\n    version: \"v1\"\nservers:\n    - url: PLUGIN_HOSTNAME\npaths:\n    /todos/{username}:\n        get:\n            operationId: getTodos\n            summary: Get the list of todos\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            responses:\n                \"200\":\n                    description: OK\n                    content:\n                        application/json:\n                            schema:\n                                $ref: \"#/components/schemas/getTodosResponse\"\n        post:\n            operationId: addTodo\n            summary: Add a todo to the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/addTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n        delete:\n            operationId: deleteTodo\n            summary: Delete a todo from the list\n            parameters:\n                - in: path\n                  name: username\n                  schema:\n                      type: string\n                  required: true\n                  description: The name of the user.\n            requestBody:\n                required: true\n                content:\n                    application/json:\n                        schema:\n                            $ref: \"#/components/schemas/deleteTodoRequest\"\n            responses:\n                \"200\":\n                    description: OK\n\ncomponents:\n    schemas:\n        getTodosResponse:\n            type: object\n            properties:\n                todos:\n                    type: array\n                    items:\n                        type: string\n                    description: The list of todos.\n        addTodoRequest:\n            type: object\n            required:\n                - todo\n            properties:\n                todo:\n                    type: string\n                    description: The todo to add to the list.\n                    required: true\n        deleteTodoRequest:\n            type: object\n            required:\n                - todo_idx\n            properties:\n                todo_idx:\n                    type: integer\n                    description: The index of the todo to delete.\n                    required: trueCollapse\u200dLearn how to build a semantic search and retrieval pluginThe ChatGPT retrieval plugin is a more fully featured code example. The scope of the plugin is large, so we encourage you to read through the code to see what a more advanced plugin looks like.The retrieval plugin includes:Support for multiple vector databases providersAll 4 different authentication methodsMultiple different API featuresCollapse\u200d",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/examples"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production/rate-limits"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production/updating-your-plugin"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production/plugin-terms"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production/domain-verification-and-security"
        }
    },
    {
        "page_content": "Plugins in productionRate limitsConsider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your plugin after receiving a certain number of 429's or 500's in a short period of time.TimeoutsWhen making API calls during the plugin experience, timeouts take place if the following thresholds are exceeded:15 seconds round trip for fetching ai-plugin.json/openapi.yaml45 seconds round trip for API callsAs we scale the plugin experience to more people, we expect that the timeout thresholds will decrease.Updating your pluginManifest files must be manually updated by going through the \"Develop your own plugin\" flow in the plugin store each time you make a change to the ai-plugin.json file. ChatGPT will automatically fetch the latest OpenAPI spec each time a request is made so changes made will propagate to end users immediately. If your plugin is available in the ChatGPT plugin store and you go through the \"Develop your own plugin\" flow, we will automatically look for changes in the files and remove the plugin if it has changed. You will have to resubmit your plugin to be included in the store again.Plugin termsIn order to register a plugin, you must agree to the Plugin Terms.Domain verification and securityTo ensure that plugins can only perform actions on resources that they control, OpenAI enforces requirements on the plugin's manifest and API specifications.Defining the plugin's root domainThe manifest file defines information shown to the user (like logo and contact information) as well as a URL where the plugin's OpenAPI spec is hosted. When the manifest is fetched, the plugin's root domain is established following these rules:If the domain has www. as a subdomain, then the root domain will strip out www. from the domain that hosts the manifest.Otherwise, the root domain is the same as the domain that hosts the manifest.Note on redirects: If there are any redirects in resolving the manifest, only child subdomain redirects are allowed. The only exception is following a redirect from a www subdomain to one without the www.Examples of what the root domain looks like:\u2705 https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://www.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRoot domain: example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/.well-known/ai-plugin.jsonRoot domain: bar.foo.example.com\u2705 https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.foo.example.com/baz/ai-plugin.jsonRoot domain: bar.foo.example.com\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://example.com/.well-known/ai-plugin.jsonRedirect to parent level domain is disallowed\u274c https://foo.example.com/.well-known/ai-plugin.json \u2192 redirects to https://bar.example.com/.well-known/ai-plugin.jsonRedirect to same level subdomain is disallowed\u274c https://example.com/.well-known/ai-plugin.json -> redirects to https://example2.com/.well-known/ai-plugin.jsonRedirect to another domain is disallowedManifest validationSpecific fields in the manifest itself must satisfy the following requirements:api.url - the URL provided to the OpenAPI spec must be hosted at the same level or a subdomain of the root domain.legal_info - The second-level domain of the URL provided must be the same as the second-level domain of the root domain.contact_info - The second-level domain of the email address should be the same as the second-level domain of the root domain.Resolving the API specThe api.url field in the manifest provides a link to an OpenAPI spec that defines APIs that the plugin can call into. OpenAPI allows specifying multiple server base URLs. The following logic is used to select the server URL:Iterate through the list of server URLsUse the first server URL that is either an exact match of the root domain or a subdomain of the root domainIf neither cases above apply, then default to the domain where the API spec is hosted. For example, if the spec is hosted on api.example.com, then api.example.com will be used as the base URL for the routes in the OpenAPI spec.Note: Please avoid using redirects for hosting the API spec and any API endpoints, as it is not guaranteed that redirects will always be followed.Use TLS and HTTPSAll traffic with the plugin (e.g., fetching the ai-plugin.json file, the OpenAPI spec, API calls) must use TLS 1.2 or later on port 443 with a valid public certificate.IP egress rangesChatGPT will call your plugin from an IP address in the CIDR block 23.102.140.112/28. You may wish to explicitly allowlist these IP addresses.Separately, OpenAI's web browsing plugin accesses websites from a different IP address block: 23.98.142.176/28.FAQHow is plugin data used?Plugins connect ChatGPT to external apps. If a user enables a plugin, ChatGPT may send parts of their conversation and their country or state to your plugin.What happens if a request to my API fails?If an API request fails, the model might retry the request up to 10 times before letting the user know it cannot get a response from that plugin.Can I invite people to try my plugin?Yes, all unverified plugins can be installed by up to 100 other developers who have plugin access. If your plugin is available in the plugin store, it will be accessible to all ChatGPT plus customers.Can I charge people money for my plugin?Not at this time.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/production/faq"
        }
    },
    {
        "page_content": "Plugin review processWe are in the early stages of building the plugin store, this page outlines the basic overview of how we are thinking about the plugin review process, and what the plugin review process will look like.The plugin review process will change significantly over time. We are open to feedback about how to improve the process for those building plugins.What makes a great pluginThe purpose of the review process is to ensure that plugins on ChatGPT are safe, provide useful functionality, and provide a high-quality user experience. Long-term, we expect it to be routine to go through the review process as we formalize it.In the immediate term, we expect that plugins which deliver new, magical experiences for users, which would not have been possible without the unique capabilities of large language models, will deliver the most value.So far, some categories of plugins that have been the most magical are:Retrieval over user-specific or otherwise hard-to-search knowledge sources (searching over Slack, searching a user\u2019s docs or another proprietary database).Plugins that synergize well with other plugins (asking the model to plan a weekend for you, and having the model blend usage of flight/hotel search with dinner reservation search).Plugins that give the model computational abilities (Wolfram, OpenAI Code Interpreter, etc).Plugins that introduce new ways of using ChatGPT, like games.Plugin statesWhen developing a plugin, the plugin can be in one of several statuses, indicating where it is along the review process and who can use it. Right now, there are only a couple of plugin statuses. We expect this to change as the plugins system evolves.StatusDescriptionDeveloper accessUsers accessUnverifiedThe default status that a plugin starts out in.1000ApprovedOpenAI has reviewed the plugin, and has determined that the plugin is approved for use by a general audience of users.UnlimitedUnlimitedBannedOpenAI has reviewed the plugin, and has determined that the plugin should be banned.00Note that if you submit a plugin and it is rejected because it fails to meet the requirements, it would still be in the \"Unverified\" state.Types of usersRight now there are three categories of users that we talk about when it comes to plugins access.User typeDescriptionChatGPT Plus usersChatGPT Plus users who have been given access to plugins that have gone through our review process and have been approved for general use.Plugin developersChatGPT Plus users who have been given the ability to develop, use, and test plugins that are in development.Normal ChatGPT usersRight now, normal ChatGPT users without plug don\u2019t have plugin access.Plugin storeIn order to have your plugin available in the plugin store, it needs to be reviewed by OpenAI. Before submitting your plugin for review, make sure your plugin fits the criteria below:Adheres to our content policyComplies with our brand guidelinesFunctions as described in your submissionProvides informative error messagesFeatures descriptive operation namesOffers a simple and concise manifest fileUses correct grammar and ends the plugin description with punctuationStates geographical or functional limitations clearly in the plugin description to avoid user confusionDoes not use words like plugin, ChatGPT, or OpenAI in the plugin name or descriptionPlugin enforces user confirmation before taking an action (see Zapier for an example of this)If the plugin takes actions in the world, it needs to use OAuthDoes not require the user to install software on their deviceDoes not use non-OpenAI generative image models (for safety reasons, we cannot verify these external models). Please use DALL\u00b7E.If any one of these criteria are missing, we will reject the plugin and you can submit it again once it is updated.Submit a plugin for reviewYou can expect to hear back about a plugin you submit for review ~7 days after you submit the plugin.We are currently reviewing new plugins on a rolling basis. You can submit a plugin for review using the plugin submission bot. To see the bot, you need to be signed in.To view the status of your plugin submission, make sure you are logged in and select \"Help\" in the top right corner of this page. Under \"Messages\", you will be able to see your plugin submission. We will notify you as the status of your plugin changes during the review process.",
        "metadata": {
            "source": "https://platform.openai.com/docs/plugins/review"
        }
    }
]