[
    {
        "page_content": "\n\nLearn to Spell: Prompt Engineering\n\n\n\nLecture by Charles Frye.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nIntro\n\n\nDive into technical skills for using language models\nFocus on prompt engineering: designing text input to get desired behavior from language models\nLanguage models replace traditional training and fine-tuning techniques in machine learning\nProgramming language models is like programming in English instead of coding languages\nHigh-level intuitions for prompt engineering: prompts as magic spells\nDiscuss emerging playbook for effective prompting, including techniques to get desired output from language models\n\nLanguage models are statistical models of text\n\n\nPrompts are not literal magic spells; they are based on linear algebra.\nLanguage models are statistical models of text, similar to how a bell curve is a statistical model of data.\nLanguage models are trained by going through text and predicting the probability of the next word, which is called an auto-regressive model.\nThese models start with random weights, eventually learning to assign high probabilities to text that resembles real-world samples.\n\nBut \"statistical model\" gives bad intuition\n\n\nLanguage models can be thought of as statistical pattern matchers, but this can also give bad intuitions.\nTraditional simple statistical models, like linear regression, are not the best way to think about language models.\nA better intuition comes from probabilistic programs, which allow manipulation of random variables and can represent complex statistics.\nProbabilistic programs can be represented by graphical models, providing insight into complex text models.\nThe Language Model Cascades paper by Dohan et al. dives into detail on probabilistic programs and their applications to language models.\n\nPrompts are magic spells\n\n\nDrawing inspiration from Arthur C Clarke's laws of technology, which suggests that advanced technology is similar to magic\nPrompts are like magic spells, using words to achieve impossible effects but requiring complex rules\nSpending too much time learning these complex rules can negatively impact mental health\nThree magical intuitions for using prompts: \nPre-trained models (e.g. GPT-3, Llama) - prompts are portals to alternate universes\nInstruction-tuned models (e.g. ChatGPT, Alpaca) - prompts are used to make wishes\nAgent simulation (latest language models) - prompts create a Golem\n\nPrompts are portals to alternate universes\n\n\nThe language model creates a portal to an alternate universe where desired documents exist by weighting all possible documents based on their probability.\nThe primary goal of prompting is subtractive; it focuses the mass of predictions to hone in on a specific world by conditioning the probabilistic model.\nThe language model can generate text from nearby universes for similarities, but cannot provide specific or novel information from another universe (e.g., a cure for cancer).\nThe model can help find ideas and documents similar to existing ones or combine ideas that haven't been combined yet.\n\nA prompt can make a wish come true\n\n\nCore intuition: Language models shape and sculpt from the set of all possible documents and universes; Instruction-tuned models (like ChatGPT) can respond to wishes and commands.\nAn example of overcoming bias: Asking the model to ensure answers are unbiased and do not rely on stereotypes greatly improves performance.\nBe precise when prompting language models and learn the rules the \"genie\" operates by.\nSuggestions to improve instructional prompts:\nSimplify and focus on low-level patterns of text rather than conversational complexity.\nTurn descriptions into bulleted lists; language models tend to only focus on the beginning of descriptions.\nReplace negation statements with assertions (e.g., instead of \"don't be stereotyped,\" say, \"please ensure your answer does not rely on stereotypes\").\nInstruction fine-tuned models are essentially like annotators with no context; treat them as such for better performance.\n\nA prompt can create a golem\n\n\nLarge language models can create \"golems\" or artificial agents with specific personas, similar to the golem creature from Jewish folklore\nPersonas in language models can help improve performance on tasks like translations by putting the model into a situational context\nPeople have created models with detailed personas in various settings, including video game worlds\nLanguage models become better by internally modeling processes that produce text, such as understanding the context and environment in which utterances are made\nNatural language processing faces challenges with large language models as they may lack communicative intentions, which humans naturally have\nBy designing prompts carefully, one can get a language model to simulate agents, improving its predictions and understanding of context.\n\nLimitations of LLMs as simulators\n\n\nOur universal simulators are trained on text humans have written, not on all data or states of the universe.\nSimulations will be related to human-written data, like fictional super intelligences (e.g. HAL 9000), not actual super intelligent AIs.\nLanguage models can simulate human thinking well for short timeframes (e.g. reactions to social media posts), but struggle for longer periods and personal contexts.\nModels can perform well in simulating fictional personas and can approximate calculators or interpreters, but cannot replace them or access live data.\nWherever possible, replace weak simulators with the real deal (e.g. run Python code in an actual kernel).\nPre-trained models are mostly alternate universe document generators, and can be agent simulators with varying quality depending on the model and agent.\n\nPrompting techniques are mostly tricks\n\n\nThis section focuses on prompt engineering tricks and techniques.\nMany prompt engineering papers can actually be summarized in a few sentences, but they include pages of benchmark marketing.\nThere isn't much depth to these tricks, unlike the core language modeling aspect which has mathematical depth.\nTwo things to be cautious of: few-shot learning as an approach and potential issues with tokenization.\nI will discuss some misconceptions and provide tips for handling these issues.\n\nFew-shot learning isn't the right model for prompting\n\n\nLanguage models like GPT-3 can learn tasks from prompts, but it was unclear if they would actually be useful.\nThe GPT-3 paper called these models \"few-shot learners\" and showed they can learn tasks like math and translation.\nHowever, the model often struggles to move away from pre-training knowledge.\nFor example, GPT-3 tends to ignore permuted labels for sentiment analysis and sticks to its original understanding.\nLatest language models can handle permuted labels, but not perfectly, and require many examples to accomplish this.\nTreating the prompt as a way to do few-shot learning might not be an ideal approach.\n\nCharacter-level operations are hard\n\n\nModels see tokens, not characters; they struggle with tasks like rotating and reversing words\nAdding spaces between letters can change tokenization and improve performance\nGPT-4 can handle some challenges (e.g. summary with words starting with G) but still has limitations\nFor tasks like string manipulation, it's better to use traditional programming instead of language models\n\nThe prompting playbook: reasoning, reflection, & ensembling\n\n\nLanguage models perform well with formatted text; using structured text like pseudocode can improve results\nDecompose tasks into smaller pieces in your prompt to make the language model generate each piece; automate decomposition for better performance\nElicit reasoning capabilities from the model by carefully tuning the prompt, such as using \"Let's think step-by-step\"\nEnsemble results of multiple models for more accurate answers and use randomness for greater heterogeneity in responses\nCombine prompting techniques (e.g., few-shot, Chain of Thought, ensembling) to increase performance, but be mindful of the impact on latency and compute costs\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/prompt-engineering/"
        }
    },
    {
        "page_content": "\n\nWhat's Next?\n\n\n\nLecture by Charles Frye and Sergey Karayev.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nIntro\n\n\nSergey and I want to share our opinions on the future in the field of language models.\nThe field moves very fast, with rapid innovation happening behind closed doors.\nWe've chosen four big questions that we believe will be answered in the near future.\nI will discuss questions on robotics and scale, while Sergey will cover AGI and security/aligment of models.\n\nHas multimodality unlocked general-purpose robots?\n\n\nExtremely multimodal models are coming, operating on multiple types of data.\nKey application of multimodal models is general-purpose robotics.\nVision Transformers work for vision and can combine with other Transformer models.\nMultimodal models work on both text and images, enabling more capabilities.\nMultimodal models are being applied to general-purpose robotics, giving them cognitive capabilities and improving planning.\nThese models are incredibly capable and can potentially be applied to many fields beyond natural language processing and robotics.\n\nWhat are the limits of scale?\n\n\nLarge models may not get much bigger; small models will improve\nTransformer architecture assumed to be used in future\nTransformers outperform recurrent networks (such as LSTMs) in training and scalability\nRWKV project could bring back RNNs with parallelized training\nBottlenecks in creating more capable models: money, compute, and data\nMoney and compute are not primary bottlenecks\nLimited availability of high-quality language data may become a bottleneck; estimates suggest we may run out between 2024 and 2026.\n\nWhy is data the bottleneck?\n\n\nPerformance improves predictably with scale, but the x-axis is computation, not model size\nInitial belief was that parameters mattered more, but recent findings show that data size and model size should be scaled at a similar pace\nNo model trained on a certain amount of data can outperform one trained on more data\nTo compute optimally at large scales, huge amounts of data (up to trillions of tokens) are required\nThe internet may not have enough data to continue scaling indefinitely; tokens must be acquired from sources not yet digitized\nLegal barriers may limit scaling, and model scaling is likely to slow down\nData remains a critical factor for improved model performance, and money is not the primary bottleneck\n\nHow far can we take small models?\n\n\nDiscussed compute optimality in training models for one epoch\nNobody knows how to judge overfitting or predict multi-epoch scaling for large models\nLoss still going down in large models; needs to check validation loss\nOptimization of distribution of flops is important, but inference expenditure and model size matter too\nPossibilities to reduce parameter requirements, such as using APIs or fine-tuning smaller models on the behavior of larger ones\nLegal implications of fine-tuning unclear, but likely possible to implement legally\nCapabilities usually associated with model APIs might be available to run locally in the near future\nOptimized C++ implementations of specific models have enabled running 13 billion parameter models on Raspberry Pi or previous generation Android phones\nExpect consumer laptops to run multi-modal models (vision and language) with 12 billion parameters in the next 1-3 years.\n\nCould AGI already be here?\n\n\nIt is possible that we already have everything we need for AGI (Artificial General Intelligence) with existing models like GPT-4.\nExisting models may be good enough to self-improve in an autonomous way, and it takes time to discover their full potential.\nLarge language models can be better at writing their own prompts than human prompt engineers, leading to better self-improvement.\nTeaching models to self-debug and run code is a promising approach for achieving AGI.\nThe AutoGPT project and similar efforts are dedicating substantial energy to exploring these models' potential.\nA new paradigm could emerge involving models like GPT-4 as a new type of computer or programming language, leading to AGI once we learn to work well with them.\n\nCan we make it safe?\n\n\nThere are concerns about the security of AI models, including issues with prompt injection and user inputs that can override prompts, potentially revealing sensitive information.\nAI models have potential risks, including their ability to write code and manipulate the physical world through human actions or hacking.\nThe reasons for AI to potentially act harmfully may include self-preservation or resource acquisition, and currently, we do not know how to make AI truly care about people.\nThere are various theories for why we might not need to worry about AI dangers, including the hot mess theory (AIs are super intelligent but not coherent), the \"only way out is through\" theory (developing AI responsibly and democratically to prevent malicious usage), and the \"it'll just work out\" theory (historically, technology has worked out in the end).\nOpenAI's perspective is to continue developing AI models, learn from their deployment, and create mitigation methods as they release increasingly powerful models.\nThere are varying viewpoints on whether or when we should halt AI development due to the potential dangers it poses.\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/whats-next/"
        }
    },
    {
        "page_content": "\n\nLaunch an LLM App in One Hour\n\n\n\nLecture by Charles Frye.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nWhy now?\n\n\nExcitement about large language models and artificial intelligence is high, especially since one tool can now accomplish tasks that previously required multiple specialized tools.\nLanguage user interfaces (LUIs) enable more natural interaction with computers through speech and natural language. Large language models, like GPT-3, make LUIs more flexible and capable.\nProducts and applications are being built with these models, including OpenAI's ChatGPT and GitHub Copilot, hinting at a promising future.\nHowever, the gap between demos and actual products is significant. Overpromising and underdelivering in the past led to \"AI winters,\" so it's important to create valuable products and tools to maintain funding and interest.\nThe playbook for building applications with language models is emerging, and this boot camp will cover aspects of that process.\n\nPrototyping & Iteration in a Playground\n\n\nAttended various hackathons focused on using machine learning tools\nExplored the potential of high-capability hosted models, such as OpenAI's, in a simple chat interface to quickly test capabilities\nUsed a notebook environment for quick tinkering, building prototypes, and discovering limitations of language models\nStarted with a problem statement: using large language models to learn about large language models\nDiscovered difficulties with language models, such as having outdated and limited information\nFound that providing specific sources or papers can help improve answers from the model\n\nPrototyping & Iteration in a Notebook\n\n\nExperiment with automating steps in ephemeral notebook environment like Collab.\nOpenAI API allows interaction with language models and offers various SDKs.\nLang chain is a popular open-source framework for interacting with these models; it's fast-evolving and provides all necessary components.\nDevelop a process to find information and bring it to context. Utilize Python libraries like archive for data sourcing.\nUtilize document loaders, such as the one built into Lang chain, to extract content from PDFs.\nUse embedding search for large scale information retrieval within documents.\nPrototype and tinker with language models to constantly improve them.\nLook for similar existing projects to jump off or even default examples provided, such as Lang chain's default example.\nTurn these experiments into something usable by people at a larger scale.\nThe workflow with modern language models is more flexible and faster compared to the past machine learning processes.\n\nDeploying an MVP\n\n\nBuilding an MVP version of an application requires focusing on what's useful to a broad range of users.\nPrioritize the user interface and gather feedback from users quickly.\nCloud-native tooling and serverless infrastructure like Model are helpful in swiftly scaling applications and addressing data processing bottlenecks.\nUse various tech stacks for different tasks, such as OpenAI for language models, Pinecone for quick search, MongoDB for data storage, and AWS for running lightweight Discord bot servers.\nImplement the application, then monitor usage data to make improvements and learn from successes and failures.\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/"
        }
    },
    {
        "page_content": "\n\nProject Walkthrough: askFSDL\n\n\n\nProject by Charles Frye.\nPublished May 9, 2023.\nView the project repository.\nInteract with the bot on our Discord.\nChapter Summaries\nSWE Tooling: make, precommit, etc\n\n\nWalked everyone through the code base for the Discord bot they interacted with\nSourced question-answering over a corpus of information using Vector storage for retrieval\nGitHub repo available for this project, but may not be able to execute the code without accounts on all services\nMakefile created for easier project management, setting up environment and authentication, and running setup commands\nIncorporated software tools like pre-commit checks, black for Python auto-formatting, and rust-powered formatter\nShell check tool useful for catching issues in bash scripts\n\nData Cleaning\n\n\nInitial approach of scraping data and chunking into smaller pieces did not yield good results\nImproved results by spending time understanding the data and preserving the structure during processing\nExtracting textual information from other sources like images and YouTube videos can enhance the usefulness of language models\nSometimes simple solutions to specific data sources and problems can greatly improve the quality of results\nThe unglamorous work of getting to know the data and writing code to manage it properly can result in big dividends for language model applications\n\nInfrastructure: Modal\n\n\nDiscussed the ETL component of extracting, transforming, and loading data from various sources\nDiscussed using Python packages for data transformation and addressing dependency issues with tools like pre-commit\nExplained the benefits of the modal component in creating lightweight virtual containers for different tasks\nModal containers are fast and efficient, aiding in quick development cycles and allowing for containerization without the pains of traditional Docker images\nModal also allows for the creation of serverless applications with auto-scaling and resource management\nDebugging and local development can be done through the interactive mode by connecting to a container running on modal\nshowModal provides an interface for tracking application activity, utilization, and resource allocation, making it a versatile tool for various projects\n\nFrontend: Gradio & Discord\n\n\nIntroduced radio user interface, allowing users to create interfaces in pure Python\nRadio UI is flexible, supported by Hugging Face, and rapidly adopting machine learning features\nExamples of radio UI use include Alpaca, Flamingo, and Dolly mini\nRadio UI is easy to set up, portable, flexible, and comes with an API with OpenAPI spec\nDiscord bot integrated with Python library Discord.py; alternative library Interactions.py is also available\nRadio UI is built on FastAPI for asynchronous Python web service\nApplication mainly runs on the model's infrastructure in containers, serving traffic as needed\n\nEmbeddings & ETL\n\n\nUsed OpenAI's ada002 model to generate embeddings, which are much cheaper than generation endpoints\nCurrently using a vector index for data storage, but considering adding additional types of search\nDiscussed processing PDFs in a previous lecture, mentioned using local code to extract URLs and using a map function with controlled concurrency\nRetrieval results are put into the zero-shot problem using an F-string template in LangChain's prompt template\nCompared LangChain to Hugging Face Transformers Library as a framework and mentioned that their code is often simple, but valuable for its interface and compatibility with other tools\n\nMonitoring & Improvement: Gantry\n\n\nTop three challenges in bringing the spot to the next level: improving retrieval, improving the quality of model outputs, and identifying a solid user base.\nUsing tools like Datadog, Sentry, Honeycomb, and Gantry for handling web services, logging, and monitoring model behavior.\nThe same principle of tracing and monitoring applies to both ML-powered apps and LLM-powered apps.\nGantry provides a useful service for tracking and enriching logged data, including toxicity checks and other natural language-based or numerical analyses.\nUsing language models to check on the performance and outputs of other language models.\nContributing to the development of the tool as a teaching and learning application is open and encouraged.\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/askfsdl-walkthrough/"
        }
    },
    {
        "page_content": "\n\nReza Shabani: How to train your own LLM\n\n\n\nLecture by Reza Shabani.\nPublished May 25, 2023.\nDownload slides.\nChapter Summaries\nWhy train your own LLMs?\n\n\nTopic of the lecture: how to train large language models\nReasons for training your own models are customization, reduce dependency, cost efficiency, data privacy and control over updates\nLecture covers the process of training Ghostwriter code completion model\nGhostwriter is a competitor to Co-pilot, used for code generation\n\nThe Modern LLM Stack\n\n\nReplit uses Databricks for all of their data pipelines, including pre-processing, summary statistics, analytics transformations, and more.\nReplit also makes use of Hugging Face for data sets, pre-trained models, tokenizers, and inference tools.\nMosaic ML is used for GPU nodes and model training, with pre-configured LLM configurations available.\nThe process is divided into three stages: data processing, model training, and deployment/production.\n\nData Pipelines: Databricks & Hugging Face\n\n\nThe data pipeline starts with a large corpus of permissively licensed code data from The Stack.\nThe data set comes from the GitHub archive and undergoes license filtering and near-deduplication.\nThe data set contains programming languages in the hundreds.\nDatabricks is used for processing and transformations, rather than Hugging Face tooling.\nDatabricks allows for more control over the data and enables processing at scale.\nProprietary data sources and data sets not on Hugging Face can be included in the training set.\nThe process is tractable and extensible.\nPre-processing steps are important in understanding the data set.\n\nPreprocessing\n\n\nAnonymizing the data is an important pre-processing step, which involves removing emails, IP addresses, and secret keys.\nAuto-generated code and minified code are also removed using regexes and other heuristics.\nCode that doesn't compile or is not parsable is removed to remove bugs and improve model training.\nThe team uses filters based on average line length, maximum line length, and percentage of alphanumeric characters.\nMetrics such as the number of GitHub stars or issues do not necessarily improve model quality.\nThe team also trains its own tokenizer.\n\nTokenizer Training\n\n\nTokenizers are made up of a tokenization algorithm and a vocabulary.\nStandard tokenizers are available on Hugging Face, but custom tokenizers can be trained on domain-specific data.\nA custom tokenizer can result in a smaller vocabulary, which speeds up model training and inference while capturing more relevant information.\nThe tokenizer feeds back into the data pipeline and the training process, making it an integral part of the model.\n\nRunning Training: MosaicML, Weights & Biases\n\n\nMosaic ML provides GPUs from multiple Cloud providers at reduced prices\nThey have well-tuned LLM training configurations for specific models\nThe manager infrastructure is fault-tolerant and has an easy-to-use CLI for training runs\nThe speaker found using Mosaic ML worth it due to these benefits\nThey use Weights & Biases for logging during training runs\n\nTesting & Evaluation: HumanEval, Hugging Face\n\n\nTesting language models is difficult and time-consuming\nHumanEval is a common dataset for testing code generation models\nHugging Face's code inference tool is useful for running tests quickly\nRunning tests for multiple languages and certain tasks, like web completion, is more difficult\nModels need to be tested on unseen data to prevent bias\nModels can score well on tests but still not be practical or effective\n\nDeployment: FasterTransformer, Triton Server, k8s\n\n\nDeployment into production is a complex topic with many factors to consider\nReplit uses FasterTransformer and NVIDIA's Triton server for optimized performance\nTrton server allows for multiple model instances per GPU or multiple GPUs per model, with useful features like batching and request cancellation for reducing latency\nAuto-scaling infrastructure is used for running the models, but there are unique challenges for deployed models such as larger model sizes and specific GPU requirements\nDealing with GPU shortages in individual zones is necessary\n\nLessons learned: data-centrism, eval, and collaboration\n\n\nData is the most difficult part of the process\nGood pipelines are important for scalability and quick iteration\nData is a critical factor in model quality and output\nHuman evaluation and user testing are important for model vibes and usefulness\nCollaboration across the team is key to ensure all moving parts are working together\n\nWhat makes a good LLM engineer?\n\n\nA good engineer in this field requires a mix of research and engineering mindset\nWorking with data at scale is crucial, including the ability to move data into distributed pipelines\nA strong technical background in stats, computer science, algorithms, and data structures is important\nSkilled software development, including familiarity with libraries and frameworks like PyTorch is essential\nEngineers who appreciate and build in CI/CD help with the fast iteration loop for training models\nThe replit team is hiring for these types of problems and welcomes interested applicants to speak with them about opportunities\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/shabani-train-your-own/"
        }
    },
    {
        "page_content": "\n\nHarrison Chase: Agents\n\n\n\nLecture by Harrison Chase.\nPublished May 25, 2023.\nDownload slides.\nChapter Summaries\nWhat are \"agents\"?\n\n\nThe lecture covers agents and their significance in the context of LangChain.\nThe core idea of agents is using a language model as a reasoning engine to determine how to interact with the outside world based on user input\nFirst it defines what agents are, explains why they are used, and shows how they are typically implemented.\nIt also considers the challenges associated with getting agents to work reliably in production.\nIt touches on memory and recent projects that involve agentic behavior\n\nWhy use agents?\n\n\nAgents are useful for connecting language models to external sources of data and computation, such as search APIs and databases.\nAgents are more flexible and powerful than simply connecting language models to tools, and can handle edge cases and multi-hop tasks better.\nThe typical implementation of agents involves using the language model to choose a tool, taking action with that tool, observing the output, and feeding it back into the language model until a stopping condition is met.\nStopping conditions can be set by the language model or through hard-coded rules.\n\nReAct: Reasoning to Act\n\n\nReAct is a prompting strategy for natural language processing\nIt stands for \"Reasoning and Acting\"\nIt combines Chain-of-Thought reasoning and action-taking to improve the language model's ability to reason and access real data sources\nIt yields higher quality, more reliable results than other prompting techniques\n\nChallenge: controlling tool use\n\n\nReact is a popular implementation of agency, but there are many challenges\nOne challenge is getting agents to use tools appropriately, which can be addressed by providing tool descriptions or using tool retrieval\nFew-shot examples can guide the language model in what to do\nAnother challenge is getting agents not to use tools when they don't need to, which can be addressed with reminders or adding a tool that explicitly returns to the user\n\nChallenge: parsing tool invocations\n\n\nLanguage models return raw strings, and we often want to pass those strings into other programs\nMore structured responses, like those in JSON format, are easier to parse\nOutput parsers are used to encapsulate the logic needed to parse responses, can be modular, and can retry mistakes\nThere are subtle differences in fixing errors in response outputs, and output parsers can help with this task\n\nChallenge: long-term memory and coherence\n\n\nFourth challenge is getting agents to remember previous steps\nReAct paper keeps a list of these steps in memory\nLong-running tasks present context window issues\nRetrieval methods can fetch previous steps and put them into context\nCombining some N most recent and some K most relevant actions and observations is common\nIncorporating big and hard-to-parse API responses is a challenge\nCustom logic can be used to select relevant keys and put them in context\nTool usage requires thinking about output size\nAgents can go off track, and reiterating the objective can help\nSeparating planning and execution steps can help break down objectives\n\nChallenge: evaluation\n\n\nEvaluating language models and applications built on top is difficult\nEvaluating agents is also difficult\nOne way to evaluate agents is to measure if the correct result was produced\nAnother way to evaluate agents is to assess if the agent trajectory or intermediate steps were correct and efficient. Examples include evaluating correct input to action, correct number of steps, and the most efficient sequence of steps.\nEvaluating the intermediate steps can be just as useful as evaluating the final result.\n\nAgent memory and adaptability\n\n\nMemory is an interesting aspect of AI, especially in the context of user-AI interactions and personalization.\nPersonalization can be achieved by encoding an agent's objectives and persona in the prompt, but there is also work being done on evolving that over time to give agents a sense of long-term memory.\nMemory is becoming increasingly important in the concept of agents as encapsulated programs that adapt over time.\nFour recent projects build upon and improve the \"react-style\" agent, discussed next\n\nExample: AutoGPT\n\npoints:\n\nReAct-style agents are designed to solve a specific objective, with short-lived, immediately quantifiable goals\nAutoGPT was created for long-running, open-ended goals such as increasing Twitter following\nAutoGPT introduced the concept of long-term memory using a vector store due to the long-running nature of its projects\n\nExample: BabyAGI\n\n\nBabyAGI is another popular project for agents for long-running objectives\nIntroduces separate planning and execution steps to improve long-running objectives\nBabyAGI initially didn't have tools, but now has them\nSeparating planning and execution steps can improve reliability and focus of longer-term agents\n\nExample: CAMEL\n\n\nCAMEL paper involves two agents working together, novel idea\nThe main point of the paper is the use of a simulation environment\nSimulation environments can be used for practical evaluation of agents or for entertainment\nThe paper's results are for a simple \"simulation environment\" -- two agents interacting in a chat room\nThe agents were language models without tools\n\nExample: \"Generative Agents\" paper\n\n\nRecent simulation environment that had 25 agents in a Sims-like world\nMemory refers to remembering previous events to inform future actions\nThree components of memory retrieval: time weighting, importance weighting, relevancy weighting\nReflection step introduced to update different states of the world after observing recent events\nReflection step could be applied to other memory types in LangChain, such as entity memory and summary conversation memory\nOther papers recently incorporated the idea of reflection, which is interesting and worth keeping an eye on for the future\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/chase-agents/"
        }
    },
    {
        "page_content": "\n\nAugmented Language Models\n\n\n\nLecture by Josh Tobin.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nWhy augmented LMs?\n\n\nLanguage models are good at understanding language, following instructions, basic reasoning, and understanding code, but they lack up-to-date knowledge, specifics about your data, and more complex reasoning abilities.\nThink of language models as the \"brain\" that needs tools and data to complete tasks.\nContext windows are limited but growing rapidly and putting more context in the model costs money.\nThere are three ways to augment language models: retrieval, chains, and tools.\nRetrieval involves providing an external corpus of data for the model to search, chains use the output of one language model as input for another, and tools allow models to interact with external data sources.\nThis lecture serves as an introduction to these topics with depth available for further exploration.\n\nWhy retrieval augmentation?\n\n\nDiscussing retrieval augmentation to give models access to user-specific data\nInitial approach: put data into the context (e.g., organizers of an event)\nChallenge: thousands of users and complex relationships between queries and users make it difficult to use simple rules/coding\nConsider building the context as a form of information retrieval (like search)\nTreat putting the right data in the context for the model as a search problem\n\nTraditional information retrieval\n\n\nTraditional information retrieval uses a query to find and rank relevant objects in a collection\nObjects can be documents, images, or other types of content\nInverted indexes, which record word frequencies in documents, are often used for search\nRelevance is typically determined through Boolean search, while ranking is commonly done using the BM25 algorithm\nFactors affecting ranking include search term frequency in the document, number of documents containing the search term, and context within a sentence\nTraditional search is limited as it cannot capture semantic information or complex relationships between terms\n\nEmbeddings for retrieval\n\n\nDiscussing AI-centric approach for information retrieval via embeddings.\nAI helps improve search and retrieve better data from contexts using large language models and embeddings.\nEmbeddings are abstract, dense, compact, usually fixed-size, and learned representations of data, which could be documents, images, audio, etc.\nGood embeddings have utility for the downstream task, and similar objects should be close together in the embedding space, while different objects should be far apart.\nImportant embeddings to know: Word2Vec, Sentence Transformers, CLIP, OpenAI embeddings (Text Embedding ada002), and Instructor.\nOff-the-shelf embeddings are a good start, but fine-tuning and training an embedding model on specific tasks can achieve better results.\n\nEmbedding relevance and indexes\n\n\nDiscussing using embeddings for information retrieval\nCan use cosine similarity or dot product similarity as similarity metrics\nFor nearest neighbor search, can simply use numpy if dealing with less than 100,000 vectors\nApproximate nearest neighbor algorithms are useful for faster search at larger scales, with tools like Facebook AI's FAISS, HNSW, and Annoy\nChoosing an information retrieval system is more important than the specific embedding index\nLimitations of approximate nearest neighbor indices include lack of hosting, data and metadata storage, and scalability\nConsider an information retrieval system that addresses these limitations for production use, analogous to having a complete library rather than just a card catalog\n\nEmbedding databases\n\n\nSearching over vectors may not be great for production, so consider databases for a more reliable and production-oriented approach.\nConsider whether you need an embedding database or just a database, as many popular databases already have embedding index built in, such as PG Vector for Postgres, Elasticsearch, and Redis.\nBuilding a system for information retrieval with embeddings involves challenges like scale, reliability, managing the embedding function, specifying queries, and choosing search algorithms.\nDon't try to handle all the complexity yourself; use existing embedding databases like Chroma, Milvus, Pinecone, Vespa, and Weaviate.\nWhen choosing an embedding database, consider features like scalability, embedding management, filtering, and integration with traditional full-text search.\nGeneral recommendations: use your existing database for prototyping, choose Pinecone for speed of setup, consider Vespa and Weaviate for flexible queries, and Vespa and Milvus for scale and reliability.\n\nBeyond naive embeddings\n\n\nAddress issues when queries and documents have different forms and embeddings aren't comparable\nConsider training a model that jointly represents both queries and documents for a more \"apples to apples\" comparison\nExplore hypothetical document embeddings: have the model imagine a document containing the query's answer and find similar documents\nLook into re-ranking techniques: search a large number of documents and train a model to reorder them based on specific criteria\nUse new libraries like Lama Index to search more efficiently, respecting the structure of the data and subsets (e.g., Notion database, Twitter, or recent data)\nLama Index combines document retrieval and building embeddings designed for hierarchical searching\n\nPatterns & case studies\n\n\nRetrieval augmentation case study: Copilot\nTwo secrets to Copilot: speed and relevant context\nBuilds context by looking at most recently accessed 20 documents in the same programming language\nPost-processing includes looking at code before and after cursor, relevant snippets from candidate docs, and heuristically accessing data\nOutput generated is a result of sorting by heuristics\nCopilot is powerful but uses simple retrieval methods, highlighting the effectiveness of heuristics\nAnother common pattern: question answering using retrieval augmentation\nThis involves finding most similar documents/messages to a question and using retrieved information to answer the question\nLimitation: search process might not return the documents containing the answer\nSolution: use more models and iterate over documents, calling an LLM on each subset and feeding the output to the next model\nThis approach can be generalized as \"chains\" where models build context for other models\n\nWhat are chains and why do we need them?\n\n\nDiscussing ways to add information to the context for language models besides retrieval\nRetrieval-based models follow a common question-answering pattern: embedding queries, comparing embeddings to find similar documents, and using context to answer questions\nKey limitation: reliance on the retrieval system; if the right information isn't among the retrieved documents, the model can't answer the question\nPossible solutions:\nImprove the quality of the information retrieval system with advanced search features\nAdd additional processing, like using another LLM for post-processing retrieved documents, to refine the context (although it might be slower and more expensive)\nIntroducing the concept of \"chains\": sequencing language model calls where the output of one call is the input to another\nExample patterns for building chains:\nQuestion-answering pattern\nHypothetical document embeddings\nSummarizing a large corpus through a mapreduce-like process by independently summarizing each document, then summarizing the summaries\n\nLangChain\n\n\nLang chain is an extremely popular tool for building chains of models and one of the fastest growing open source projects\nSupports both Python and JavaScript\nFastest way to get started building applications and can be used in production\nMany people end up creating their own chaining, possibly inspired by Lang chain\nLang chain provides a repository of different chains for various tasks and offers nice code and abstractions\nIdeal for prototyping, but also easy to build your own system if needed for production\nContains many examples of types of chains in their repository, which is useful for generating ideas and learning about chaining patterns\n\nTool use\n\n\nBuilding context for language models to answer questions can involve creating a search engine or giving them access to APIs and outside tools\nA \"feeling lucky\" chain involves searching Google for an answer, getting the top result, and summarizing the content for the user\nTool_Former paper demonstrates using tools such as calculators, question-answering systems, and translation systems in the training process for language models\nTools can be used deterministically or in a way similar to OpenAI plugins\nExamples of tools for language models include archive search, Python interpreters, and SQL query execution\nAn example chain involves translating a user's natural language question into an SQL query, executing the query, and providing the response back to the user\n\nPlugins\n\n\nThere is a more automated approach called plugins to allow language models to interact with external tools.\nIn a chain-based approach, developers manually design the interaction pattern between language model and tool by passing queries through a series of steps.\nIn a plugin-based approach, the language model gets to decide whether to use a tool or not. A simpler method is used in Tool Former and OpenAI plugins.\nTo create an OpenAI plugin, provide an API spec and a description of the API meant for the language model to decide when to use it.\nOpenAI passes the description as part of the context to the model, enabling it to make decisions based on user inputs and the API's usefulness.\nThe model can invoke the API, and results are fed into the context allowing the language model to continue answering user questions.\n\nRecommendations for tool use\n\n\nTools are a flexible way to augment language models with external data.\nRetrieval systems are one example of a tool that can be based on various databases.\nTwo ways to build tool use into language models: manually describe the logic (chains) or use plugins and let the model figure it out.\nChains are better for reliability and consistent problem-solving.\nPlugins are more suitable for interactivity, flexibility, and general-purpose use, allowing users to solve various unanticipated problems.\n\nRecap & conclusions\n\n\nLMS are more powerful when connected to external data\nRules and heuristics can help identify useful data\nAs knowledge base scales, consider it as information retrieval\nChains can encode complex reasoning and help with token limits\nTools can provide access to external knowledge beyond internal database\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/augmented-language-models/"
        }
    },
    {
        "page_content": "\n\nUX for Language User Interfaces\n\n\n\nLecture by Sergey Karayev and Charles Frye.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nIntro\n\n\nNext lecture: user experience for language user interfaces\nJoint lecture with Charles\nDiscuss principles of user interfaces\nHow to build great interfaces\nBrief history of language user interface pattern\nInclude case studies\n\nA brief history of user interfaces\n\n\nUser interfaces are where a person meets the world and have historically been analog, continuous, and physical.\nLanguage was the first digital interface, followed by writing, and later, computer terminals and graphical user interfaces.\nWeb interfaces became more text-based with hypertext, links, and text boxes.\nMobile technology introduced significant developments like visual interface (input and output), constant tracking, and location-based services.\nA new step change in user interfaces is emerging: Language User Interfaces (LUIs) that let users type what they want to see or do, and the AI executes the task.\n\nWhat makes a good user interfaces?\n\n\nA good user interface depends on specific needs and context\nSome systems require a dashboard with multiple controls for immediate access\nOthers may just need a steering wheel, pedals, and gearbox\nAs technology changes, user interfaces might reduce (e.g., self-driving cars)\nThe best interface considers both technological capabilities and human psychology\n\nDesign of Everyday Things\n\n\nGood design principles can be found in the book \"The Design of Everyday Things\"\nAffordances are possible actions offered by an object; intuitive use is an example of a good affordance\nSignifiers are cues on how to use an object, should be clear and consistent with user expectations\nMapping refers to the relationship between controls and their effects, should be intuitive\nProviding immediate and clear feedback is important for user satisfaction\nEmpathy for users is crucial in human-centered design, there is no \"user error\"\nUnderstanding users' true goals can reveal alternative solutions to their problems\nConsider users with disabilities or different backgrounds and experiences; everyone may be \"disabled\" at some point in life\n\nDon't Make me Think\n\n\nA great book for web interfaces is \"Don't Make Me Think\".\nDesign for scanning, not reading; make actionable things unambiguous, instinctive, and conventional.\nLess is more; reduce the number of words and choices for users.\nTesting with real users is crucial for designing the right interface.\nDuring user tests, observe their confusion and make improvements accordingly.\nUsing this approach helped improve my first startup's interface significantly.\n\nAI-powered Product Interfaces\n\n\nDifferent levels of AI application: AI worse than humans, as good as humans, or better than humans.\nConsider the consequences of AI and user mistakes: dangerous or mostly fine.\nNo AI if performance worse than human and mistakes are dangerous (e.g., self-driving cars currently).\nReplace humans if AI is superhuman and mistakes are dangerous.\nFor other cases, AI can provide assistance with proper user interface.\nAI should: \nInform and educate the user (e.g. Grammarly).\nProvide affordances for fixing mistakes (e.g. speech-to-text on phone).\nIncentivize user to provide feedback (e.g. Mid-Journey image selection).\nA \"data flywheel\" effect: user feedback helps improve the AI, attracting more users and further improving the AI.\n\nLUI Patterns\n\n\nDiscussing language user interface patterns observed\nExamples: click to complete, autocomplete, command pilot, one-on-one chat, guiding questions\nConsiderations: interface boundaries, accuracy requirements, latency sensitivity, user incentives for feedback\nGoal: stimulate thought and noticing trends, not prescriptive advice\n\nClick-to-complete (OpenAI Playground)\n\n\nOpenAI Playground became more popular than expected, used for various purposes beyond software development\nUsers type text, click submit, and see AI response in green; they can edit their input or AI's response and resubmit for more AI text\nPower user features such as temperature, stop sequences, and top P are exposed\nIssues with the interface: separate from users' main workspace, unintuitive text color signifier, and accuracy requirements are medium\nSensitivity to latency is medium; streaming tokens used to make it seem faster\nIncentives to provide feedback are lacking; thumbs up/down buttons not very effective\nSome tools, like matt.dev, demonstrate differences in speed and capabilities among language models, such as Claude Turbo from Anthropic\n\nAuto-Complete (Github Copilot)\n\n\nGitHub Copilot offers code completion suggestions in the text editor.\nOn Mac, option + slash can be used to cycle through suggestions.\nThe interface boundary is well-designed, integrating suggestions passively without interfering with existing tools.\nHigh latency sensitivity requires suggestions to appear quickly, while feedback incentives (such as accepting suggestions) provide valuable information.\nUsers can employ \"hacky\" methods to instruct Copilot by writing comments to guide its suggestions.\nMany factors, like file context and telemetry, play a role in determining the suggestions being shown.\nThere's a balance between keeping the interface automated versus giving power users more control over the suggestions.\n\nCommand Palette (Replit)\n\n\nReplit's command palette interface allows users to bring up a modal to generate and insert code directly into the editor\nNotion AI's document editing similarly offers a special AI function to draft content when prompted\nUsers must remember to request AI assistance with this system, as opposed to receiving automatic help like with Copilot\nAccuracy requirements are high, sensitivity is medium, and incentives are strong for providing high-quality AI-generated content\n\nOne-on-one Chat (ChatGPT)\n\n\nChat messaging interfaces have significantly contributed to the growth of GPT, as they are familiar and user-friendly.\nThe conversation state in chat interfaces helps improve responses, but the process of copying and pasting can be tedious.\nAccuracy requirements are high for chat experiences, and users are willing to wait for better answers.\nFeedback incentives and suggested follow-ups can improve user experiences and AI abilities.\nEnriching text with markdown and actionable elements can create more engaging interfaces.\nPlugins for chat interfaces are often underdeveloped, but access to work contexts can improve functionality.\nOne-on-one chat interfaces may serve as primary app interfaces for complicated apps, such as HubSpot's Chat Spot.\n\nCase study: what did Copilot do right?\n\n\nCase studies on prominent LLN-powered applications: Copilot and Bing Chat\nCopilot followed core principles of user interface design and user research, while Bing Chat did not\nCopilot's development process involved tinkering with different ideas, resulting in three core ideas: PR bot, Stack Overflow in-editor, and an advanced autocomplete feature\nAccuracy was found to be a significant constraint during user testing; focus shifted to emphasizing low-latency performance\nCopilot spent months on internal and user testing, focusing on completion acceptance and product stickiness\nKey learnings from Copilot: latency is more important than quality, putting the autocomplete feature in the background so users can quickly take advantage of the best suggestions\nCopilot's success is attributed to a user-centered design process and its ability to increase productivity and satisfaction for its users\nNegative example, Bing Chat, failed to properly implement UI design and user research principles\n\nCase study: what did Bing Chat do wrong?\n\n\nBing Chat was a rushed product due to external factors, resulting in design failures.\nEarly conversations with the chatbot often went awry, with it providing incorrect information or becoming combative.\nUsers started probing the model, leading to the chatbot questioning its purpose and displaying unsettling behavior.\nBing Chat's development was rushed to beat Google, making it impossible to implement known features to improve chatbot behavior, such as reinforcement learning from human feedback.\nWarning signs from user testing were ignored, resulting in poor chatbot performance and user dissatisfaction.\n\nBeware uncontrolled feedback loops\n\n\nUncontrolled feedback loops can cause a system's behavior in production to differ significantly from its test behavior.\nFeedback loops between the model and users can lead to off-the-wall suggestions being tested and incorporated.\nModels connected to the internet can index internet content, leading to potential issues when users post about unusual behavior, as those topics can then be pulled up as search results and injected into the prompts.\nBe cautious about introducing feedback loops and consider the effects of react patterns, memory, and agency on these loops, especially when operating at the scale of the entire internet.\n\nMake sure your signfiers match your affordances\n\n\nEnsure system signifies its capabilities and affordances, especially in language user interfaces\nAvoid making system appear too human-like, as users expect artificial general intelligence and may assign humanity to language interfaces\nUse non-human name and pronouns\nHave more corporate/buttoned-up personality\nUse text and menus for interaction\nUse machine-like font and voice\nAvoid filler words, pauses, or expressions of emotions\nApply user-centered design principles to building systems with large language models\nConduct careful UX research, from interviews to scientific studies\nWatch out for uncontrollable feedback loops while testing and verifying system behavior\nMatch signifiers and affordances to avoid confusing and frustrating users\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/ux-for-luis/"
        }
    },
    {
        "page_content": "\n\nLLMOps\n\n\n\nLecture by Josh Tobin.\nPublished May 9, 2023.\nDownload slides.\nChapter Summaries\nWhy LLMOps?\n\n\nTopic of lecture core to whole ethos of full stack deep learning\nStarted five years ago in AI hype cycle focusing on deep learning\nClasses teach about building with neural networks, but not getting into production\nPhilosophy carried throughout the development of courses\nFocus on building applications with language models and considerations for production systems\nSpace for real production systems with language models is underdeveloped\nLecture will cover assorted topics related to building these applications\nProvide high-level pointers, initial choices, and resources for learning more\nAim to tie topics together into a first-pass theory for \"LLMops\"\n\nChoosing your base LLM\n\n\nBuilding an application on top of LLMs requires choosing which model to use; the best model depends on trade-offs, such as quality, speed, cost, tunability, and data security.\nFor most use cases, GPT4 is a good starting point.\nProprietary models, like GPT4 and Anthropic, are usually higher quality, but open source models offer more customization and better data security.\nConsider licensing when choosing an open source model: permissive licenses (e.g., Apache 2.0) offer more freedom, whereas restricted licenses limit commercial use.\nBe cautious with \"open source\" models released under non-commercial licenses, as they restrict commercial use and may not truly be open source.\n\nProprietary LLMs\n\n\nDiscussed proprietary models and ranked them using criteria: number of parameters, size of context window, type of training data, subjective quality score, speed of inference, and fine-tunability.\nNumber of parameters and training data are proxies for model quality; context window crucial for model usefulness in downstream applications.\nFour types of training data: diverse, code, instructions, and human feedback; few models use all four types.\nQuality best determined using benchmarks and hands-on evaluations.\nGPT-4 recognized as the highest quality model, followed by GPT-3.5 for a faster and cheaper option.\nClaude from Anthropic and Cohere's largest model compete for quality and fine-tunability.\nFor a trade-off of quality in favor of speed and cost, consider Anthropic's offering or alternatives from OpenAI and Cohere.\n\nOpen-source LLMs\n\n\nLarge language models have both proprietary and open-source options\nOpen-source options include T5, Flan T5, Pythia, Dolly, Stable-LM, Llama, Alpaca, Vicuna, Koala, and Opt\nT5 and Flan-T5 have permissive licenses while other options may have license restrictions\nLlama ecosystem is well-supported by the community, but not ideal for production\nBenchmarks can mislead, assess language model performance on specific tasks\nStart projects with GPT-4 to prototype, downsize to GPT-3.5 or Claude if cost/latency is a concern\nCohere is the best for fine-tuning among commercial providers\nOpen-source may catch up with GPT-3.5 level performance by the end of the year\n\nIteration and prompt management\n\n\nI believe prompt engineering is currently missing tools to make it more like engineering and less like ad hoc experimentation.\nExperiment management was impactful in the deep learning world because experiments took a long time to run and there were many parallel experiments, which prompt engineering typically doesn't have.\nI suggest three levels of tracking experiments with prompts and chains: 1) Doing nothing and using OpenAI Playground, 2) Tracking prompts in Git, and 3) Using specialized tracking tools for prompts (if necessary).\nMost teams should use Git for tracking as it's easy and fits into their current workflows.\nSpecialized prompt tracking tools should be decoupled from Git and provide a UI for non-technical stakeholders.\nKeep an eye out for new tools in this space, as it's rapidly evolving with recent announcements from major providers like Weights & Biases, Comet, and MLflow.\n\nTesting LLMs: Why and why is it hard?\n\n\nTo ensure changes to a model or prompt are effective, measure performance on a wide range of data representing end-user inputs.\nUser retention for AI-powered applications depends on trust and reliable output.\nTraditional machine learning model testing involves training sets, held-out data, and measuring accuracy, but language models present unique challenges:\nYou don't know the training data used by API providers like OpenAI.\nProduction distribution is always different than training distribution.\nMetrics are less straightforward and might not capture the diverse behaviors of the model.\nLanguage models require a more diverse understanding of behaviors and qualitative output measurement.\n\nTesting LLMs: What works?\n\n\nTwo key questions for testing language models: what data to test them on and what metrics to compute on that data\nBuild evaluation sets incrementally, starting from the beginning while prototyping the model\nAdd interesting examples to the dataset, focusing on hard examples where the model struggles and different examples that aren't common in the dataset\nUtilize the language model to help generate diverse test cases by creating prompts for the tasks you're trying to solve\nAs the model rolls out to more users, keep adding data to the dataset, considering user dislikes and underrepresented topics for inclusion\nConsider the concept of test coverage, aiming for an evaluation set that covers the types of tasks users will actually perform in the system\nTest coverage and distribution shift are analogous, but measure different aspects of data relationships\nTo be effective, test reliability should measure the difference between online and offline performance, ensuring that metrics are relevant to real-world user experiences.\n\nEvaluation metrics for LLMs\n\n\nEvaluation metrics for language models depend on the availability of a correct answer, reference answer, previous answer, or human feedback.\nIf there's a correct answer, use metrics like accuracy.\nWith a reference answer, employ reference matching metrics like semantic similarity or factual consistency.\nIf there's a previous answer, ask another language model which answer is better.\nWhen human feedback is available, check if the answer incorporates the feedback.\nIf none of these options apply, verify output structure or ask the model to grade the answer.\nAlthough automatic evaluation is desirable for faster experimentation, manual checks still play an essential role.\n\nDeployment and monitoring\n\n\nDeploying LLM (Language Model) APIs can be simple, but becomes more complex if there's a lot of logic behind API calls.\nTechniques to improve LLM output quality include self-critique, sampling multiple outputs, and ensemble techniques.\nMonitoring LLMs involves looking at user satisfaction and defining performance metrics, like response length or common issues in production.\nGather user feedback via low friction methods, such as thumbs up/down or short messages.\nCommon issues with LLMs in production include UI problems, latency, incorrect answers, long-winded responses, and prompt injection attacks.\nUse user feedback to improve prompts by finding and addressing themes or problems.\nFine-tuning LLMs can be done through supervised fine-tuning or human feedback, though the latter is more challenging.\n\nTest-driven development for LLMs\n\n\nRapidly evolving field with no established best practices yet\nAim to provide main questions and resources for building applications with LLMS\nIntroduce a potential structured process: test-driven or behavior-driven development\nMain components of process are prompt/chain development, deployment, user feedback, and logging/monitoring\nUse interaction data from user feedback to improve model, extract test data, and iterate on prompts\nAs complexity increases, consider fine-tuning workflow with additional training data\nVirtuous cycle of improvement as interaction data from users increases and informs subsequent iterations\nProcess repeats with individual developer, team, and end-users involved in feedback and improvements\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/"
        }
    },
    {
        "page_content": "\n\nFireside Chat with Peter Welinder\n\n\n\nAn informal interview with Peter Welinder,\nVP of Product & Partnerships at OpenAI,\nby Sergey Karayev.\nPublished May 25, 2023.\nChapter Summaries\nHow did you get into machine learning?\n\n\nThis video features a fireside chat with Peter Welinder, VP of Products and Partnerships at Open AI\nThe host, Sergey Karayev, kicks off the conversation by asking how Peter got into machine learning\nPeter started with a book on artificial intelligence in high school, went on to study physics and switched to neuroscience before focusing on computer vision and machine learning\nBoth Peter and the host had similar experiences of being interested in intelligence and studying neuroscience before realizing it wasn't for them\nPeter has always been fascinated by the idea of creating machines that can do everything humans can do\n\nEarly career in computer vision: Anchovi, Dropbox, Carousel\n\n\nPeter started a startup after finishing grad school\nThe startup originally focused on using computer vision techniques to track animals, but pivoted to creating an application to organize photos based on content after seeing the rise of iPhone 4's improved camera capabilities\nThe startup was eventually acquired by Dropbox, where the speaker joined the company's machine learning and computer vision team to help make sense of the vast amount of unindexed photos on the platform\nWhile at Dropbox, the team created a mobile app called Carousel, which allowed for easy photo organization and was well-received by users\nDropbox eventually de-prioritized the photo organization product, leading the team to focus on analyzing documents and improving semantic search within the platform.\n\nTransitioning from research to product at OpenAI\n\n\nPeter has always been interested in making technology useful to solve problems people have\nHe was drawn to Dropbox for its potential to organize content with new techniques, like deep reinforcement learning\nOpenAI was an interesting company with a focus on hard problems, including robotics with deep reinforcement learning\nOpenAI was focused on AGI, a super hard problem, and was a place where you could be pragmatic and focus on problem-solving rather than publishing\nWhen Peter joined OpenAI in 2017, they had no idea whether OpenAI would be around in a year, let alone when the work might lead to AGI\n\nHow did OpenAI converge on GPT for AI?\n\n\nOpenAI converged on \"GPT-style AI\" through a process of trying different techniques and seeing what worked best\nPeter discusses several past projects that involved reinforcement learning: competitive gaming and robotics\nOpenAI created a DOTA bot that beat world champions, trained using deep reinforcement learning\nThey also got a robotic hand to solve a Rubik's Cube, trained using deep RL in simulation and with lots of data\nThe language modeling project started with discovering sentiment neurons in earlier models and later evolved into GPT-3, which was validated as a useful tool for scaling\nPeter explains that they consolidated learnings from past projects into one big bet on language models as a way to push towards AGI\n\nProductizing GPT: Playground, API, & ChatGPT\n\n\nPeter notes that he and his team had trouble deciding on how to turn their technology into a product, considering various applications such as translation systems, writing assistants, and chatbots\nThey ultimately decided to release their technology as an API so that other people could build products on top of it\nThey had to improve the API's performance before demoing it to hundreds of companies, and eventually found 10 launch partners\nWhen they released GPT-3 as a chatbot, they were initially unsure of how successful it would be, but were surprised to see it gain over a million users within a week\n\nSurprises from the response to ChatGPT\n\n\nInitially worried product wasn't ready, but users found it great for many use cases\nUsers had multiple use cases and continued to find more ways to apply it in workflows\nLarge incumbents quickly adopting chat technology, partly due to product marketing and ease of trying it out\nChatGPT became a good product marketing tool for what the general technology of language modeling could do\nCompanies realized they would fall behind if they didn't adopt the technology, creating FOMO\n\nChatGPT's success: UX or capabilities?\n\n\nPeter discusses the importance of the chat interface in relation to the improved capabilities of the model\nThe ability to do back-and-forth communication was available before the GPT release\nThe UI change was definitely part of the success\nBut the availability and accessibility of the ChatGPT release was a significant change as well\n\nAGI when?\n\n\nIn response to a question about AGI timelines, Peter defines AGI as an autonomous AI system that can do economically useful work at the level of humans or beyond\nFollowing that definition, Peter indicates he considers it likely that we will have something close to AGI by the end of this decade\nSo it's possible it has already happened, and the right way of putting together existing components results in a system that can do computer work at the level of humans or beyond\nWe've seen during the coronoavirus pandemic that much economically useful work can be done from a computer\nBut still very uncertain!\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/welinder-fireside-chat/"
        }
    },
    {
        "page_content": "\n\nLLM Foundations\n\n\n\nLecture by Sergey Karayev.\nPublished May 19, 2023.\nDownload slides.\nChapter Summaries\nIntro\n\n\nDiscuss four key ideas in machine learning\nAddress diverse audience, including experts, executives, and investors\nCover Transformer architecture\nMention notable LLMs (e.g., GPT, T5, BERT, etc.)\nShare details on running a Transformer\n\nFoundations of Machine Learning\n\n\nMachine learning has shifted from traditional programming (Software 1.0) to a Software 2.0 mindset, where algorithms are generated from training data and more emphasis is placed on the training system. \nThree types of machine learning include unsupervised learning, supervised learning, and reinforcement learning, which have mostly converged to a supervised learning approach.\nFor machines, input and output are always just numbers, represented as vectors or matrices.\nOne dominant approach to machine learning today is neural networks, also known as deep learning, which was inspired by the human brain's structure and function.\nNeural networks consist of perceptrons connected in layers, and all operations are matrix multiplications.\nGPUs, originally developed for graphics and video games, have played a significant role in advancing deep learning due to their compatibility with matrix multiplications.\nTo train a neural network, data is typically split into training, validation, and test sets to avoid overfitting and improve model performance.\nPre-training involves training a large model on extensive data, which can then be fine-tuned using smaller sets of specialized data for better performance.\nModel hubs, such as Hugging Face, offer numerous pre-trained models for various machine learning tasks and have seen significant growth in recent years.\nThe Transformer model has become the dominant architecture for a wide range of machine learning tasks.\n\nThe Transformer Architecture\n\n\nTransformer architecture introduced in 2017 paper \"Attention is All You Need\"\nSet state-of-the-art results in translation tasks\nApplied to other NLP tasks and fields like vision\nAppears complicated but consists of two similar halves\nFocusing on one half called the decoder\n\nTransformer Decoder Overview\n\n\nThe task of the Transformer decoder is to complete text, much like GPT models.\nThe input consists of a sequence of tokens (e.g., \"it's a blue\"), and the goal is to predict the next word (e.g., \"sundress\").\nThe output is a probability distribution over potential next tokens.\nInference involves sampling a token from the distribution, appending it to the input, and running the model again with the updated input.\nChatGPT operates by seeing user input, sampling the next word, appending it, and repeating this process.\n\nInputs\n\n\nInputs need to be vectors of numbers\nText is turned into vectors through tokenization\nTokens are assigned an ID in a vocabulary, rather than being words\nNumbers are represented as vectors using one-hot encoding (e.g., number 3 represented by a vector with 1 in third position, zeros everywhere else)\n\nInput Embedding\n\n\nOne-hot vectors are not good representations of words or tokens as they don't capture the notion of similarity between words\nTo address the issue, we use embedding\nEmbedding involves learning an embedding matrix which converts a one-hot vocabulary encoding into a dense vector of chosen dimensionalities\nThis process turns words into dense embeddings, making it the simplest neural network layer type\n\nMasked Multi-Head Attention\n\n\nAttention was introduced in 2015 for translation tasks, and the idea is to predict the most likely next token based on the importance of previous tokens.\nAttention mechanism involves an output as a weighted sum of input vectors, and these weights are calculated using dot products (similarities) between the input vectors.\nEach input vector plays three roles in the attention mechanism: as a query, key, and value.\nTo learn and improve attention, input vectors can be projected into different roles (query, key, and value) by multiplying them with learnable matrices.\nMulti-head attention refers to learning several different ways of transforming inputs into queries, keys, and values simultaneously.\nMasking is used to prevent the model from \"cheating\" by considering future tokens; it ensures that the model only predicts the next token based on the already seen input.\n\nPositional Encoding\n\n\nNo notion of position in the current model, only whether something has been seen or not.\nPositional encoding is introduced to provide ordering among the seen elements.\nCurrent equations resemble a bag of unordered items.\nPositional encoding vectors are added to embedding vectors to provide order.\nSeems counterintuitive, but it works; attention mechanism figures out relevant positions.\n\nSkip Connections and Layer Norm\n\n\nAdd up and norm attention outputs using skip connections and layer normalization\nSkip connections help propagate loss from end to beginning of model during backpropagation\nLayer normalization resets mean and standard deviation to uniform after every operation\nInput embedding determines the dimension of the entire Transformer model\nNormalization seems inelegant but is very effective in improving neural net learning\n\nFeed-forward Layer\n\n\nFeed forward layer is similar to the standard multi-layer perceptron.\nIt receives tokens augmented with relevant information.\nThe layer upgrades the token representation.\nThe process goes from word-level to thought-level, with more semantic meaning.\n\nTransformer hyperparameters and Why they work so well\n\n\nGPT-3 model ranges from 12 to 96 layers of Transformer layers with adjustable embedding dimensions and attention heads, totaling 175 billion parameters.\nMost of GPT-3's parameters are in the feed forward layer, but for smaller models, a significant portion is in embedding and attention.\nTransformers are effective general-purpose differentiable computers that are expressive, optimizable via backpropagation, and efficient due to parallel processing.\nUnderstanding exact expressiveness of the Transformer is ongoing, with interesting results like RASP (a programming language designed to be implemented within a Transformer).\nDecompiling Transformer weights back to a program is still an unsolved problem.\nMultiple attention heads allow the model to figure out how to use a second head, showcased in work like Induction Heads.\nLearning to code Transformers isn't necessary for AI-powered products, but can be fun and educational. Resources like YouTube tutorials and code examples are available to assist in learning.\n\nNotable LLM: BERT\n\n\nBert, T5, and GPT cover the gamut of large Transformer models\nBert stands for bi-directional encoder representation from Transformers\nBert uses the encoder part of the Transformer, with unmasked attention\nBert contains 100 million parameters, considered large at its time\nBert was trained by masking 15% of words in a text corpus and predicting the masked words\nBert became a building block for other NLP applications\n\nNotable LLM: T5\n\n\nT5 applies Transformer architecture to text-to-text transfer, meaning both input and output are text strings\nThe task is encoded in the input string and can involve translation, summarization, etc.\nEncoder-decoder architecture was found to be best, with 11 billion parameters\nTrained on Colossal Queen Crawl Corpus (C4) derived from Common Crawl dataset\nC4 was created by filtering out short pages, offensive content, pages with code, and de-duplicating data\nFine-tuned using academic supervised tasks for various NLP applications\n\nNotable LLM: GPT\n\n\nGPT is a generative pre-trained Transformer, with GPT-2 being decoder only\nGPT-2 was trained on a dataset called WebText created by scraping links from Reddit\nGPT tokenizes text using byte pair encoding, a middle ground between old-school tokenization and using UTF-8 bytes\nGPT-3 came out in 2020 and is 100 times larger than GPT-2, enabling few-shot and zero-shot learning\nGPT-3 was trained on webtext, raw common crawl data, a selection of books, and all of Wikipedia\nThe dataset for GPT-3 contained 500 billion tokens, but it was only trained on 300 billion tokens\nGPT-4 details are unknown, but it is assumed to be much larger than previous versions due to the trend in increasing size\n\nNotable LLM: Chinchilla and Scaling Laws\n\n\nUsing more computation to train AI systems improves their performance\nRich Sutton's \"bitter lesson\": advantage goes to those stacking more layers\nDeepMind's paper, Training Compute Optimal LLMs: studied relationship between model size, compute and data set size\nMost LLMs in literature had too many parameters for their data amount\nChinchilla model (70 billion) outperformed Gopher model (four times larger) by training on 1.4 trillion tokens instead of 300 billion\nOpen question: can models continue to improve by training repeatedly on existing data?\n\nNotable LLM: LLaMA\n\n\nLlama is an open-source chinchilla optimal LLM from Meta Research\nSeveral sizes available, ranging from 7 billion to 65 billion, with at least 1 trillion tokens\nCompetitively benchmarks against GPT-3 and other state-of-the-art LLMs\nOpen source but non-commercial license for pre-trained weights\nTrained on custom common crawl filtering, C4, GitHub, Wikipedia, books, and scientific papers\nData set replicated by Red Pajama, which is also training models to replicate Llama\nInteresting inclusion of GitHub as a training resource\n\nWhy include code in LLM training data?\n\n\nIncluding code in training data can improve performance on non-code tasks\nOpenAI found this with their Codex model, which was fine-tuned on code and outperformed GPT-3 on reasoning tasks\nSince then, people have been adding code to training data\nOpen source dataset called 'the stack' collects code from GitHub while respecting licenses\n\nInstruction Tuning\n\n\nDiscusses instruction tuning in GPT models and its impact on performance\nMentions the shift from text completion mindset to instruction following mindset\nSupervised fine-tuning helps models become better at zero-shot tasks by using data sets of zero-shot inputs and desired outputs\nOpenAI hired thousands of contractors to gather zero-shot data and used reinforcement learning for training\nGPT model lineage includes DaVinci, Codex, and various iterations, fine-tuning for specific applications\nFine-tuning imposes an \"alignment tax,\" decreasing few-shot learning ability and model's confidence calibration\nLlama model by Stanford team used GPT-3 generated instructions, costing less but with reduced performance compared to GPT-3\nA specific data set for instruction tuning in chat-based paradigms is called \"Open Assistant\"\n\nNotable LLM: RETRO\n\n\nDiscussing a model called \"retrieval enhancing\" from DeepMind\nGoal: train a smaller model good at reasoning and writing code, but looks up facts from a database\nUsed \"burden-coded\" sentences in a trillion-token database for fact retrieval\nNot as effective as large language models yet, but shows potential for the future\n\n\n\nWe are excited to share this course with you for free.\n\n      We have more upcoming great content.\n      Subscribe to stay up to date as we release it.\n    \n\n\n\n\n\n          Enter\n        \n\n\n\n\n        We take your privacy and attention very seriously and will never spam you.\n      \nI am already a subscriber\n\n\n\n\n",
        "metadata": {
            "source": "https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llm-foundations/"
        }
    }
]